{"fanshi118/CitiBike-and-Motor-Vehicle-Collisions": [{"CitiBike_spatial_parser.ipynb": [["import pandas as pd, numpy as np, matplotlib.pyplot as plt, urllib2\n", "from zipfile import ZipFile\n", "from StringIO import StringIO\n", "%matplotlib inline\n", "plt.style.use('ggplot')"], ["bike_months = [\n", "    '201501',\n", "    '201504',\n", "    '201507',\n", "    '201510'\n", "]"], ["frames = []\n", "for m in bike_months:\n", "    url = 'https://s3.amazonaws.com/tripdata/%s-citibike-tripdata.zip'%m\n", "    r = urllib2.urlopen(url).read()\n", "    myfile = ZipFile(StringIO(r))\n", "    df_csv = myfile.open('%s-citibike-tripdata.csv'%m)\n", "    frame = pd.read_csv(df_csv)\n", "    frames.append(frame)\n", "\n", "df = pd.concat(frames, ignore_index=True)\n", "df"], ["start = df.groupby('start station id').count()['start station name']\n", "end = df.groupby('end station id').count()['end station name']"], ["df_s = pd.DataFrame(columns=['StationID','NumberofTrips'])\n", "df_s['StationID'] = start.index\n", "df_s['NumberofTrips'] = start.values\n", "df_s = df_s.sort(['NumberofTrips'],ascending=False)\n", "df_s"], ["df_s['StationID'][:10].tolist()"], ["stations = pd.DataFrame(columns=['id','lat','lon'])\n", "count = 0\n", "for i in df_s['StationID'][:20].tolist():\n", "    stations.loc[count,'id'] = i\n", "    stations.loc[count,'lat'] = df[df['start station id']==i]['start station latitude'].values[0]\n", "    stations.loc[count,'lon'] = df[df['start station id']==i]['start station longitude'].values[0]\n", "    count += 1\n", "stations"], ["df_sf = df_s.merge(stations,how='inner',left_on='StationID',right_on='id')\n", "df_sf"], ["df_e = pd.DataFrame(columns=['StationID','NumberofTrips'])\n", "df_e['StationID'] = end.index\n", "df_e['NumberofTrips'] = end.values\n", "df_e = df_e.sort(['NumberofTrips'],ascending=False)\n", "stationss = pd.DataFrame(columns=['id','lat','lon'])\n", "count = 0\n", "for i in df_e['StationID'][:20].tolist():\n", "    stationss.loc[count,'id'] = i\n", "    stationss.loc[count,'lat'] = df[df['end station id']==i]['end station latitude'].values[0]\n", "    stationss.loc[count,'lon'] = df[df['end station id']==i]['end station longitude'].values[0]\n", "    count += 1\n", "df_ef = df_e.merge(stationss,how='inner',left_on='StationID',right_on='id')\n", "df_ef"], ["df_test = df_sf.merge(df_ef,how='outer',on='id')\n", "df_test"], ["df_test.loc[20,'lat_x'] = df_test.loc[20,'lat_y']\n", "df_test.loc[20,'lon_x'] = df_test.loc[20,'lon_y']\n", "df_test"], ["df_test = df_test.rename(columns={'lat_x':'lat','lon_x':'lon'})\n", "df_f = df_test.drop(['StationID_x','NumberofTrips_x','StationID_y','NumberofTrips_y','lat_y','lon_y'],axis=1)\n", "df_f"], ["for i in df_f.index:\n", "    if i==15:\n", "        df_f.loc[i,'popular as'] = 'start'\n", "    elif i==20:\n", "        df_f.loc[i,'popular as'] = 'end'\n", "    else:\n", "        df_f.loc[i,'popular as'] = 'both start & end'\n", "df_f"], ["df_f.to_csv('data_/cb_pop_locs.csv')"], []]}, {"CitiBike_temporal_parser1.ipynb": [["import pandas as pd, numpy as np, matplotlib.pyplot as plt, datetime,  statsmodels.formula.api as smf, matplotlib.dates as mdates\n", "from zipfile import ZipFile\n", "from StringIO import StringIO\n", "from scipy import stats\n", "import urllib2\n", "%matplotlib inline\n", "plt.style.use('ggplot')"], ["bike_months2 = [\n", "    '201307',\n", "    '201308',\n", "    '201309',\n", "    '201310',\n", "    '201311',\n", "    '201312',\n", "    '201401',\n", "    '201402',\n", "    '201403',\n", "    '201404',\n", "    '201405',\n", "    '201406',\n", "    '201407',\n", "    '201408'\n", "]"], ["bike_months = [\n", "    '201409',\n", "    '201410',\n", "    '201411',\n", "    '201412',\n", "    '201501',\n", "    '201502',\n", "    '201503',\n", "    '201504',\n", "    '201505',\n", "    '201506',\n", "    '201507',\n", "    '201508',\n", "    '201509',\n", "    '201510',\n", "    '201511'\n", "]"], ["bike_dict = {}\n", "for m in bike_months:\n", "    url = 'https://s3.amazonaws.com/tripdata/%s-citibike-tripdata.zip'%m\n", "    r = urllib2.urlopen(url).read()\n", "    myfile = ZipFile(StringIO(r))\n", "    df_csv = myfile.open('%s-citibike-tripdata.csv'%m)\n", "    df = pd.read_csv(df_csv)\n", "    bike_dict[m] = len(df)\n", "bike_dict"], ["bike_dict2 = {}\n", "for m in bike_months2:\n", "    url = 'https://s3.amazonaws.com/tripdata/%s-citibike-tripdata.zip'%m\n", "    r = urllib2.urlopen(url).read()\n", "    myfile = ZipFile(StringIO(r))\n", "    df_csv = myfile.open('%s-%s - Citi Bike trip data.csv'%(m[:4],m[4:]))\n", "    df = pd.read_csv(df_csv)\n", "    bike_dict2[m] = len(df)\n", "bike_dict2"], ["num_of_trips = bike_dict2.copy()\n", "num_of_trips.update(bike_dict)"], ["for i in num_of_trips.keys():\n", "    num_of_trips[i[:4]+'-'+i[4:]] = num_of_trips.pop(i)\n", "num_of_trips"], ["trips = pd.DataFrame(num_of_trips.items(),columns=['Month','Num_of_trips'])\n", "trips"], ["for i,j in trips['Month'].iteritems():\n", "    trips.loc[i,'Month'] = datetime.datetime.strptime(j, '%Y-%m')\n", "trips['Month']"], ["cbtrips = trips.groupby('Month').mean()['Num_of_trips']"], ["cbtrips"], ["plt.figure(figsize=(16,5))\n", "cbtrips.plot(label='Citi Bike trips')\n", "plt.title('Number of Citi Bike trips over time')\n", "plt.legend(loc=0)"], ["def convertDate(d):\n", "    dt = datetime.datetime.strptime(d, '%m/%d/%Y')\n", "    dt = dt.replace(day=1)\n", "    return dt\n", "\n", "df = pd.read_csv('NYPD_Motor_Vehicle_Collisions.csv', converters={'DATE': convertDate})\n", "c_inj_agg = df.groupby('DATE').sum()['NUMBER OF CYCLIST INJURED']\n", "c_k_agg = df.groupby('DATE').sum()['NUMBER OF CYCLIST KILLED']\n", "cyc = c_inj_agg + 10*c_k_agg"], ["fig, ax1 = plt.subplots(figsize=(16,5))\n", "ax1.plot(cbtrips.index, cbtrips, color='b', label='Number of Citi Bike trips')\n", "ax1.legend(loc=3)\n", "ax2 = ax1.twinx()\n", "cyc_ = cyc[12:-1]\n", "ax2.plot(cyc_.index, cyc_, label='Number of accidents (weighted)')\n", "plt.title('Number of Citi Bike trips and cyclist accidents over time')\n", "plt.xlabel('Months')\n", "ax2.legend(loc=4)"], ["corr=stats.pearsonr(cbtrips,cyc_)[0]\n", "print('Correlation bewteen Citi Bike trips and cyclist accidents: {0}'.format(corr))"], []]}, {"CitiBike_temporal_parser2.ipynb": [["import pandas as pd, numpy as np, matplotlib.pyplot as plt, datetime,  statsmodels.formula.api as smf, matplotlib.dates as mdates\n", "from zipfile import ZipFile\n", "from StringIO import StringIO\n", "from scipy import stats\n", "import urllib2\n", "%matplotlib inline\n", "plt.style.use('ggplot')"], ["bike_months = [\n", "    '201501',\n", "    '201504',\n", "    '201507',\n", "    '201510'\n", "]"], ["def convertTime(d):\n", "    if len(d)<17:\n", "        dt = datetime.datetime.strptime(d, '%m/%d/%Y %H:%M')\n", "    else:\n", "        dt = datetime.datetime.strptime(d, '%m/%d/%Y %H:%M:%S')\n", "    dt = dt.replace(day=1, month=1, minute=0, year=2015)\n", "    return dt\n", "\n", "frames = []\n", "for m in bike_months:\n", "    url = 'https://s3.amazonaws.com/tripdata/%s-citibike-tripdata.zip'%m\n", "    r = urllib2.urlopen(url).read()\n", "    myfile = ZipFile(StringIO(r))\n", "    df_csv = myfile.open('%s-citibike-tripdata.csv'%m)\n", "    df = pd.read_csv(df_csv, converters={'starttime': convertTime})\n", "    frames.append(df)\n", "\n", "df_cb = pd.concat(frames, ignore_index=True)\n", "df_cb"], ["df_cb['hour'] = df_cb.apply(lambda x: x['starttime'].hour, axis=1)\n", "trips = df_cb.groupby('hour').count()['tripduration']\n", "trips"], ["trips = trips/120.0 #get daily amount"], ["def convertTime1(d):\n", "    dt = datetime.datetime.strptime(d, '%H:%M')\n", "    dt = dt.replace(day=1, month=1, minute=0, year=2015)\n", "    return dt\n", "\n", "df_pd = pd.read_csv('data/NYPD_Motor_Vehicle_Collisions.csv', converters={'TIME': convertTime1})\n", "df_pd['HOUR'] = df_pd.apply(lambda x: x['TIME'].hour, axis=1)\n", "c_inj_agg = df_pd.groupby('HOUR').sum()['NUMBER OF CYCLIST INJURED']\n", "c_k_agg = df_pd.groupby('HOUR').sum()['NUMBER OF CYCLIST KILLED']\n", "cyc = c_inj_agg + 10*c_k_agg"], ["fig, ax1 = plt.subplots(figsize=(16,5))\n", "ax1.plot(trips.index, trips, color='b', label='Number of Citi Bike trips')\n", "ax1.legend(loc=2)\n", "ax2 = ax1.twinx()\n", "ax2.plot(cyc.index, cyc, label='Number of accidents (weighted)')\n", "plt.title('Number of Citi Bike trips and cyclist accidents over hours')\n", "ax2.legend(loc=1)"], ["from scipy import stats\n", "corr=stats.pearsonr(trips,cyc)[0]\n", "print('Correlation bewteen Citi Bike trips and cyclist accidents: {0}'.format(corr))"], []]}, {"NYPD_spatial_parser.ipynb": [["import pandas as pd, numpy as np, matplotlib.pyplot as plt, datetime, statsmodels.formula.api as smf, matplotlib.dates as mdates\n", "%matplotlib inline\n", "plt.style.use('ggplot')"], ["def convertDate(d):\n", "    dt = datetime.datetime.strptime(d, '%m/%d/%Y')\n", "    return dt\n", "\n", "df = pd.read_csv('data/NYPD_Motor_Vehicle_Collisions.csv', converters={'DATE': convertDate})"], ["c_inj_agg = df.groupby('DATE').sum()['NUMBER OF CYCLIST INJURED']"], ["df_ = df[(df['NUMBER OF CYCLIST INJURED']>0)|(df['NUMBER OF CYCLIST KILLED']>0)]\n", "print len(df),len(df_)"], ["df_.head()"], ["df_.to_csv('data/Bicycle_Collisions.csv')"], ["df__ = df[df['NUMBER OF CYCLIST KILLED']>0]\n", "df__.to_csv('data_/Bicycle_Deaths.csv')"], ["d = pd.Timestamp(datetime.datetime(2013, 7, 1))"], ["d1 = df_[df_['DATE']<d]\n", "d2 = df_[df_['DATE']>=d]"], ["print len(d1),len(d2)\n", "d1"], ["d1.to_csv('data/b4ct_collisions.csv')\n", "d2.to_csv('data/afct_collisions.csv')"], ["inj_b4 = d1.groupby('ZIP CODE').sum()['NUMBER OF CYCLIST INJURED']\n", "inj_af = d2.groupby('ZIP CODE').sum()['NUMBER OF CYCLIST INJURED']"], ["inj_b4.values"], ["df_b4 = pd.DataFrame(columns=['ZipCode','NumberofIncidents'])\n", "df_b4['ZipCode'] = inj_b4.index\n", "df_b4['NumberofIncidents'] = inj_b4.values\n", "df_b4"], ["df_b4 = df_b4.sort(['NumberofIncidents'],ascending=False)\n", "df_b4[:20]"], ["df_b4.to_csv('data_/b4_collision_zip.csv')"], ["df_af = pd.DataFrame(columns=['ZipCode','NumberofIncidents'])\n", "df_af['ZipCode'] = inj_af.index\n", "df_af['NumberofIncidents'] = inj_af.values\n", "df_af = df_af.sort(['NumberofIncidents'],ascending=False)\n", "df_af[:20]"], ["df_af.to_csv('data_/af_collision_zip.csv')"], []]}, {"NYPD_temporal_parser1.ipynb": [["import pandas as pd, numpy as np, matplotlib.pyplot as plt, datetime, statsmodels.formula.api as smf, matplotlib.dates as mdates\n", "%matplotlib inline\n", "plt.style.use('ggplot')"], ["def convertDate(d):\n", "    dt = datetime.datetime.strptime(d, '%m/%d/%Y')\n", "    if dt.month<7:\n", "        dt = dt.replace(day=1, month=1)\n", "    elif dt.month>7:\n", "        dt = dt.replace(day=1, month=7)\n", "    else:\n", "        dt = dt.replace(day=1)\n", "    return dt\n", "\n", "df = pd.read_csv('data/NYPD_Motor_Vehicle_Collisions.csv', converters={'DATE': convertDate})"], ["df.columns"], ["c_inj_agg = df.groupby('DATE').sum()['NUMBER OF CYCLIST INJURED']\n", "c_k_agg = df.groupby('DATE').sum()['NUMBER OF CYCLIST KILLED']"], ["plt.figure(figsize=(16,5))\n", "c_inj_agg.plot(color='b', label='Number of accidents')\n", "plt.axvline(c_inj_agg.index[3], linestyle='--', color='r', label='Vision Zero')\n", "plt.axvline(c_inj_agg.index[2], linestyle='--', color='g', label='Citi Bike')\n", "plt.ylabel('Number of bikers injured')\n", "plt.title('Number of bikers injured over time')\n", "plt.legend(loc=2)\n", "plt.figure(figsize=(16,5))\n", "c_k_agg.plot(color='b', label='Number of accidents')\n", "plt.axvline(c_k_agg.index[3], linestyle='--', color='r', label='Vision Zero')\n", "plt.axvline(c_k_agg.index[2], linestyle='--', color='g', label='Citi Bike')\n", "plt.ylabel('Number of bikers killed')\n", "plt.title('Number of bikers killed over time')\n", "plt.legend(loc=2)"], ["c_sev = c_inj_agg + 10*c_k_agg\n", "plt.figure(figsize=(16,7))\n", "idx = [c_sev.index[i] for i in range(0,len(c_sev),2)]\n", "y = [c_sev[i] for i in range(0,len(c_sev),2)]\n", "x = np.arange(len(y))\n", "mod = smf.ols(formula='y ~ x', data = {'x': x, 'y': y}).fit()\n", "fit = mod.params[0] + x*mod.params[1]\n", "plt.plot(idx, fit, linestyle='--', lw=3, label='2nd half year fit')\n", "idx1 = [c_sev.index[i] for i in range(1,len(c_sev),2)]\n", "y1 = [c_sev[i] for i in range(1,len(c_sev),2)]\n", "x1 = np.arange(len(y1))\n", "mod1 = smf.ols(formula='y1 ~ x1', data = {'x1': x1, 'y1': y1}).fit()\n", "fit1 = mod1.params[0] + x1*mod1.params[1]\n", "plt.plot(idx1, fit1, linestyle='--', lw=3, label='1st half year fit')\n", "c_sev.plot(color='b', label='Number of accidents')\n", "plt.axvline(c_sev.index[3], linestyle='-.', color='r', label='Vision Zero')\n", "plt.axvline(c_sev.index[2], linestyle='-.', color='g', label='Citi Bike')\n", "plt.xlabel('1/2 years')\n", "plt.ylabel('Number of bikers injured+10*Number of bikers killed')\n", "plt.title('Weighted number of bicycle accidents over every half year')\n", "plt.legend(loc=0)"], []]}, {"NYPD_temporal_parser2.ipynb": [["import pandas as pd, numpy as np, matplotlib.pyplot as plt, datetime, statsmodels.formula.api as smf, matplotlib.dates as mdates\n", "%matplotlib inline\n", "plt.style.use('ggplot')"], ["def convertDate(d):\n", "    dt = datetime.datetime.strptime(d, '%m/%d/%Y')\n", "    dt = dt.replace(day=1)\n", "    return dt\n", "\n", "df = pd.read_csv('data/NYPD_Motor_Vehicle_Collisions.csv', converters={'DATE': convertDate})"], ["c_inj_agg = df.groupby('DATE').sum()['NUMBER OF CYCLIST INJURED']\n", "c_k_agg = df.groupby('DATE').sum()['NUMBER OF CYCLIST KILLED']"], ["plt.figure(figsize=(16,5))\n", "c_inj_agg.plot(color='b', label='Number of accidents')\n", "plt.axvline(c_inj_agg.index[18], linestyle='--', color='r', label='Vision Zero')\n", "plt.axvline(c_inj_agg.index[12], linestyle='--', color='g', label='Citi Bike')\n", "plt.ylabel('Number of bikers injured')\n", "plt.xlabel('Months')\n", "plt.title('Number of bikers injured over months')\n", "plt.legend(loc=0)\n", "plt.figure(figsize=(16,5))\n", "c_k_agg.plot(color='b', label='Number of accidents')\n", "plt.axvline(c_k_agg.index[18], linestyle='--', color='r', label='Vision Zero')\n", "plt.axvline(c_k_agg.index[12], linestyle='--', color='g', label='Citi Bike')\n", "plt.ylabel('Number of bikers killed')\n", "plt.xlabel('Months')\n", "plt.title('Number of bikers killed over months')\n", "plt.legend(loc=0)"], ["df['MONTH'] = df.apply(lambda x: x['DATE'].month, axis=1)\n", "months = df.groupby('DATE').mean()['MONTH']"], ["c_sev = c_inj_agg + 10*c_k_agg\n", "plt.figure(figsize=(16,5))\n", "data = {'y': c_sev, 'x': range(len(c_sev)), 'm': months}\n", "model = smf.ols(formula='y ~ x + C(m)', data = data).fit()\n", "print model.summary()\n", "fit = model.predict(data)\n", "plt.plot_date(c_sev.index, fit, 'r', linestyle='--', lw=2, label='Fitted model')\n", "c_sev.plot(color='b', label='Number of accidents')\n", "plt.axvline(c_sev.index[18], linestyle='-.', color='r', label='Vision Zero')\n", "plt.axvline(c_sev.index[12], linestyle='-.', color='g', label='Citi Bike')\n", "plt.ylabel('Number of bikers injured+10*Number of bikers killed')\n", "plt.xlabel('Months')\n", "plt.title('Weighted number of bicycle accidents over months')\n", "plt.legend(loc=0)"], ["plt.figure(figsize=(16,5))\n", "idx = [c_sev.index[i] for i in range(13)]\n", "y = [c_sev[i] for i in range(13)]\n", "x = np.arange(len(y))\n", "model = smf.ols(formula='y ~ x', data = {'x': x, 'y': y}).fit()\n", "fit = model.params[0] + x*model.params[1]\n", "plt.plot(idx, fit, linestyle='--', lw=2, label='Fit before Citi Bike')\n", "idx1 = [c_sev.index[i] for i in range(12,len(c_sev))]\n", "y1 = [c_sev[i] for i in range(12,len(c_sev))]\n", "x1 = np.arange(len(y1))\n", "model1 = smf.ols(formula='y ~ x', data = {'x': x1, 'y': y1}).fit()\n", "fit1 = model1.params[0] + x1*model1.params[1]\n", "plt.plot(idx1, fit1, linestyle='--', lw=2, label='Fit after Citi Bike')\n", "#idx2 = [c_sev.index[i] for i in range(18,len(c_sev))]\n", "#y2 = [c_sev[i] for i in range(18,len(c_sev))]\n", "#x2 = np.arange(len(y2))\n", "#model2 = smf.ols(formula='y ~ x', data = {'x': x2, 'y': y2}).fit()\n", "#fit2 = model2.params[0] + x2*model2.params[1]\n", "#plt.plot(idx2, fit2, linestyle='--', lw=2, label='Fit after Vision Zero')\n", "c_sev.plot(color='b', label='Number of accidents')\n", "plt.axvline(c_sev.index[18], linestyle='-.', color='r', label='Vision Zero')\n", "plt.axvline(c_sev.index[12], linestyle='-.', color='g', label='Citi Bike')\n", "plt.ylabel('Number of bikers injured+10*Number of bikers killed')\n", "plt.xlabel('Months')\n", "plt.title('Weighted number of bicycle accidents over months')\n", "plt.legend(loc=0)"], []]}], "vgprasanna/Airplane-Accident": [{"prediction-using-6-different-model.ipynb": [["1. LGBM\n", "2. Xgboost\n", "3. RandomForestClassifier\n", "4. Gradient Boosting classifier\n", "5. Extra Tree classifier\n", "6. Voting Classifier"], ["# If you think this notebook is worth reading and has gained some knowledge from this,please consider upvoting my kernel.Your appreciation means a lot to me"], ["# Import Required Module"], ["\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "df_train=pd.read_csv('../input/airplane-accident-dataset/train.csv')\n", "df_test=pd.read_csv('../input/airplane-accident-dataset/test.csv')\n", "df_train.head()"], ["obj=LabelEncoder()\n", "df_train['target']=obj.fit_transform(df_train['Severity'])\n", "df=pd.concat([df_train.drop(['Severity','target'],axis=1),df_test],axis=0,sort=False)\n", "df.head()"], ["As we know dataset does not contain any nan value so direct preprocess the data"], ["df['Total_Safety_Complaints']=pd.qcut(df['Total_Safety_Complaints'],3)\n", "df['Cabin_Temperature']=pd.qcut(df['Cabin_Temperature'],3)\n", "df['Violations']=df['Violations'].map({2:0,1:1,3:2,0:4,4:4,5:5})\n", "\n", "df['Adverse_Weather_Metric']=pd.qcut(df['Adverse_Weather_Metric'],3)\n", "\n", "df['Max_Elevation']=pd.qcut(df['Max_Elevation'],3)\n", "\n", "df['Turbulence_In_gforces']=pd.qcut(df['Turbulence_In_gforces'],3)"], ["from sklearn.preprocessing import LabelEncoder\n", "lbl=LabelEncoder()\n", "\n", "df['Total_Safety_Complaints']=lbl.fit_transform(df['Total_Safety_Complaints'])\n", "df['Cabin_Temperature']=lbl.fit_transform(df['Cabin_Temperature'])\n", "df['Max_Elevation']=lbl.fit_transform(df['Max_Elevation'])\n", "df['Turbulence_In_gforces']=lbl.fit_transform(df['Turbulence_In_gforces'])\n", "\n", "df['Adverse_Weather_Metric']=lbl.fit_transform(df['Adverse_Weather_Metric'])"], ["col1=['Safety_Score', 'Days_Since_Inspection', 'Total_Safety_Complaints',\n", "       'Accident_Type_Code','Control_Metric' ]\n", "col3=['Safety_Score', 'Days_Since_Inspection', 'Total_Safety_Complaints',\n", "       'Accident_Type_Code','Control_Metric','Turbulence_In_gforces', 'Cabin_Temperature',\n", "        'Violations',\n", "       'Adverse_Weather_Metric','Max_Elevation']\n", "obj=StandardScaler()\n", "df[col1]=obj.fit_transform(df[col1])\n", "obj1=MinMaxScaler()\n", "df[col3]=obj1.fit_transform(df[col3])"], ["column=['Safety_Score', 'Days_Since_Inspection', 'Total_Safety_Complaints',\n", "       'Accident_Type_Code','Control_Metric','Turbulence_In_gforces', 'Cabin_Temperature','Violations','Adverse_Weather_Metric',\n", "       'Max_Elevation']"], ["# LGBM"], ["X=df.iloc[0:10000,:][column]\n", "y=df_train['target']\n", "x=df.iloc[10000:12500,:][column]\n", "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=2020,test_size=0.25)\n", "import lightgbm as lgb\n", "from sklearn.model_selection import cross_val_score\n", "model = lgb.LGBMClassifier( learning_rate=0.2, n_estimators= 1000)\n", "result=cross_val_score(estimator=model,X=X_train,y=y_train,cv=10)\n", "print(result)\n", "print(result.mean())"], ["# Xgboost"], ["X=df.iloc[0:10000,:][column]\n", "y=df_train['target']\n", "x=df.iloc[10000:12500,:][column]\n", "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=2020,test_size=0.25)\n", "import xgboost as xgb\n", "from sklearn.model_selection import cross_val_score\n", "model1=xgb.XGBClassifier(colsample_bylevel= 1, learning_rate= 0.1,max_depth=10, n_estimators= 1000)\n", "result=cross_val_score(estimator=model1,X=X_train,y=y_train,cv=10)\n", "print(result)\n", "print(result.mean())"], ["model.fit(X,y)\n", "id=df_test['Accident_ID']\n", "y_pred=model.predict(x)\n", "submission=pd.DataFrame({'Accident_ID':id,'Severity':y_pred})\n", "submission.head()\n", "submission['Severity']=submission['Severity'].map({1:'Minor_Damage_And_Injuries',2:'Significant_Damage_And_Fatalities',3:'Significant_Damage_And_Serious_Injuries',0:'Highly_Fatal_And_Damaging'})\n", "#submission.to_csv('submission.csv',index=False)"], ["model1.fit(X,y)\n", "id=df_test['Accident_ID']\n", "y_pred1=model1.predict(x)\n", "submission1=pd.DataFrame({'Accident_ID':id,'Severity':y_pred1})\n", "submission1.head()\n", "submission1['Severity']=submission1['Severity'].map({1:'Minor_Damage_And_Injuries',2:'Significant_Damage_And_Fatalities',3:'Significant_Damage_And_Serious_Injuries',0:'Highly_Fatal_And_Damaging'})\n", "#submission1.to_csv('F:\\\\PYTHON PROGRAM\\\\JAISHREERAMhacker75.csv',index=False)"], ["indices=np.argsort(model1.feature_importances_)\n", "plt.figure(figsize=(10,10))\n", "g = sns.barplot(y=X_train.columns[indices][:40],x = model1.feature_importances_[indices][:40] , orient='h')"], ["from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve"], ["# Cross validate model with Kfold stratified cross val\n", "kfold = StratifiedKFold(n_splits=10)"], ["# Gradient Boosting Classifeir"], ["\n", "GBC = GradientBoostingClassifier()\n", "gb_param_grid = {'loss' : [\"deviance\"],\n", "              'n_estimators' : [400,500],\n", "              'learning_rate': [0.1, 0.2],\n", "              'max_depth': [4, 8],\n", "              'min_samples_leaf': [100,150],\n", "              'max_features': [0.3, 0.1] \n", "              }\n", "\n", "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n", "\n", "gsGBC.fit(X_train,y_train)\n", "\n", "model2 = gsGBC.best_estimator_\n", "\n", "# Best score\n", "print(gsGBC.best_score_)\n", "print(gsGBC.best_params_)"], ["model2 = gsGBC.best_estimator_"], ["model2.fit(X,y)\n", "id=df_test['Accident_ID']\n", "y_pred2=model2.predict(x)\n", "submission2=pd.DataFrame({'Accident_ID':id,'Severity':y_pred2})\n", "submission2.head()\n", "submission2['Severity']=submission2['Severity'].map({1:'Minor_Damage_And_Injuries',2:'Significant_Damage_And_Fatalities',3:'Significant_Damage_And_Serious_Injuries',0:'Highly_Fatal_And_Damaging'})\n", "#submission2.to_csv('F:\\\\PYTHON PROGRAM\\\\JAISHREERAMhacker36.csv',index=False)"], ["# randomForest Classifier"], ["# RFC Parameters tunning \n", "RFC = RandomForestClassifier()\n", "\n", "\n", "## Search grid for optimal parameters\n", "rf_param_grid = {\n", "              \"max_features\": [1, 3, 10],\n", "              \"min_samples_split\": [2, 3, 10],\n", "              \"min_samples_leaf\": [1, 3, 10],\n", "            \n", "              \"n_estimators\" :[400,500,1000],\n", "              \"criterion\": [\"gini\"]}\n", "\n", "\n", "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n", "\n", "gsRFC.fit(X_train,y_train)\n", "\n", "model3 = gsRFC.best_estimator_\n", "\n", "# Best score\n", "print(gsRFC.best_score_)\n", "print(gsRFC.best_params_)"], ["model3.fit(X,y)\n", "id=df_test['Accident_ID']\n", "y_pred3=model3.predict(x)\n", "submission3=pd.DataFrame({'Accident_ID':id,'Severity':y_pred3})\n", "submission3.head()\n", "submission3['Severity']=submission3['Severity'].map({1:'Minor_Damage_And_Injuries',2:'Significant_Damage_And_Fatalities',3:'Significant_Damage_And_Serious_Injuries',0:'Highly_Fatal_And_Damaging'})\n", "#submission3.to_csv('F:\\\\PYTHON PROGRAM\\\\JAISHREERAMhacker57.csv',index=False)"], ["# Extra Tree classifeir"], ["#ExtraTrees \n", "ExtC = ExtraTreesClassifier()\n", "\n", "\n", "## Search grid for optimal parameters\n", "ex_param_grid = {\"max_depth\": [None],\n", "              \"max_features\": [1, 3, 10],\n", "              \"min_samples_split\": [2, 3, 10],\n", "              \"min_samples_leaf\": [1, 3, 10],\n", "              \"bootstrap\": [False],\n", "              \"n_estimators\" :[400,500],\n", "              \"criterion\": [\"gini\"]}\n", "\n", "\n", "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n", "\n", "gsExtC.fit(X_train,y_train)\n", "\n", "model5 = gsExtC.best_estimator_\n", "\n", "# Best score\n", "print(gsExtC.best_score_)\n", "print(gsExtC.best_params_)"], ["model5.fit(X,y)\n", "id=df_test['Accident_ID']\n", "y_pred5=model5.predict(x)\n", "submission5=pd.DataFrame({'Accident_ID':id,'Severity':y_pred5})\n", "submission5.head()\n", "submission5['Severity']=submission5['Severity'].map({1:'Minor_Damage_And_Injuries',2:'Significant_Damage_And_Fatalities',3:'Significant_Damage_And_Serious_Injuries',0:'Highly_Fatal_And_Damaging'})\n", "#submission5.to_csv('F:\\\\PYTHON PROGRAM\\\\JAISHREERAMhacker36.csv',index=False)"], ["# Voting Classifier "], ["You can try with both hard and soft voting"], ["model = lgb.LGBMClassifier( learning_rate=0.2, n_estimators= 500,max_depth=10)\n", "model1=xgb.XGBClassifier(colsample_bylevel= 1, learning_rate= 0.1, max_depth= 10, n_estimators= 400)\n", "model2 = GradientBoostingClassifier(learning_rate= 0.2, loss= 'deviance', max_depth= 8, max_features =0.3, min_samples_leaf= 100, n_estimators= 500)\n", "model3 = RandomForestClassifier(criterion= 'gini', min_samples_leaf= 1, min_samples_split= 3, n_estimators= 500)"], ["from sklearn.ensemble import VotingClassifier\n", "votingC = VotingClassifier(estimators=[('gbc',model2),('rfc',model3),('xgb',model1)], voting='soft', n_jobs=4)\n", "votingC.fit(X,y)\n", "id=df_test['Accident_ID']\n", "y_pred2=votingC.predict(x)\n", "submission=pd.DataFrame({'Accident_ID':id,'Severity':y_pred2})\n", "submission.head()\n", "submission['Severity']=submission['Severity'].map({1:'Minor_Damage_And_Injuries',2:'Significant_Damage_And_Fatalities',3:'Significant_Damage_And_Serious_Injuries',0:'Highly_Fatal_And_Damaging'})\n", "#submission.to_csv('F:\\\\PYTHON PROGRAM\\\\JAISHREERAMhacker72.csv',index=False)"], ["# If you like my kernel please consider upvoting it\n", "\n", "# Don't hesitate to give your suggestions in the comment section\n", "\n", "# Thank you..."]]}], "greenwoodgiant/speeding-in-texas": [{"database_creation_census.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Create OS paths for each data set\n", "cen2010 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_10_5YR_DP05_with_ann.csv\")\n", "cen2011 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_11_5YR_DP05_with_ann.csv\")\n", "cen2012 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_12_5YR_DP05_with_ann.csv\")\n", "cen2013 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_13_5YR_DP05_with_ann.csv\")\n", "cen2014 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_14_5YR_DP05_with_ann.csv\")\n", "cen2015 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_15_5YR_DP05_with_ann.csv\")\n", "cen2016 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_16_5YR_DP05_with_ann.csv\")\n"], ["# Create data frames for each data set via read_csv\n", "cen2010_df = pd.read_csv(cen2010, encoding = \"utf-8\")\n", "cen2011_df = pd.read_csv(cen2011, encoding = \"utf-8\")\n", "cen2012_df = pd.read_csv(cen2012, encoding = \"utf-8\")\n", "cen2013_df = pd.read_csv(cen2013, encoding = \"utf-8\")\n", "cen2014_df = pd.read_csv(cen2014, encoding = \"utf-8\")\n", "cen2015_df = pd.read_csv(cen2015, encoding = \"utf-8\")\n", "cen2016_df = pd.read_csv(cen2016, encoding = \"utf-8\")\n", "\n"], ["cen2014_df"], ["# Drop columns with 'Margins of Error' and 'Percent', since df's are only supposed to contain estimates\n", "# Rename columns to be ORM table friendly and simplification\n", "\n", "# df_list = [cen2010_df, cen2011_df, cen2012_df, cen2013_df, cen2014_df, cen2015_df, cen2016_df]\n", "\n", "# for df in kdf_list:\n", "#     df = df.drop(df.filter(like='Margin').columns, 1)\n", "#     df = df.drop(df.filter(like='Percent').columns, 1)\n", "#     df = df.drop(df.filter(like='Id').columns, 1)\n", "\n", "# cen2010_df\n", "\n", "cen2010_df = cen2010_df.drop(cen2010_df.filter(like='Margin').columns, 1)\n", "cen2010_df = cen2010_df.drop(cen2010_df.filter(like='Percent').columns, 1)\n", "cen2010_df = cen2010_df.drop(cen2010_df.filter(like='Id').columns, 1)\n", "\n", "cen2010_df = cen2010_df.rename(columns = {cen2010_df.columns[0]: 'year', cen2010_df.columns[1]: 'total_population',\n", "                                         cen2010_df.columns[2]: 'male', cen2010_df.columns[3]: 'female',\n", "                                         cen2010_df.columns[4]: 'age_under_5', cen2010_df.columns[5]: 'age_5_to_9',\n", "                                         cen2010_df.columns[6]: 'age_10_to_14', cen2010_df.columns[7]: 'age_15_to_19',\n", "                                         cen2010_df.columns[8]: 'age_20_to_24', cen2010_df.columns[9]: 'age_25_to_34',\n", "                                         cen2010_df.columns[10]: 'age_35_to_44', cen2010_df.columns[11]: 'age_45_to_54',\n", "                                         cen2010_df.columns[12]: 'age_55_to_60', cen2010_df.columns[13]: 'age_60_to_64',\n", "                                         cen2010_df.columns[14]: 'age_65_to_74', cen2010_df.columns[15]: 'age_75_to_84',\n", "                                         cen2010_df.columns[16]: 'age_85_and_over', cen2010_df.columns[17]: 'white',\n", "                                         cen2010_df.columns[18]: 'black', cen2010_df.columns[19]: 'native_american',\n", "                                          cen2010_df.columns[20]: 'asian', cen2010_df.columns[21]: 'pacific_islander',\n", "                                          cen2010_df.columns[22]: 'other_race', cen2010_df.columns[23]: 'hispanic'})\n", "\n", "# Change the 'geography' column to year column. Assign the year as the row value\n", "cen2010_df[\"year\"] = 2010\n", "\n", "cen2011_df = cen2011_df.drop(cen2011_df.filter(like='Margin').columns, 1)\n", "cen2011_df = cen2011_df.drop(cen2011_df.filter(like='Percent').columns, 1)\n", "cen2011_df = cen2011_df.drop(cen2011_df.filter(like='Id').columns, 1)\n", "\n", "cen2011_df = cen2011_df.rename(columns = {cen2011_df.columns[0]: 'year', cen2011_df.columns[1]: 'total_population',\n", "                                         cen2011_df.columns[2]: 'male', cen2011_df.columns[3]: 'female',\n", "                                         cen2011_df.columns[4]: 'age_under_5', cen2011_df.columns[5]: 'age_5_to_9',\n", "                                         cen2011_df.columns[6]: 'age_10_to_14', cen2011_df.columns[7]: 'age_15_to_19',\n", "                                         cen2011_df.columns[8]: 'age_20_to_24', cen2011_df.columns[9]: 'age_25_to_34',\n", "                                         cen2011_df.columns[10]: 'age_35_to_44', cen2011_df.columns[11]: 'age_45_to_54',\n", "                                         cen2011_df.columns[12]: 'age_55_to_60', cen2011_df.columns[13]: 'age_60_to_64',\n", "                                         cen2011_df.columns[14]: 'age_65_to_74', cen2011_df.columns[15]: 'age_75_to_84',\n", "                                         cen2011_df.columns[16]: 'age_85_and_over', cen2011_df.columns[17]: 'white',\n", "                                         cen2011_df.columns[18]: 'black', cen2011_df.columns[19]: 'native_american',\n", "                                          cen2011_df.columns[20]: 'asian', cen2011_df.columns[21]: 'pacific_islander',\n", "                                          cen2011_df.columns[22]: 'other_race', cen2011_df.columns[23]: 'hispanic'})\n", "\n", "# Change the 'geography' column to year column. Assign the year as the row value\n", "cen2011_df[\"year\"] = 2011\n", "\n", "cen2012_df = cen2012_df.drop(cen2012_df.filter(like='Margin').columns, 1)\n", "cen2012_df = cen2012_df.drop(cen2012_df.filter(like='Percent').columns, 1)\n", "cen2012_df = cen2012_df.drop(cen2012_df.filter(like='Id').columns, 1)\n", "\n", "cen2012_df = cen2012_df.rename(columns = {cen2012_df.columns[0]: 'year', cen2012_df.columns[1]: 'total_population',\n", "                                         cen2012_df.columns[2]: 'male', cen2012_df.columns[3]: 'female',\n", "                                         cen2012_df.columns[4]: 'age_under_5', cen2012_df.columns[5]: 'age_5_to_9',\n", "                                         cen2012_df.columns[6]: 'age_10_to_14', cen2012_df.columns[7]: 'age_15_to_19',\n", "                                         cen2012_df.columns[8]: 'age_20_to_24', cen2012_df.columns[9]: 'age_25_to_34',\n", "                                         cen2012_df.columns[10]: 'age_35_to_44', cen2012_df.columns[11]: 'age_45_to_54',\n", "                                         cen2012_df.columns[12]: 'age_55_to_60', cen2012_df.columns[13]: 'age_60_to_64',\n", "                                         cen2012_df.columns[14]: 'age_65_to_74', cen2012_df.columns[15]: 'age_75_to_84',\n", "                                         cen2012_df.columns[16]: 'age_85_and_over', cen2012_df.columns[17]: 'white',\n", "                                         cen2012_df.columns[18]: 'black', cen2012_df.columns[19]: 'native_american',\n", "                                          cen2012_df.columns[20]: 'asian', cen2012_df.columns[21]: 'pacific_islander',\n", "                                          cen2012_df.columns[22]: 'other_race', cen2012_df.columns[23]: 'hispanic'})\n", "\n", "cen2012_df[\"year\"] = 2012\n", "\n", "cen2013_df = cen2013_df.drop(cen2013_df.filter(like='Margin').columns, 1)\n", "cen2013_df = cen2013_df.drop(cen2013_df.filter(like='Percent').columns, 1)\n", "cen2013_df = cen2013_df.drop(cen2013_df.filter(like='Id').columns, 1)\n", "\n", "cen2013_df = cen2013_df.rename(columns = {cen2013_df.columns[0]: 'year', cen2013_df.columns[1]: 'total_population',\n", "                                         cen2013_df.columns[2]: 'male', cen2013_df.columns[3]: 'female',\n", "                                         cen2013_df.columns[4]: 'age_under_5', cen2013_df.columns[5]: 'age_5_to_9',\n", "                                         cen2013_df.columns[6]: 'age_10_to_14', cen2013_df.columns[7]: 'age_15_to_19',\n", "                                         cen2013_df.columns[8]: 'age_20_to_24', cen2013_df.columns[9]: 'age_25_to_34',\n", "                                         cen2013_df.columns[10]: 'age_35_to_44', cen2013_df.columns[11]: 'age_45_to_54',\n", "                                         cen2013_df.columns[12]: 'age_55_to_60', cen2013_df.columns[13]: 'age_60_to_64',\n", "                                         cen2013_df.columns[14]: 'age_65_to_74', cen2013_df.columns[15]: 'age_75_to_84',\n", "                                         cen2013_df.columns[16]: 'age_85_and_over', cen2013_df.columns[17]: 'white',\n", "                                         cen2013_df.columns[18]: 'black', cen2013_df.columns[19]: 'native_american',\n", "                                          cen2013_df.columns[20]: 'asian', cen2013_df.columns[21]: 'pacific_islander',\n", "                                          cen2013_df.columns[22]: 'other_race', cen2013_df.columns[23]: 'hispanic'})\n", "\n", "cen2013_df[\"year\"] = 2013\n", "\n", "cen2014_df = cen2014_df.drop(cen2014_df.filter(like='Margin').columns, 1)\n", "cen2014_df = cen2014_df.drop(cen2014_df.filter(like='Percent').columns, 1)\n", "cen2014_df = cen2014_df.drop(cen2014_df.filter(like='Id').columns, 1)\n", "\n", "cen2014_df = cen2014_df.rename(columns = {cen2014_df.columns[0]: 'year', cen2014_df.columns[1]: 'total_population',\n", "                                         cen2014_df.columns[2]: 'male', cen2014_df.columns[3]: 'female',\n", "                                         cen2014_df.columns[4]: 'age_under_5', cen2014_df.columns[5]: 'age_5_to_9',\n", "                                         cen2014_df.columns[6]: 'age_10_to_14', cen2014_df.columns[7]: 'age_15_to_19',\n", "                                         cen2014_df.columns[8]: 'age_20_to_24', cen2014_df.columns[9]: 'age_25_to_34',\n", "                                         cen2014_df.columns[10]: 'age_35_to_44', cen2014_df.columns[11]: 'age_45_to_54',\n", "                                         cen2014_df.columns[12]: 'age_55_to_60', cen2014_df.columns[13]: 'age_60_to_64',\n", "                                         cen2014_df.columns[14]: 'age_65_to_74', cen2014_df.columns[15]: 'age_75_to_84',\n", "                                         cen2014_df.columns[16]: 'age_85_and_over', cen2014_df.columns[17]: 'white',\n", "                                         cen2014_df.columns[18]: 'black', cen2014_df.columns[19]: 'native_american',\n", "                                          cen2014_df.columns[20]: 'asian', cen2014_df.columns[21]: 'pacific_islander',\n", "                                          cen2014_df.columns[22]: 'other_race', cen2014_df.columns[23]: 'hispanic'})\n", "\n", "cen2014_df[\"year\"] = 2014\n", "\n", "cen2015_df = cen2015_df.drop(cen2015_df.filter(like='Margin').columns, 1)\n", "cen2015_df = cen2015_df.drop(cen2015_df.filter(like='Percent').columns, 1)\n", "cen2015_df = cen2015_df.drop(cen2015_df.filter(like='Id').columns, 1)\n", "\n", "cen2015_df = cen2015_df.rename(columns = {cen2015_df.columns[0]: 'year', cen2015_df.columns[1]: 'total_population',\n", "                                         cen2015_df.columns[2]: 'male', cen2015_df.columns[3]: 'female',\n", "                                         cen2015_df.columns[4]: 'age_under_5', cen2015_df.columns[5]: 'age_5_to_9',\n", "                                         cen2015_df.columns[6]: 'age_10_to_14', cen2015_df.columns[7]: 'age_15_to_19',\n", "                                         cen2015_df.columns[8]: 'age_20_to_24', cen2015_df.columns[9]: 'age_25_to_34',\n", "                                         cen2015_df.columns[10]: 'age_35_to_44', cen2015_df.columns[11]: 'age_45_to_54',\n", "                                         cen2015_df.columns[12]: 'age_55_to_60', cen2015_df.columns[13]: 'age_60_to_64',\n", "                                         cen2015_df.columns[14]: 'age_65_to_74', cen2015_df.columns[15]: 'age_75_to_84',\n", "                                         cen2015_df.columns[16]: 'age_85_and_over', cen2015_df.columns[17]: 'white',\n", "                                         cen2015_df.columns[18]: 'black', cen2015_df.columns[19]: 'native_american',\n", "                                          cen2015_df.columns[20]: 'asian', cen2015_df.columns[21]: 'pacific_islander',\n", "                                          cen2015_df.columns[22]: 'other_race', cen2015_df.columns[23]: 'hispanic'})\n", "\n", "cen2015_df[\"year\"] = 2015\n", "\n", "cen2016_df = cen2016_df.drop(cen2016_df.filter(like='Margin').columns, 1)\n", "cen2016_df = cen2016_df.drop(cen2016_df.filter(like='Percent').columns, 1)\n", "cen2016_df = cen2016_df.drop(cen2016_df.filter(like='Id').columns, 1)\n", "\n", "cen2016_df = cen2016_df.rename(columns = {cen2016_df.columns[0]: 'year', cen2016_df.columns[1]: 'total_population',\n", "                                         cen2016_df.columns[2]: 'male', cen2016_df.columns[3]: 'female',\n", "                                         cen2016_df.columns[4]: 'age_under_5', cen2016_df.columns[5]: 'age_5_to_9',\n", "                                         cen2016_df.columns[6]: 'age_10_to_14', cen2016_df.columns[7]: 'age_15_to_19',\n", "                                         cen2016_df.columns[8]: 'age_20_to_24', cen2016_df.columns[9]: 'age_25_to_34',\n", "                                         cen2016_df.columns[10]: 'age_35_to_44', cen2016_df.columns[11]: 'age_45_to_54',\n", "                                         cen2016_df.columns[12]: 'age_55_to_60', cen2016_df.columns[13]: 'age_60_to_64',\n", "                                         cen2016_df.columns[14]: 'age_65_to_74', cen2016_df.columns[15]: 'age_75_to_84',\n", "                                         cen2016_df.columns[16]: 'age_85_and_over', cen2016_df.columns[17]: 'white',\n", "                                         cen2016_df.columns[18]: 'black', cen2016_df.columns[19]: 'native_american',\n", "                                          cen2016_df.columns[20]: 'asian', cen2016_df.columns[21]: 'pacific_islander',\n", "                                          cen2016_df.columns[22]: 'other_race', cen2016_df.columns[23]: 'hispanic'})\n", "\n", "cen2016_df[\"year\"] = 2016\n", "\n", "cen2016_df"], ["# Append all the data sets into a single data set\n", "census_data_df = cen2010_df.append(cen2011_df)\n", "census_data_df = census_data_df.append(cen2012_df)\n", "census_data_df = census_data_df.append(cen2013_df)\n", "census_data_df = census_data_df.append(cen2014_df)\n", "census_data_df = census_data_df.append(cen2015_df)\n", "census_data_df = census_data_df.append(cen2016_df)\n", "\n", "census_data_df.head(10)"], ["# Reset the index for the census df\n", "census_data_df = census_data_df.reset_index(drop=True)\n", "census_data_df"], ["## Database engineering\n", "\n", "# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey, BigInteger\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["# Create Engine\n", "engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["# Use `declarative_base` from SQLAlchemy to model the 'crashes' table as an ORM class\n", "# Declare a Base object here\n", "Base = declarative_base()"], ["# Define the ORM class for `Demographics`\n", "class Demographics(Base):\n", "    \n", "    __tablename__ = 'demographics'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Integer)\n", "    total_population = Column(Integer)\n", "    male = Column(Integer)\n", "    female = Column(Integer)\n", "    age_under_5 = Column(Integer)\n", "    age_5_to_9 = Column(Integer)\n", "    age_10_to_14 = Column(Integer)                                \n", "    age_15_to_19 = Column(Integer)\n", "    age_20_to_24 = Column(Integer)\n", "    age_25_to_34 = Column(Integer)\n", "    age_35_to_44 = Column(Integer)\n", "    age_45_to_54 = Column(Integer)\n", "    age_55_to_60 = Column(Integer)\n", "    age_60_to_64 = Column(Integer)\n", "    age_65_to_74 = Column(Integer)\n", "    age_75_to_84 = Column(Integer)\n", "    age_85_and_over = Column(Integer)\n", "    white = Column(Integer)\n", "    black = Column(Integer)\n", "    native_american = Column(Integer)\n", "    asian = Column(Integer)\n", "    pacific_islander = Column(Integer)\n", "    other_race = Column(Integer)\n", "    hispanic = Column(Integer)\n", "            \n", "        \n", "    def __repr__(self):\n", "        return f\"id={self.crash_id}, name={self.year}\""], ["# Use `create_all` to create the tables\n", "Base.metadata.create_all(engine)"], ["# Verify that the table names exist in the database\n", "engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('demographics')"], ["census_data_df.astype(np.int8).dtypes"], ["pandas_records = census_data_df.to_dict(orient='records')"], ["len(pandas_records)"], ["fixed_records = []\n", "\n", "for item in pandas_records:\n", "    fixed_records.append({ key: int(value) for key, value in item.items() })"], ["# Use Pandas to Bulk insert each data frame into their appropriate table\n", "def populate_table(engine, table):\n", "    \"\"\"Populates a table from a Pandas DataFrame.\"\"\"\n", "    \n", "    # connect to the database\n", "    conn = engine.connect()\n", "    \n", "    # Orient='records' creates a list of data to write\n", "    data = census_data_df.to_dict(orient='records')\n", "\n", "    # Optional: Delete all rows in the table \n", "    conn.execute(table.delete())\n", "\n", "    # Insert the dataframe into the database in one bulk insert\n", "    conn.execute(table.insert(), fixed_records)\n", "    conn.close()\n", "    \n", "# Call the function to insert the data for each table\n", "populate_table(engine, Demographics.__table__)"], ["# Use a basic query to validate that the data was inserted correctly for table `demographics`\n", "engine.execute(\"SELECT * FROM demographics LIMIT 7\").fetchall()"], [], []]}, {"database_creation_crashes.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Create OS paths for each data set\n", "y2010 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2010 Speed Related Crashes Data.csv\")\n", "y2011 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2011 Speed Related Crashes Data.csv\")\n", "y2012 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2012 Speed Related Crashes Data.csv\")\n", "y2013 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2013 Speed Related Crashes Data.csv\")\n", "y2014 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2014 Speed Related Crashes Data.csv\")\n", "y2015 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2015 Speed Related Crashes Data.csv\")\n", "y2016 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2016 Speed Related Crashes Data.csv\")\n", "y2017 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2017 Speed Related Crashes Data.csv\")\n", "y2018 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2018 Speed Related Crashes Data.csv\")\n"], ["# Create data frames for each data set via read_csv\n", "y2010_df = pd.read_csv(y2010, encoding = \"utf-8\", low_memory = False)\n", "y2011_df = pd.read_csv(y2011, encoding = \"utf-8\", low_memory = False)\n", "y2012_df = pd.read_csv(y2012, encoding = \"utf-8\", low_memory = False)\n", "y2013_df = pd.read_csv(y2013, encoding = \"utf-8\", low_memory = False)\n", "y2014_df = pd.read_csv(y2014, encoding = \"utf-8\", low_memory = False)\n", "y2015_df = pd.read_csv(y2015, encoding = \"utf-8\", low_memory = False)\n", "y2016_df = pd.read_csv(y2016, encoding = \"utf-8\", low_memory = False)\n", "y2017_df = pd.read_csv(y2017, encoding = \"utf-8\", low_memory = False)\n", "y2018_df = pd.read_csv(y2018, encoding = \"utf-8\", low_memory = False)"], ["y2018_df.head()"], ["# Append all the data sets into a single data set\n", "speedlimit_crash_df = y2010_df.append(y2011_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2012_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2013_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2014_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2015_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2016_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2017_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2018_df)\n", "speedlimit_crash_df.head()"], ["# Drop rows from teh data set with duplicate Crash ID's\n", "speedlimit_crash_df = speedlimit_crash_df.drop_duplicates(['Crash ID'], keep='first')\n", "speedlimit_crash_df = speedlimit_crash_df.reset_index(drop=True)\n", "speedlimit_crash_df.head(2)"], ["# Rename column names to be ORM class/table friendly\n", "speedlimit_crash_df = speedlimit_crash_df.rename(columns={'Crash ID': 'crash_id','Crash Death Count': 'crash_death_count', 'Crash Severity': 'crash_severity', 'Crash Time': 'crash_time', 'Crash Total Injury Count': 'crash_total_injury_count', 'Crash Year': 'crash_year', 'Day of Week': 'day_of_week', 'Manner of Collision': 'manner_of_collision', 'Population Group': 'population_group', 'Road Class': 'road_class', 'Speed Limit': 'speed_limit', 'Weather Condition': 'weather_condition', 'Vehicle Color': 'vehicle_color', 'Person Age': 'person_age', 'Person Ethnicity': 'person_ethnicity', 'Person Gender': 'person_gender', 'Person Type': 'person_type'})\n", "\n", "# Turn all columns headers to lowercase\n", "speedlimit_crash_df.columns = speedlimit_crash_df.columns.str.lower()\n", "\n", "speedlimit_crash_df.head(20)"], ["# Replace \n", "for column in speedlimit_crash_df.columns[1:]:\n", "    if (column == \"crash_death_count\") or (column == \"crash_time\") or (column == \"crash_total_injury_count\") or (column == \"crash_year\") or (column == \"latitude\") or (column == \"longitude\") or (column == \"speed_limit\") or (column == \"person_age\"):\n", "         speedlimit_crash_df[column] = speedlimit_crash_df[column].replace(\"No Data\", 0)\n", "    else:\n", "        speedlimit_crash_df[column] = speedlimit_crash_df[column].replace(\"No Data\", \"\")\n", "\n", "#speedlimit_crash_df.iloc[:,0:11]\n", "speedlimit_crash_df.head()"], ["# Number of rows in the df\n", "len(speedlimit_crash_df.index)"], ["# Create new data frame with crash totals by gender\n", "\n", "by_gender = y2010_df[y2010_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender = by_gender[by_gender[\"Person Gender\"] != \"No Data\"]\n", "by_gender = by_gender.groupby(\"Person Gender\").count()\n", "by_gender = by_gender[[\"Crash ID\"]]\n", "by_gender.columns = [\"crashes_2010\"]\n", "\n", "by_gender11 = y2011_df[y2011_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender11 = by_gender11[by_gender11[\"Person Gender\"] != \"No Data\"]\n", "by_gender11 = by_gender11.groupby(\"Person Gender\").count()\n", "by_gender11 = by_gender11[[\"Crash ID\"]]\n", "by_gender11.columns = [\"crashes_2011\"]\n", "\n", "by_gender12 = y2012_df[y2012_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender12 = by_gender12[by_gender12[\"Person Gender\"] != \"No Data\"]\n", "by_gender12 = by_gender12.groupby(\"Person Gender\").count()\n", "by_gender12 = by_gender12[[\"Crash ID\"]]\n", "by_gender12.columns = [\"crashes_2012\"]\n", "\n", "by_gender13 = y2013_df[y2013_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender13 = by_gender13[by_gender13[\"Person Gender\"] != \"No Data\"]\n", "by_gender13 = by_gender13.groupby(\"Person Gender\").count()\n", "by_gender13 = by_gender13[[\"Crash ID\"]]\n", "by_gender13.columns = [\"crashes_2013\"]\n", "\n", "by_gender14 = y2014_df[y2014_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender14 = by_gender14[by_gender14[\"Person Gender\"] != \"No Data\"]\n", "by_gender14 = by_gender14.groupby(\"Person Gender\").count()\n", "by_gender14 = by_gender14[[\"Crash ID\"]]\n", "by_gender14.columns = [\"crashes_2014\"]\n", "\n", "by_gender15 = y2015_df[y2015_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender15 = by_gender15[by_gender15[\"Person Gender\"] != \"No Data\"]\n", "by_gender15 = by_gender15.groupby(\"Person Gender\").count()\n", "by_gender15 = by_gender15[[\"Crash ID\"]]\n", "by_gender15.columns = [\"crashes_2015\"]\n", "\n", "by_gender16 = y2016_df[y2016_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender16 = by_gender16[by_gender16[\"Person Gender\"] != \"No Data\"]\n", "by_gender16 = by_gender16.groupby(\"Person Gender\").count()\n", "by_gender16 = by_gender16[[\"Crash ID\"]]\n", "by_gender16.columns = [\"crashes_2016\"]\n", "\n", "by_gender17 = y2017_df[y2017_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender17 = by_gender17[by_gender17[\"Person Gender\"] != \"No Data\"]\n", "by_gender17 = by_gender17.groupby(\"Person Gender\").count()\n", "by_gender17 = by_gender17[[\"Crash ID\"]]\n", "by_gender17.columns = [\"crashes_2017\"]\n", "\n", "by_gender18 = y2018_df[y2018_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender18 = by_gender18[by_gender18[\"Person Gender\"] != \"No Data\"]\n", "by_gender18 = by_gender18.groupby(\"Person Gender\").count()\n", "by_gender18 = by_gender18[[\"Crash ID\"]]\n", "by_gender18.columns = [\"crashes_2018\"]\n", "\n", "by_gender16"], ["# Joining dataframes together\n", "by_gender[\"crashes_2011\"] = by_gender11[\"crashes_2011\"]\n", "by_gender[\"crashes_2012\"] = by_gender12[\"crashes_2012\"]\n", "by_gender[\"crashes_2013\"] = by_gender13[\"crashes_2013\"]\n", "by_gender[\"crashes_2014\"] = by_gender14[\"crashes_2014\"]\n", "by_gender[\"crashes_2015\"] = by_gender15[\"crashes_2015\"]\n", "by_gender[\"crashes_2016\"] = by_gender16[\"crashes_2016\"]\n", "by_gender[\"crashes_2017\"] = by_gender17[\"crashes_2017\"]\n", "by_gender[\"crashes_2018\"] = by_gender18[\"crashes_2018\"]\n", "\n", "# Reset index and rename for clarity\n", "by_gender.reset_index(inplace=True)\n", "by_gender.head()"], ["by_gender = by_gender.transpose()\n", "by_gender.reset_index(inplace=True)\n", "by_gender.columns = [\"year\", \"female\", \"male\"]\n", "by_gender = by_gender.drop(by_gender.index[0])\n", "by_gender[\"year\"] = by_gender[\"year\"].map(lambda x: str(x)[8:])\n", "by_gender"], ["# Rename the df\n", "total_crashes_by_gender = by_gender"], ["# Same df creation process with grouping by race\n", "by_race = y2010_df[y2010_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race = by_race[by_race[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race = by_race.groupby(\"Person Ethnicity\").count()\n", "by_race = by_race[[\"Crash ID\"]]\n", "by_race.columns = [\"crashes_2010\"]\n", "\n", "by_race11 = y2011_df[y2011_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race11 = by_race11[by_race11[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race11 = by_race11.groupby(\"Person Ethnicity\").count()\n", "by_race11 = by_race11[[\"Crash ID\"]]\n", "by_race11.columns = [\"crashes_2011\"]\n", "\n", "by_race12 = y2012_df[y2012_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race12 = by_race12[by_race12[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race12 = by_race12.groupby(\"Person Ethnicity\").count()\n", "by_race12 = by_race12[[\"Crash ID\"]]\n", "by_race12.columns = [\"crashes_2012\"]\n", "\n", "by_race13 = y2013_df[y2013_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race13 = by_race13[by_race13[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race13 = by_race13.groupby(\"Person Ethnicity\").count()\n", "by_race13 = by_race13[[\"Crash ID\"]]\n", "by_race13.columns = [\"crashes_2013\"]\n", "\n", "by_race14 = y2014_df[y2014_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race14 = by_race14[by_race14[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race14 = by_race14.groupby(\"Person Ethnicity\").count()\n", "by_race14 = by_race14[[\"Crash ID\"]]\n", "by_race14.columns = [\"crashes_2014\"]\n", "\n", "by_race15 = y2015_df[y2015_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race15 = by_race15[by_race15[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race15 = by_race15.groupby(\"Person Ethnicity\").count()\n", "by_race15 = by_race15[[\"Crash ID\"]]\n", "by_race15.columns = [\"crashes_2015\"]\n", "\n", "by_race16 = y2016_df[y2016_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race16 = by_race16[by_race16[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race16 = by_race16.groupby(\"Person Ethnicity\").count()\n", "by_race16 = by_race16[[\"Crash ID\"]]\n", "by_race16.columns = [\"crashes_2016\"]\n", "\n", "by_race17 = y2017_df[y2017_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race17 = by_race17[by_race17[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race17 = by_race17.groupby(\"Person Ethnicity\").count()\n", "by_race17 = by_race17[[\"Crash ID\"]]\n", "by_race17.columns = [\"crashes_2017\"]\n", "\n", "by_race18 = y2018_df[y2018_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race18 = by_race18[by_race18[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race18 = by_race18.groupby(\"Person Ethnicity\").count()\n", "by_race18 = by_race18[[\"Crash ID\"]]\n", "by_race18.columns = [\"crashes_2018\"]\n", "\n", "\n", "by_race16"], ["# Joining dataframes together\n", "by_race[\"crashes_2011\"] = by_race11[\"crashes_2011\"]\n", "by_race[\"crashes_2012\"] = by_race12[\"crashes_2012\"]\n", "by_race[\"crashes_2013\"] = by_race13[\"crashes_2013\"]\n", "by_race[\"crashes_2014\"] = by_race14[\"crashes_2014\"]\n", "by_race[\"crashes_2015\"] = by_race15[\"crashes_2015\"]\n", "by_race[\"crashes_2016\"] = by_race16[\"crashes_2016\"]\n", "by_race[\"crashes_2017\"] = by_race17[\"crashes_2017\"]\n", "by_race[\"crashes_2018\"] = by_race18[\"crashes_2018\"]\n", "\n", "# Reset index and rename for clarity\n", "by_race.reset_index(inplace=True)\n", "by_race"], ["by_race = by_race.transpose()\n", "by_race.reset_index(inplace=True)\n", "by_race.columns = [\"year\", \"native_american\", \"asian\", \"black\", \"hispanic\", \"other\", \"white\"]\n", "by_race = by_race.drop(by_race.index[0])\n", "by_race[\"year\"] = by_race[\"year\"].map(lambda x: str(x)[8:])\n", "by_race"], ["# Rename df\n", "total_crashes_by_race = by_race"], ["# Same df creation process with grouping by race\n", "by_county = y2010_df[y2010_df[\"County\"] != \"Unknown\"]\n", "by_county = by_county[by_county[\"County\"] != \"No Data\"]\n", "by_county = by_county.groupby(\"County\").count()\n", "by_county = by_county[[\"Crash ID\"]]\n", "by_county.columns = [\"crashes_2010\"]\n", "\n", "by_county11 = y2011_df[y2011_df[\"County\"] != \"Unknown\"]\n", "by_county11 = by_county11[by_county11[\"County\"] != \"No Data\"]\n", "by_county11 = by_county11.groupby(\"County\").count()\n", "by_county11 = by_county11[[\"Crash ID\"]]\n", "by_county11.columns = [\"crashes_2011\"]\n", "\n", "by_county12 = y2012_df[y2012_df[\"County\"] != \"Unknown\"]\n", "by_county12 = by_county12[by_county12[\"County\"] != \"No Data\"]\n", "by_county12 = by_county12.groupby(\"County\").count()\n", "by_county12 = by_county12[[\"Crash ID\"]]\n", "by_county12.columns = [\"crashes_2012\"]\n", "\n", "by_county13 = y2013_df[y2013_df[\"County\"] != \"Unknown\"]\n", "by_county13 = by_county13[by_county13[\"County\"] != \"No Data\"]\n", "by_county13 = by_county13.groupby(\"County\").count()\n", "by_county13 = by_county13[[\"Crash ID\"]]\n", "by_county13.columns = [\"crashes_2013\"]\n", "\n", "by_county14 = y2014_df[y2014_df[\"County\"] != \"Unknown\"]\n", "by_county14 = by_county14[by_county14[\"County\"] != \"No Data\"]\n", "by_county14 = by_county14.groupby(\"County\").count()\n", "by_county14 = by_county14[[\"Crash ID\"]]\n", "by_county14.columns = [\"crashes_2014\"]\n", "\n", "by_county15 = y2015_df[y2015_df[\"County\"] != \"Unknown\"]\n", "by_county15 = by_county15[by_county15[\"County\"] != \"No Data\"]\n", "by_county15 = by_county15.groupby(\"County\").count()\n", "by_county15 = by_county15[[\"Crash ID\"]]\n", "by_county15.columns = [\"crashes_2015\"]\n", "\n", "by_county16 = y2016_df[y2016_df[\"County\"] != \"Unknown\"]\n", "by_county16 = by_county16[by_county16[\"County\"] != \"No Data\"]\n", "by_county16 = by_county16.groupby(\"County\").count()\n", "by_county16 = by_county16[[\"Crash ID\"]]\n", "by_county16.columns = [\"crashes_2016\"]\n", "\n", "by_county17 = y2017_df[y2017_df[\"County\"] != \"Unknown\"]\n", "by_county17 = by_county17[by_county17[\"County\"] != \"No Data\"]\n", "by_county17 = by_county17.groupby(\"County\").count()\n", "by_county17 = by_county17[[\"Crash ID\"]]\n", "by_county17.columns = [\"crashes_2017\"]\n", "\n", "by_county18 = y2018_df[y2018_df[\"County\"] != \"Unknown\"]\n", "by_county18 = by_county18[by_county18[\"County\"] != \"No Data\"]\n", "by_county18 = by_county18.groupby(\"County\").count()\n", "by_county18 = by_county18[[\"Crash ID\"]]\n", "by_county18.columns = [\"crashes_2018\"]\n", "\n", "by_county17.head()"], ["# Joining dataframes together\n", "by_county[\"crashes_2011\"] = by_county11[\"crashes_2011\"]\n", "by_county[\"crashes_2012\"] = by_county12[\"crashes_2012\"]\n", "by_county[\"crashes_2013\"] = by_county13[\"crashes_2013\"]\n", "by_county[\"crashes_2014\"] = by_county14[\"crashes_2014\"]\n", "by_county[\"crashes_2015\"] = by_county15[\"crashes_2015\"]\n", "by_county[\"crashes_2016\"] = by_county16[\"crashes_2016\"]\n", "by_county[\"crashes_2017\"] = by_county17[\"crashes_2017\"]\n", "by_county[\"crashes_2018\"] = by_county18[\"crashes_2018\"]\n", "\n", "# Reset index and rename for clarity\n", "by_county.reset_index(inplace=True)\n", "by_county.columns = by_county.columns.str.lower()\n", "by_county.head()"], ["# Rename df\n", "total_crashes_by_county = by_county"], ["# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["# Create Engine\n", "engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["# Use `declarative_base` from SQLAlchemy to model the 'crashes' table as an ORM class\n", "# Declare a Base object here\n", "Base = declarative_base()"], ["# Define the ORM class for `Crashes`\n", "class Crashes(Base):\n", "    \n", "    __tablename__ = 'crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    crash_id = Column(Integer, unique=True)\n", "    agency = Column(Text)\n", "    city = Column(Text)\n", "    county = Column(Text)\n", "    crash_death_count = Column(Integer)\n", "    crash_severity = Column(Text)\n", "    crash_time = Column(Integer)\n", "    crash_total_injury_count = Column(Integer)\n", "    crash_year = Column(Integer)\n", "    day_of_week = Column(Text)\n", "    latitude = Column(Float)\n", "    longitude = Column(Float)\n", "    manner_of_collision = Column(Text)\n", "    population_group = Column(Text)\n", "    road_class = Column(Text)\n", "    speed_limit = Column(Integer)\n", "    weather_condition = Column(Text)\n", "    vehicle_color = Column(Text)\n", "    person_age = Column(Integer)\n", "    person_ethnicity = Column(Text)\n", "    person_gender = Column(Text)\n", "    person_type = Column(Text)\n", "    \n", "    \n", "    def __repr__(self):\n", "        return f\"id={self.crash_id}, name={self.agency}\""], ["class Gender_crashes(Base):\n", "    \n", "    __tablename__ = 'gender_crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    female = Column(Integer)\n", "    male = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.gender}: 2010 - {self.stops_2010}\"\n", "\n", "class Race_crashes(Base):\n", "    \n", "    __tablename__ = 'race_crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    native_american = Column(Integer)\n", "    asian = Column(Integer)\n", "    black = Column(Integer)\n", "    hispanic = Column(Integer)\n", "    other = Column(Integer)\n", "    white = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.race}: 2010 - {self.stops_2010}\"\n", "\n", "class County_crashes(Base):\n", "    \n", "    __tablename__ = 'county_crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    county = Column(Text)\n", "    crashes_2010 = Column(Integer)\n", "    crashes_2011 = Column(Integer)\n", "    crashes_2012 = Column(Integer)\n", "    crashes_2013 = Column(Integer)\n", "    crashes_2014 = Column(Integer)\n", "    crashes_2015 = Column(Integer)\n", "    crashes_2016 = Column(Integer)\n", "    crashes_2017 = Column(Integer)\n", "    crashes_2018 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.county}: 2010 - {self.stops_2010}\""], ["# Use `create_all` to create the tables\n", "Base.metadata.create_all(engine)\n"], ["# Verify that the table names exist in the database\n", "engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('race_crashes')"], ["# connect to the database\n", "conn = engine.connect()\n", "\n", "# Orient ='records' creates a list of data to write\n", "gender_data = total_crashes_by_gender.to_dict(orient='records')\n", "race_data = total_crashes_by_race.to_dict(orient='records')\n", "county_data = total_crashes_by_county.to_dict(orient='records')\n", "\n", "# Insert the dataframe into the database in one bulk insert\n", "conn.execute(Gender_crashes.__table__.delete())\n", "conn.execute(Gender_crashes.__table__.insert(), gender_data)\n", "conn.execute(Race_crashes.__table__.delete())\n", "conn.execute(Race_crashes.__table__.insert(), race_data)\n", "conn.execute(County_crashes.__table__.delete())\n", "conn.execute(County_crashes.__table__.insert(), county_data)\n", "\n", "conn.close()"], ["# Testing the county_crashes table via engine \n", "engine.execute(\"SELECT * FROM county_crashes LIMIT 10\").fetchall()"], ["# Use Pandas to Bulk insert each data frame into their appropriate table\n", "def populate_table(engine, table):\n", "    \"\"\"Populates a table from a Pandas DataFrame.\"\"\"\n", "    \n", "    # connect to the database\n", "    conn = engine.connect()\n", "    \n", "    # Orient='records' creates a list of data to write\n", "    data = speedlimit_crash_df.to_dict(orient='records')\n", "\n", "    # Optional: Delete all rows in the table \n", "    conn.execute(table.delete())\n", "\n", "    # Insert the dataframe into the database in one bulk insert\n", "    conn.execute(table.insert(), data)\n", "    conn.close()\n", "    \n", "# Call the function to insert the data for each table\n", "populate_table(engine, Crashes.__table__)"], ["# Use a basic query to validate that the data was inserted correctly for table `crashes`\n", "engine.execute(\"SELECT * FROM crashes LIMIT 5\").fetchall()"], ["# Count the number of records in the 'crashes' table\n", "engine.execute(\"SELECT COUNT(*) FROM crashes\").fetchall()"]]}, {"database_creation_stops.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Read CSVs into dataframes\n", "stops10 = pd.read_csv(\"stops_2010.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops11 = pd.read_csv(\"stops_2011.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops12 = pd.read_csv(\"stops_2012.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops13 = pd.read_csv(\"stops_2013.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops14 = pd.read_csv(\"stops_2014.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops15 = pd.read_csv(\"stops_2015.csv\", encoding=\"utf-8\", low_memory=False)"], ["# Cut down to desired columns\n", "stops10 = stops10[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']]\n", "stops11 = stops11[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops12 = stops12[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops13 = stops13[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops14 = stops14[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops15 = stops15[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] "], ["# Clean column names\n", "stops10.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops11.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops12.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops13.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops14.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops15.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']"], ["# Group by date for total stops per day of year\n", "by_date10 = stops10.groupby(\"date\").count()\n", "by_date10 = by_date10[[\"county\"]]\n", "by_date10.columns = [\"stops_2010\"]\n", "\n", "# Reset index and remove year to leave just month/day\n", "by_date10.reset_index(inplace=True) \n", "by_date10[\"date\"] = by_date10[\"date\"].map(lambda x: str(x)[5:])\n", "\n", "# Group other dfs by date and reset index so they can be joined\n", "by_date11 = stops11.groupby(\"date\").count()\n", "by_date11.reset_index(inplace=True)\n", "by_date11 = by_date11[[\"county\"]]\n", "by_date11.columns = [\"stops_2011\"]\n", "\n", "by_date12 = stops12.groupby(\"date\").count()\n", "by_date12.reset_index(inplace=True)\n", "by_date12 = by_date12[[\"county\"]]\n", "by_date12.columns = [\"stops_2012\"]\n", "\n", "by_date13 = stops13.groupby(\"date\").count()\n", "by_date13.reset_index(inplace=True)\n", "by_date13 = by_date13[[\"county\"]]\n", "by_date13.columns = [\"stops_2013\"]\n", "\n", "by_date14 = stops14.groupby(\"date\").count()\n", "by_date14.reset_index(inplace=True)\n", "by_date14 = by_date14[[\"county\"]]\n", "by_date14.columns = [\"stops_2014\"]\n", "\n", "by_date15 = stops15.groupby(\"date\").count()\n", "by_date15.reset_index(inplace=True)\n", "by_date15 = by_date15[[\"county\"]]\n", "by_date15.columns = [\"stops_2015\"]"], ["# Add columns together for single dataframe\n", "by_date10[\"stops_2011\"] = by_date11[\"stops_2011\"]\n", "by_date10[\"stops_2012\"] = by_date12[\"stops_2012\"]\n", "by_date10[\"stops_2013\"] = by_date13[\"stops_2013\"]\n", "by_date10[\"stops_2014\"] = by_date14[\"stops_2014\"]\n", "by_date10[\"stops_2015\"] = by_date15[\"stops_2015\"]\n", "\n", "# Rename for clarity\n", "total_stops_by_year = by_date10\n", "total_stops_by_year.head()"], ["# Same stuff with grouping by gender\n", "by_gender = stops10.groupby(\"gender\").count()\n", "by_gender = by_gender[[\"outcome\"]]\n", "by_gender.columns = [\"stops_2010\"]\n", "\n", "by_gender11 = stops11.groupby(\"gender\").count()\n", "by_gender11 = by_gender11[[\"outcome\"]]\n", "by_gender11.columns = [\"stops_2011\"]\n", "\n", "by_gender12 = stops12.groupby(\"gender\").count()\n", "by_gender12 = by_gender12[[\"outcome\"]]\n", "by_gender12.columns = [\"stops_2012\"]\n", "\n", "by_gender13 = stops13.groupby(\"gender\").count()\n", "by_gender13 = by_gender13[[\"outcome\"]]\n", "by_gender13.columns = [\"stops_2013\"]\n", "\n", "by_gender14 = stops14.groupby(\"gender\").count()\n", "by_gender14 = by_gender14[[\"outcome\"]]\n", "by_gender14.columns = [\"stops_2014\"]\n", "\n", "by_gender15 = stops15.groupby(\"gender\").count()\n", "by_gender15 = by_gender15[[\"outcome\"]]\n", "by_gender15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_gender[\"stops_2011\"] = by_gender11[\"stops_2011\"]\n", "by_gender[\"stops_2012\"] = by_gender12[\"stops_2012\"]\n", "by_gender[\"stops_2013\"] = by_gender13[\"stops_2013\"]\n", "by_gender[\"stops_2014\"] = by_gender14[\"stops_2014\"]\n", "by_gender[\"stops_2015\"] = by_gender15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_gender.reset_index(inplace=True)\n", "by_gender.head()"], ["by_gender = by_gender.transpose()\n", "by_gender.reset_index(inplace=True)\n", "by_gender.columns = [\"year\", \"male\", \"female\"]\n", "by_gender = by_gender.drop(by_gender.index[0])\n", "by_gender[\"year\"] = by_gender[\"year\"].map(lambda x: str(x)[6:])\n", "by_gender"], ["total_stops_by_gender = by_gender"], ["# Same stuff with grouping by race\n", "by_race = stops10.groupby(\"race\").count()\n", "by_race = by_race[[\"outcome\"]]\n", "by_race.columns = [\"stops_2010\"]\n", "\n", "by_race11 = stops11.groupby(\"race\").count()\n", "by_race11 = by_race11[[\"outcome\"]]\n", "by_race11.columns = [\"stops_2011\"]\n", "\n", "by_race12 = stops12.groupby(\"race\").count()\n", "by_race12 = by_race12[[\"outcome\"]]\n", "by_race12.columns = [\"stops_2012\"]\n", "\n", "by_race13 = stops13.groupby(\"race\").count()\n", "by_race13 = by_race13[[\"outcome\"]]\n", "by_race13.columns = [\"stops_2013\"]\n", "\n", "by_race14 = stops14.groupby(\"race\").count()\n", "by_race14 = by_race14[[\"outcome\"]]\n", "by_race14.columns = [\"stops_2014\"]\n", "\n", "by_race15 = stops15.groupby(\"race\").count()\n", "by_race15 = by_race15[[\"outcome\"]]\n", "by_race15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_race[\"stops_2011\"] = by_race11[\"stops_2011\"]\n", "by_race[\"stops_2012\"] = by_race12[\"stops_2012\"]\n", "by_race[\"stops_2013\"] = by_race13[\"stops_2013\"]\n", "by_race[\"stops_2014\"] = by_race14[\"stops_2014\"]\n", "by_race[\"stops_2015\"] = by_race15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_race.reset_index(inplace=True)\n", "by_race.head()"], ["by_race = by_race.transpose()\n", "by_race.reset_index(inplace=True)\n", "by_race.columns = [\"year\", \"asian\", \"black\", \"hispanic\", \"other\", \"white\"]\n", "by_race = by_race.drop(by_race.index[0])\n", "by_race[\"year\"] = by_race[\"year\"].map(lambda x: str(x)[6:])\n", "by_race"], ["total_stops_by_race = by_race"], ["# Same stuff with grouping by county\n", "by_county = stops10.groupby(\"county\").count()\n", "by_county = by_county[[\"outcome\"]]\n", "by_county.columns = [\"stops_2010\"]\n", "\n", "by_county11 = stops11.groupby(\"county\").count()\n", "by_county11 = by_county11[[\"outcome\"]]\n", "by_county11.columns = [\"stops_2011\"]\n", "\n", "by_county12 = stops12.groupby(\"county\").count()\n", "by_county12 = by_county12[[\"outcome\"]]\n", "by_county12.columns = [\"stops_2012\"]\n", "\n", "by_county13 = stops13.groupby(\"county\").count()\n", "by_county13 = by_county13[[\"outcome\"]]\n", "by_county13.columns = [\"stops_2013\"]\n", "\n", "by_county14 = stops14.groupby(\"county\").count()\n", "by_county14 = by_county14[[\"outcome\"]]\n", "by_county14.columns = [\"stops_2014\"]\n", "\n", "by_county15 = stops15.groupby(\"county\").count()\n", "by_county15 = by_county15[[\"outcome\"]]\n", "by_county15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_county[\"stops_2011\"] = by_county11[\"stops_2011\"]\n", "by_county[\"stops_2012\"] = by_county12[\"stops_2012\"]\n", "by_county[\"stops_2013\"] = by_county13[\"stops_2013\"]\n", "by_county[\"stops_2014\"] = by_county14[\"stops_2014\"]\n", "by_county[\"stops_2015\"] = by_county15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_county.reset_index(inplace=True)\n", "by_county.head()"], ["total_stops_by_county = by_county"], ["# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["Base = declarative_base()"], ["class Date_stops(Base):\n", "    \n", "    __tablename__ = 'date_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    date = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.date}: 2010 - {self.stops_2010}\"\n", "    \n", "class Gender_stops(Base):\n", "    \n", "    __tablename__ = 'gender_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    male = Column(Integer)\n", "    female = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.gender}: 2010 - {self.stops_2010}\"\n", "\n", "class Race_stops(Base):\n", "    \n", "    __tablename__ = 'race_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    asian = Column(Integer)\n", "    black = Column(Integer)\n", "    hispanic = Column(Integer)\n", "    other = Column(Integer)\n", "    white = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.race}: 2010 - {self.stops_2010}\"\n", "\n", "class County_stops(Base):\n", "    \n", "    __tablename__ = 'county_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    county = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.county}: 2010 - {self.stops_2010}\""], ["Base.metadata.create_all(engine)"], ["engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('race_stops')"], ["# connect to the database\n", "conn = engine.connect()\n", "\n", "# Orient='records' creates a list of data to write\n", "date_data = total_stops_by_year.to_dict(orient='records')\n", "gender_data = total_stops_by_gender.to_dict(orient='records')\n", "race_data = total_stops_by_race.to_dict(orient='records')\n", "county_data = total_stops_by_county.to_dict(orient='records')\n", "\n", "# Insert the dataframe into the database in one bulk insert\n", "#conn.execute(Date_stops.__table__.delete())\n", "conn.execute(Date_stops.__table__.insert(), date_data)\n", "#conn.execute(Gender_stops.__table__.delete())\n", "conn.execute(Gender_stops.__table__.insert(), gender_data)\n", "#conn.execute(Race_stops.__table__.delete())\n", "conn.execute(Race_stops.__table__.insert(), race_data)\n", "#conn.execute(County_stops.__table__.delete())\n", "conn.execute(County_stops.__table__.insert(), county_data)\n", "\n", "conn.close()"], ["engine.execute(\"SELECT * FROM date_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM gender_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM race_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM county_stops LIMIT 5\").fetchall()"], []]}, {"Stops DB-checkpoint.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Read CSVs into dataframes\n", "stops10 = pd.read_csv(\"stops_2010.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops11 = pd.read_csv(\"stops_2011.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops12 = pd.read_csv(\"stops_2012.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops13 = pd.read_csv(\"stops_2013.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops14 = pd.read_csv(\"stops_2014.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops15 = pd.read_csv(\"stops_2015.csv\", encoding=\"utf-8\", low_memory=False)"], ["# Cut down to desired columns\n", "stops10 = stops10[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']]\n", "stops11 = stops11[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops12 = stops12[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops13 = stops13[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops14 = stops14[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops15 = stops15[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] "], ["# Clean column names\n", "stops10.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops11.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops12.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops13.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops14.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops15.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']"], ["# Group by date for total stops per day of year\n", "by_date10 = stops10.groupby(\"date\").count()\n", "by_date10 = by_date10[[\"county\"]]\n", "by_date10.columns = [\"stops_2010\"]\n", "\n", "# Reset index and remove year to leave just month/day\n", "by_date10.reset_index(inplace=True) \n", "by_date10[\"date\"] = by_date10[\"date\"].map(lambda x: str(x)[5:])\n", "\n", "# Group other dfs by date and reset index so they can be joined\n", "by_date11 = stops11.groupby(\"date\").count()\n", "by_date11.reset_index(inplace=True)\n", "by_date11 = by_date11[[\"county\"]]\n", "by_date11.columns = [\"stops_2011\"]\n", "\n", "by_date12 = stops12.groupby(\"date\").count()\n", "by_date12.reset_index(inplace=True)\n", "by_date12 = by_date12[[\"county\"]]\n", "by_date12.columns = [\"stops_2012\"]\n", "\n", "by_date13 = stops13.groupby(\"date\").count()\n", "by_date13.reset_index(inplace=True)\n", "by_date13 = by_date13[[\"county\"]]\n", "by_date13.columns = [\"stops_2013\"]\n", "\n", "by_date14 = stops14.groupby(\"date\").count()\n", "by_date14.reset_index(inplace=True)\n", "by_date14 = by_date14[[\"county\"]]\n", "by_date14.columns = [\"stops_2014\"]\n", "\n", "by_date15 = stops15.groupby(\"date\").count()\n", "by_date15.reset_index(inplace=True)\n", "by_date15 = by_date15[[\"county\"]]\n", "by_date15.columns = [\"stops_2015\"]"], ["# Add columns together for single dataframe\n", "by_date10[\"stops_2011\"] = by_date11[\"stops_2011\"]\n", "by_date10[\"stops_2012\"] = by_date12[\"stops_2012\"]\n", "by_date10[\"stops_2013\"] = by_date13[\"stops_2013\"]\n", "by_date10[\"stops_2014\"] = by_date14[\"stops_2014\"]\n", "by_date10[\"stops_2015\"] = by_date15[\"stops_2015\"]\n", "\n", "# Rename for clarity\n", "total_stops_by_year = by_date10\n", "total_stops_by_year.head()"], ["# Same stuff with grouping by gender\n", "by_gender = stops10.groupby(\"gender\").count()\n", "by_gender = by_gender[[\"outcome\"]]\n", "by_gender.columns = [\"stops_2010\"]\n", "\n", "by_gender11 = stops11.groupby(\"gender\").count()\n", "by_gender11 = by_gender11[[\"outcome\"]]\n", "by_gender11.columns = [\"stops_2011\"]\n", "\n", "by_gender12 = stops12.groupby(\"gender\").count()\n", "by_gender12 = by_gender12[[\"outcome\"]]\n", "by_gender12.columns = [\"stops_2012\"]\n", "\n", "by_gender13 = stops13.groupby(\"gender\").count()\n", "by_gender13 = by_gender13[[\"outcome\"]]\n", "by_gender13.columns = [\"stops_2013\"]\n", "\n", "by_gender14 = stops14.groupby(\"gender\").count()\n", "by_gender14 = by_gender14[[\"outcome\"]]\n", "by_gender14.columns = [\"stops_2014\"]\n", "\n", "by_gender15 = stops15.groupby(\"gender\").count()\n", "by_gender15 = by_gender15[[\"outcome\"]]\n", "by_gender15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_gender[\"stops_2011\"] = by_gender11[\"stops_2011\"]\n", "by_gender[\"stops_2012\"] = by_gender12[\"stops_2012\"]\n", "by_gender[\"stops_2013\"] = by_gender13[\"stops_2013\"]\n", "by_gender[\"stops_2014\"] = by_gender14[\"stops_2014\"]\n", "by_gender[\"stops_2015\"] = by_gender15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_gender.reset_index(inplace=True)\n", "total_stops_by_gender = by_gender\n", "total_stops_by_gender.head()"], ["# Same stuff with grouping by race\n", "by_race = stops10.groupby(\"race\").count()\n", "by_race = by_race[[\"outcome\"]]\n", "by_race.columns = [\"stops_2010\"]\n", "\n", "by_race11 = stops11.groupby(\"race\").count()\n", "by_race11 = by_race11[[\"outcome\"]]\n", "by_race11.columns = [\"stops_2011\"]\n", "\n", "by_race12 = stops12.groupby(\"race\").count()\n", "by_race12 = by_race12[[\"outcome\"]]\n", "by_race12.columns = [\"stops_2012\"]\n", "\n", "by_race13 = stops13.groupby(\"race\").count()\n", "by_race13 = by_race13[[\"outcome\"]]\n", "by_race13.columns = [\"stops_2013\"]\n", "\n", "by_race14 = stops14.groupby(\"race\").count()\n", "by_race14 = by_race14[[\"outcome\"]]\n", "by_race14.columns = [\"stops_2014\"]\n", "\n", "by_race15 = stops15.groupby(\"race\").count()\n", "by_race15 = by_race15[[\"outcome\"]]\n", "by_race15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_race[\"stops_2011\"] = by_race11[\"stops_2011\"]\n", "by_race[\"stops_2012\"] = by_race12[\"stops_2012\"]\n", "by_race[\"stops_2013\"] = by_race13[\"stops_2013\"]\n", "by_race[\"stops_2014\"] = by_race14[\"stops_2014\"]\n", "by_race[\"stops_2015\"] = by_race15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_race.reset_index(inplace=True)\n", "total_stops_by_race = by_race\n", "total_stops_by_race.head()"], ["# Same stuff with grouping by county\n", "by_county = stops10.groupby(\"county\").count()\n", "by_county = by_county[[\"outcome\"]]\n", "by_county.columns = [\"stops_2010\"]\n", "\n", "by_county11 = stops11.groupby(\"county\").count()\n", "by_county11 = by_county11[[\"outcome\"]]\n", "by_county11.columns = [\"stops_2011\"]\n", "\n", "by_county12 = stops12.groupby(\"county\").count()\n", "by_county12 = by_county12[[\"outcome\"]]\n", "by_county12.columns = [\"stops_2012\"]\n", "\n", "by_county13 = stops13.groupby(\"county\").count()\n", "by_county13 = by_county13[[\"outcome\"]]\n", "by_county13.columns = [\"stops_2013\"]\n", "\n", "by_county14 = stops14.groupby(\"county\").count()\n", "by_county14 = by_county14[[\"outcome\"]]\n", "by_county14.columns = [\"stops_2014\"]\n", "\n", "by_county15 = stops15.groupby(\"county\").count()\n", "by_county15 = by_county15[[\"outcome\"]]\n", "by_county15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_county[\"stops_2011\"] = by_county11[\"stops_2011\"]\n", "by_county[\"stops_2012\"] = by_county12[\"stops_2012\"]\n", "by_county[\"stops_2013\"] = by_county13[\"stops_2013\"]\n", "by_county[\"stops_2014\"] = by_county14[\"stops_2014\"]\n", "by_county[\"stops_2015\"] = by_county15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_county.reset_index(inplace=True)\n", "total_stops_by_county = by_county\n", "total_stops_by_county.head()"], [], ["# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["Base = declarative_base()"], ["class Date_stops(Base):\n", "    \n", "    __tablename__ = 'date_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    date = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.date}: 2010 - {self.stops_2010}\"\n", "    \n", "class Gender_stops(Base):\n", "    \n", "    __tablename__ = 'gender_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    gender = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.gender}: 2010 - {self.stops_2010}\"\n", "\n", "class Race_stops(Base):\n", "    \n", "    __tablename__ = 'race_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    race = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.race}: 2010 - {self.stops_2010}\"\n", "\n", "class County_stops(Base):\n", "    \n", "    __tablename__ = 'county_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    county = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.county}: 2010 - {self.stops_2010}\""], ["Base.metadata.create_all(engine)"], ["engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('county_stops')"], ["# connect to the database\n", "conn = engine.connect()\n", "\n", "# Orient='records' creates a list of data to write\n", "date_data = total_stops_by_year.to_dict(orient='records')\n", "gender_data = total_stops_by_gender.to_dict(orient='records')\n", "race_data = total_stops_by_race.to_dict(orient='records')\n", "county_data = total_stops_by_county.to_dict(orient='records')\n", "\n", "# Insert the dataframe into the database in one bulk insert\n", "conn.execute(Date_stops.__table__.insert(), date_data)\n", "conn.execute(Gender_stops.__table__.insert(), gender_data)\n", "conn.execute(Race_stops.__table__.insert(), race_data)\n", "conn.execute(County_stops.__table__.insert(), county_data)\n", "\n", "conn.close()"], ["engine.execute(\"SELECT * FROM date_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM gender_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM race_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM county_stops LIMIT 5\").fetchall()"], []]}, {"data & database engineering-checkpoint.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Create OS paths for each data set\n", "y2010 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2010 Speed Related Crashes Data.csv\")\n", "y2011 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2011 Speed Related Crashes Data.csv\")\n", "y2012 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2012 Speed Related Crashes Data.csv\")\n", "y2013 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2013 Speed Related Crashes Data.csv\")\n", "y2014 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2014 Speed Related Crashes Data.csv\")\n", "y2015 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2015 Speed Related Crashes Data.csv\")\n", "y2016 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2016 Speed Related Crashes Data.csv\")\n", "y2017 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2017 Speed Related Crashes Data.csv\")\n", "y2018 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2018 Speed Related Crashes Data.csv\")\n"], ["# Create data frames for each data set via read_csv\n", "y2010_df = pd.read_csv(y2010, encoding = \"utf-8\")\n", "y2011_df = pd.read_csv(y2011, encoding = \"utf-8\")\n", "y2012_df = pd.read_csv(y2012, encoding = \"utf-8\")\n", "y2013_df = pd.read_csv(y2013, encoding = \"utf-8\")\n", "y2014_df = pd.read_csv(y2014, encoding = \"utf-8\")\n", "y2015_df = pd.read_csv(y2015, encoding = \"utf-8\")\n", "y2016_df = pd.read_csv(y2016, encoding = \"utf-8\")\n", "y2017_df = pd.read_csv(y2017, encoding = \"utf-8\")\n", "y2018_df = pd.read_csv(y2018, encoding = \"utf-8\")"], ["y2018_df.head()"], ["# Append all the data sets into a single data set\n", "speedlimit_crash_df = y2010_df.append(y2011_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2012_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2013_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2014_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2015_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2016_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2017_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2018_df)\n", "speedlimit_crash_df.head()"], ["# Drop rows from teh data set with duplicate Crash ID's\n", "speedlimit_crash_df = speedlimit_crash_df.drop_duplicates(['Crash ID'], keep='first')\n", "speedlimit_crash_df = speedlimit_crash_df.reset_index(drop=True)\n", "speedlimit_crash_df.head(2)"], ["# Rename column names to be ORM class/table friendly\n", "speedlimit_crash_df = speedlimit_crash_df.rename(columns={'Crash ID': 'crash_id','Crash Death Count': 'crash_death_count', 'Crash Severity': 'crash_severity', 'Crash Time': 'crash_time', 'Crash Total Injury Count': 'crash_total_injury_count', 'Crash Year': 'crash_year', 'Day of Week': 'day_of_week', 'Manner of Collision': 'manner_of_collision', 'Population Group': 'population_group', 'Road Class': 'road_class', 'Speed Limit': 'speed_limit', 'Weather Condition': 'weather_condition', 'Vehicle Color': 'vehicle_color', 'Person Age': 'person_age', 'Person Ethnicity': 'person_ethnicity', 'Person Gender': 'person_gender', 'Person Type': 'person_type'})\n", "\n", "# Turn all columns headers to lowercase\n", "speedlimit_crash_df.columns = speedlimit_crash_df.columns.str.lower()\n", "\n", "speedlimit_crash_df.head(20)"], ["# Replace \n", "for column in speedlimit_crash_df.columns[1:]:\n", "    if (column == \"crash_death_count\") or (column == \"crash_time\") or (column == \"crash_total_injury_count\") or (column == \"crash_year\") or (column == \"latitude\") or (column == \"longitude\") or (column == \"speed_limit\") or (column == \"person_age\"):\n", "         speedlimit_crash_df[column] = speedlimit_crash_df[column].replace(\"No Data\", 0)\n", "    else:\n", "        speedlimit_crash_df[column] = speedlimit_crash_df[column].replace(\"No Data\", \"\")\n", "\n", "#speedlimit_crash_df.iloc[:,0:11]\n", "speedlimit_crash_df.head()"], ["# speedlimit_crash_df = speedlimit_crash_df.set_index(\"crash_id\")\n", "# speedlimit_crash_df.head()"], ["# Number of rows in the df\n", "len(speedlimit_crash_df.index)"], ["# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["# Create Engine\n", "engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["# Use `declarative_base` from SQLAlchemy to model the 'crashes' table as an ORM class\n", "# Declare a Base object here\n", "Base = declarative_base()"], ["# Define the ORM class for `Crashes`\n", "class Crashes(Base):\n", "    \n", "    __tablename__ = 'crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    crash_id = Column(Integer, unique=True)\n", "    agency = Column(Text)\n", "    city = Column(Text)\n", "    county = Column(Text)\n", "    crash_death_count = Column(Integer)\n", "    crash_severity = Column(Text)\n", "    crash_time = Column(Integer)\n", "    crash_total_injury_count = Column(Integer)\n", "    crash_year = Column(Integer)\n", "    day_of_week = Column(Text)\n", "    latitude = Column(Float)\n", "    longitude = Column(Float)\n", "    manner_of_collision = Column(Text)\n", "    population_group = Column(Text)\n", "    road_class = Column(Text)\n", "    speed_limit = Column(Integer)\n", "    weather_condition = Column(Text)\n", "    vehicle_color = Column(Text)\n", "    person_age = Column(Integer)\n", "    person_ethnicity = Column(Text)\n", "    person_gender = Column(Text)\n", "    person_type = Column(Text)\n", "    \n", "    \n", "    def __repr__(self):\n", "        return f\"id={self.crash_id}, name={self.agency}\""], ["# Use `create_all` to create the tables\n", "Base.metadata.create_all(engine)\n", "# speedlimit_crash_df.set_index(\"crash_id\")\\\n", "#                     .to_sql(name=\"crashes\", con=engine, if_exists=\"append\")"], ["# Verify that the table names exist in the database\n", "engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('crashes')\n"], ["# Use Pandas to Bulk insert each data frame into their appropriate table\n", "def populate_table(engine, table):\n", "    \"\"\"Populates a table from a Pandas DataFrame.\"\"\"\n", "    \n", "    # connect to the database\n", "    conn = engine.connect()\n", "    \n", "    # Orient='records' creates a list of data to write\n", "    data = speedlimit_crash_df.to_dict(orient='records')\n", "\n", "    # Optional: Delete all rows in the table \n", "    conn.execute(table.delete())\n", "\n", "    # Insert the dataframe into the database in one bulk insert\n", "    conn.execute(table.insert(), data)\n", "    conn.close()\n", "    \n", "# Call the function to insert the data for each table\n", "populate_table(engine, Crashes.__table__)"], ["# Use a basic query to validate that the data was inserted correctly for table `crashes`\n", "engine.execute(\"SELECT * FROM crashes LIMIT 5\").fetchall()"], ["# Count the number of records in the 'crashes' table\n", "engine.execute(\"SELECT COUNT(*) FROM crashes\").fetchall()"], []]}, {"data & db engineering - Census data-checkpoint.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Create OS paths for each data set\n", "cen2010 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_10_5YR_DP05_with_ann.csv\")\n", "cen2011 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_11_5YR_DP05_with_ann.csv\")\n", "cen2012 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_12_5YR_DP05_with_ann.csv\")\n", "cen2013 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_13_5YR_DP05_with_ann.csv\")\n", "cen2014 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_14_5YR_DP05_with_ann.csv\")\n", "cen2015 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_15_5YR_DP05_with_ann.csv\")\n", "cen2016 = os.path.join(\"Texas Census Demographic Data Sets\",\"ACS_16_5YR_DP05_with_ann.csv\")\n"], ["# Create data frames for each data set via read_csv\n", "cen2010_df = pd.read_csv(cen2010, encoding = \"utf-8\")\n", "cen2011_df = pd.read_csv(cen2011, encoding = \"utf-8\")\n", "cen2012_df = pd.read_csv(cen2012, encoding = \"utf-8\")\n", "cen2013_df = pd.read_csv(cen2013, encoding = \"utf-8\")\n", "cen2014_df = pd.read_csv(cen2014, encoding = \"utf-8\")\n", "cen2015_df = pd.read_csv(cen2015, encoding = \"utf-8\")\n", "cen2016_df = pd.read_csv(cen2016, encoding = \"utf-8\")\n", "\n"], ["cen2014_df"], ["# Drop columns with 'Margins of Error' and 'Percent', since df's are only supposed to contain estimates\n", "# Rename columns to be ORM table friendly and simplification\n", "\n", "# df_list = [cen2010_df, cen2011_df, cen2012_df, cen2013_df, cen2014_df, cen2015_df, cen2016_df]\n", "\n", "# for df in df_list:\n", "#     df = df.drop(df.filter(like='Margin').columns, 1)\n", "#     df = df.drop(df.filter(like='Percent').columns, 1)\n", "#     df = df.drop(df.filter(like='Id').columns, 1)\n", "\n", "# cen2010_df\n", "\n", "cen2010_df = cen2010_df.drop(cen2010_df.filter(like='Margin').columns, 1)\n", "cen2010_df = cen2010_df.drop(cen2010_df.filter(like='Percent').columns, 1)\n", "cen2010_df = cen2010_df.drop(cen2010_df.filter(like='Id').columns, 1)\n", "\n", "cen2010_df = cen2010_df.rename(columns = {cen2010_df.columns[0]: 'year', cen2010_df.columns[1]: 'total_population',\n", "                                         cen2010_df.columns[2]: 'male', cen2010_df.columns[3]: 'female',\n", "                                         cen2010_df.columns[4]: 'age_under_5', cen2010_df.columns[5]: 'age_5_to_9',\n", "                                         cen2010_df.columns[6]: 'age_10_to_14', cen2010_df.columns[7]: 'age_15_to_19',\n", "                                         cen2010_df.columns[8]: 'age_20_to_24', cen2010_df.columns[9]: 'age_25_to_34',\n", "                                         cen2010_df.columns[10]: 'age_35_to_44', cen2010_df.columns[11]: 'age_45_to_54',\n", "                                         cen2010_df.columns[12]: 'age_55_to_60', cen2010_df.columns[13]: 'age_60_to_64',\n", "                                         cen2010_df.columns[14]: 'age_65_to_74', cen2010_df.columns[15]: 'age_75_to_84',\n", "                                         cen2010_df.columns[16]: 'age_85_and_over', cen2010_df.columns[17]: 'white',\n", "                                         cen2010_df.columns[18]: 'black', cen2010_df.columns[19]: 'native_american',\n", "                                          cen2010_df.columns[20]: 'asian', cen2010_df.columns[21]: 'pacific_islander',\n", "                                          cen2010_df.columns[22]: 'other_race', cen2010_df.columns[23]: 'hispanic'})\n", "\n", "# Change the 'geography' column to year column. Assign the year as the row value\n", "cen2010_df[\"year\"] = 2010\n", "\n", "cen2011_df = cen2011_df.drop(cen2011_df.filter(like='Margin').columns, 1)\n", "cen2011_df = cen2011_df.drop(cen2011_df.filter(like='Percent').columns, 1)\n", "cen2011_df = cen2011_df.drop(cen2011_df.filter(like='Id').columns, 1)\n", "\n", "cen2011_df = cen2011_df.rename(columns = {cen2011_df.columns[0]: 'year', cen2011_df.columns[1]: 'total_population',\n", "                                         cen2011_df.columns[2]: 'male', cen2011_df.columns[3]: 'female',\n", "                                         cen2011_df.columns[4]: 'age_under_5', cen2011_df.columns[5]: 'age_5_to_9',\n", "                                         cen2011_df.columns[6]: 'age_10_to_14', cen2011_df.columns[7]: 'age_15_to_19',\n", "                                         cen2011_df.columns[8]: 'age_20_to_24', cen2011_df.columns[9]: 'age_25_to_34',\n", "                                         cen2011_df.columns[10]: 'age_35_to_44', cen2011_df.columns[11]: 'age_45_to_54',\n", "                                         cen2011_df.columns[12]: 'age_55_to_60', cen2011_df.columns[13]: 'age_60_to_64',\n", "                                         cen2011_df.columns[14]: 'age_65_to_74', cen2011_df.columns[15]: 'age_75_to_84',\n", "                                         cen2011_df.columns[16]: 'age_85_and_over', cen2011_df.columns[17]: 'white',\n", "                                         cen2011_df.columns[18]: 'black', cen2011_df.columns[19]: 'native_american',\n", "                                          cen2011_df.columns[20]: 'asian', cen2011_df.columns[21]: 'pacific_islander',\n", "                                          cen2011_df.columns[22]: 'other_race', cen2011_df.columns[23]: 'hispanic'})\n", "\n", "# Change the 'geography' column to year column. Assign the year as the row value\n", "cen2011_df[\"year\"] = 2011\n", "\n", "cen2012_df = cen2012_df.drop(cen2012_df.filter(like='Margin').columns, 1)\n", "cen2012_df = cen2012_df.drop(cen2012_df.filter(like='Percent').columns, 1)\n", "cen2012_df = cen2012_df.drop(cen2012_df.filter(like='Id').columns, 1)\n", "\n", "cen2012_df = cen2012_df.rename(columns = {cen2012_df.columns[0]: 'year', cen2012_df.columns[1]: 'total_population',\n", "                                         cen2012_df.columns[2]: 'male', cen2012_df.columns[3]: 'female',\n", "                                         cen2012_df.columns[4]: 'age_under_5', cen2012_df.columns[5]: 'age_5_to_9',\n", "                                         cen2012_df.columns[6]: 'age_10_to_14', cen2012_df.columns[7]: 'age_15_to_19',\n", "                                         cen2012_df.columns[8]: 'age_20_to_24', cen2012_df.columns[9]: 'age_25_to_34',\n", "                                         cen2012_df.columns[10]: 'age_35_to_44', cen2012_df.columns[11]: 'age_45_to_54',\n", "                                         cen2012_df.columns[12]: 'age_55_to_60', cen2012_df.columns[13]: 'age_60_to_64',\n", "                                         cen2012_df.columns[14]: 'age_65_to_74', cen2012_df.columns[15]: 'age_75_to_84',\n", "                                         cen2012_df.columns[16]: 'age_85_and_over', cen2012_df.columns[17]: 'white',\n", "                                         cen2012_df.columns[18]: 'black', cen2012_df.columns[19]: 'native_american',\n", "                                          cen2012_df.columns[20]: 'asian', cen2012_df.columns[21]: 'pacific_islander',\n", "                                          cen2012_df.columns[22]: 'other_race', cen2012_df.columns[23]: 'hispanic'})\n", "\n", "cen2012_df[\"year\"] = 2012\n", "\n", "cen2013_df = cen2013_df.drop(cen2013_df.filter(like='Margin').columns, 1)\n", "cen2013_df = cen2013_df.drop(cen2013_df.filter(like='Percent').columns, 1)\n", "cen2013_df = cen2013_df.drop(cen2013_df.filter(like='Id').columns, 1)\n", "\n", "cen2013_df = cen2013_df.rename(columns = {cen2013_df.columns[0]: 'year', cen2013_df.columns[1]: 'total_population',\n", "                                         cen2013_df.columns[2]: 'male', cen2013_df.columns[3]: 'female',\n", "                                         cen2013_df.columns[4]: 'age_under_5', cen2013_df.columns[5]: 'age_5_to_9',\n", "                                         cen2013_df.columns[6]: 'age_10_to_14', cen2013_df.columns[7]: 'age_15_to_19',\n", "                                         cen2013_df.columns[8]: 'age_20_to_24', cen2013_df.columns[9]: 'age_25_to_34',\n", "                                         cen2013_df.columns[10]: 'age_35_to_44', cen2013_df.columns[11]: 'age_45_to_54',\n", "                                         cen2013_df.columns[12]: 'age_55_to_60', cen2013_df.columns[13]: 'age_60_to_64',\n", "                                         cen2013_df.columns[14]: 'age_65_to_74', cen2013_df.columns[15]: 'age_75_to_84',\n", "                                         cen2013_df.columns[16]: 'age_85_and_over', cen2013_df.columns[17]: 'white',\n", "                                         cen2013_df.columns[18]: 'black', cen2013_df.columns[19]: 'native_american',\n", "                                          cen2013_df.columns[20]: 'asian', cen2013_df.columns[21]: 'pacific_islander',\n", "                                          cen2013_df.columns[22]: 'other_race', cen2013_df.columns[23]: 'hispanic'})\n", "\n", "cen2013_df[\"year\"] = 2013\n", "\n", "cen2014_df = cen2014_df.drop(cen2014_df.filter(like='Margin').columns, 1)\n", "cen2014_df = cen2014_df.drop(cen2014_df.filter(like='Percent').columns, 1)\n", "cen2014_df = cen2014_df.drop(cen2014_df.filter(like='Id').columns, 1)\n", "\n", "cen2014_df = cen2014_df.rename(columns = {cen2014_df.columns[0]: 'year', cen2014_df.columns[1]: 'total_population',\n", "                                         cen2014_df.columns[2]: 'male', cen2014_df.columns[3]: 'female',\n", "                                         cen2014_df.columns[4]: 'age_under_5', cen2014_df.columns[5]: 'age_5_to_9',\n", "                                         cen2014_df.columns[6]: 'age_10_to_14', cen2014_df.columns[7]: 'age_15_to_19',\n", "                                         cen2014_df.columns[8]: 'age_20_to_24', cen2014_df.columns[9]: 'age_25_to_34',\n", "                                         cen2014_df.columns[10]: 'age_35_to_44', cen2014_df.columns[11]: 'age_45_to_54',\n", "                                         cen2014_df.columns[12]: 'age_55_to_60', cen2014_df.columns[13]: 'age_60_to_64',\n", "                                         cen2014_df.columns[14]: 'age_65_to_74', cen2014_df.columns[15]: 'age_75_to_84',\n", "                                         cen2014_df.columns[16]: 'age_85_and_over', cen2014_df.columns[17]: 'white',\n", "                                         cen2014_df.columns[18]: 'black', cen2014_df.columns[19]: 'native_american',\n", "                                          cen2014_df.columns[20]: 'asian', cen2014_df.columns[21]: 'pacific_islander',\n", "                                          cen2014_df.columns[22]: 'other_race', cen2014_df.columns[23]: 'hispanic'})\n", "\n", "cen2014_df[\"year\"] = 2014\n", "\n", "cen2015_df = cen2015_df.drop(cen2015_df.filter(like='Margin').columns, 1)\n", "cen2015_df = cen2015_df.drop(cen2015_df.filter(like='Percent').columns, 1)\n", "cen2015_df = cen2015_df.drop(cen2015_df.filter(like='Id').columns, 1)\n", "\n", "cen2015_df = cen2015_df.rename(columns = {cen2015_df.columns[0]: 'year', cen2015_df.columns[1]: 'total_population',\n", "                                         cen2015_df.columns[2]: 'male', cen2015_df.columns[3]: 'female',\n", "                                         cen2015_df.columns[4]: 'age_under_5', cen2015_df.columns[5]: 'age_5_to_9',\n", "                                         cen2015_df.columns[6]: 'age_10_to_14', cen2015_df.columns[7]: 'age_15_to_19',\n", "                                         cen2015_df.columns[8]: 'age_20_to_24', cen2015_df.columns[9]: 'age_25_to_34',\n", "                                         cen2015_df.columns[10]: 'age_35_to_44', cen2015_df.columns[11]: 'age_45_to_54',\n", "                                         cen2015_df.columns[12]: 'age_55_to_60', cen2015_df.columns[13]: 'age_60_to_64',\n", "                                         cen2015_df.columns[14]: 'age_65_to_74', cen2015_df.columns[15]: 'age_75_to_84',\n", "                                         cen2015_df.columns[16]: 'age_85_and_over', cen2015_df.columns[17]: 'white',\n", "                                         cen2015_df.columns[18]: 'black', cen2015_df.columns[19]: 'native_american',\n", "                                          cen2015_df.columns[20]: 'asian', cen2015_df.columns[21]: 'pacific_islander',\n", "                                          cen2015_df.columns[22]: 'other_race', cen2015_df.columns[23]: 'hispanic'})\n", "\n", "cen2015_df[\"year\"] = 2015\n", "\n", "cen2016_df = cen2016_df.drop(cen2016_df.filter(like='Margin').columns, 1)\n", "cen2016_df = cen2016_df.drop(cen2016_df.filter(like='Percent').columns, 1)\n", "cen2016_df = cen2016_df.drop(cen2016_df.filter(like='Id').columns, 1)\n", "\n", "cen2016_df = cen2016_df.rename(columns = {cen2016_df.columns[0]: 'year', cen2016_df.columns[1]: 'total_population',\n", "                                         cen2016_df.columns[2]: 'male', cen2016_df.columns[3]: 'female',\n", "                                         cen2016_df.columns[4]: 'age_under_5', cen2016_df.columns[5]: 'age_5_to_9',\n", "                                         cen2016_df.columns[6]: 'age_10_to_14', cen2016_df.columns[7]: 'age_15_to_19',\n", "                                         cen2016_df.columns[8]: 'age_20_to_24', cen2016_df.columns[9]: 'age_25_to_34',\n", "                                         cen2016_df.columns[10]: 'age_35_to_44', cen2016_df.columns[11]: 'age_45_to_54',\n", "                                         cen2016_df.columns[12]: 'age_55_to_60', cen2016_df.columns[13]: 'age_60_to_64',\n", "                                         cen2016_df.columns[14]: 'age_65_to_74', cen2016_df.columns[15]: 'age_75_to_84',\n", "                                         cen2016_df.columns[16]: 'age_85_and_over', cen2016_df.columns[17]: 'white',\n", "                                         cen2016_df.columns[18]: 'black', cen2016_df.columns[19]: 'native_american',\n", "                                          cen2016_df.columns[20]: 'asian', cen2016_df.columns[21]: 'pacific_islander',\n", "                                          cen2016_df.columns[22]: 'other_race', cen2016_df.columns[23]: 'hispanic'})\n", "\n", "cen2016_df[\"year\"] = 2016\n", "\n", "cen2016_df"], ["# Append all the data sets into a single data set\n", "census_data_df = cen2010_df.append(cen2011_df)\n", "census_data_df = census_data_df.append(cen2012_df)\n", "census_data_df = census_data_df.append(cen2013_df)\n", "census_data_df = census_data_df.append(cen2014_df)\n", "census_data_df = census_data_df.append(cen2015_df)\n", "census_data_df = census_data_df.append(cen2016_df)\n", "\n", "census_data_df.head(10)"], ["# Reset the index for the census df\n", "census_data_df = census_data_df.reset_index(drop=True)\n", "census_data_df"], ["## Database engineering\n", "\n", "# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey, BigInteger\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["# Create Engine\n", "engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["# Use `declarative_base` from SQLAlchemy to model the 'crashes' table as an ORM class\n", "# Declare a Base object here\n", "Base = declarative_base()"], ["# Define the ORM class for `Demographics`\n", "class Demographics(Base):\n", "    \n", "    __tablename__ = 'demographics'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Integer)\n", "    total_population = Column(Integer)\n", "    male = Column(Integer)\n", "    female = Column(Integer)\n", "    age_under_5 = Column(Integer)\n", "    age_5_to_9 = Column(Integer)\n", "    age_10_to_14 = Column(Integer)                                \n", "    age_15_to_19 = Column(Integer)\n", "    age_20_to_24 = Column(Integer)\n", "    age_25_to_34 = Column(Integer)\n", "    age_35_to_44 = Column(Integer)\n", "    age_45_to_54 = Column(Integer)\n", "    age_55_to_60 = Column(Integer)\n", "    age_60_to_64 = Column(Integer)\n", "    age_65_to_74 = Column(Integer)\n", "    age_75_to_84 = Column(Integer)\n", "    age_85_and_over = Column(Integer)\n", "    white = Column(Integer)\n", "    black = Column(Integer)\n", "    native_american = Column(Integer)\n", "    asian = Column(Integer)\n", "    pacific_islander = Column(Integer)\n", "    other_race = Column(Integer)\n", "    hispanic = Column(Integer)\n", "            \n", "        \n", "    def __repr__(self):\n", "        return f\"id={self.crash_id}, name={self.year}\""], ["# Use `create_all` to create the tables\n", "Base.metadata.create_all(engine)"], ["# Verify that the table names exist in the database\n", "engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('demographics')"], ["census_data_df.astype(np.int8).dtypes"], ["pandas_records = census_data_df.to_dict(orient='records')"], ["len(pandas_records)"], ["fixed_records = []\n", "\n", "for item in pandas_records:\n", "    fixed_records.append({ key: int(value) for key, value in item.items() })"], ["# Use Pandas to Bulk insert each data frame into their appropriate table\n", "def populate_table(engine, table):\n", "    \"\"\"Populates a table from a Pandas DataFrame.\"\"\"\n", "    \n", "    # connect to the database\n", "    conn = engine.connect()\n", "    \n", "    # Orient='records' creates a list of data to write\n", "    data = census_data_df.to_dict(orient='records')\n", "\n", "    # Optional: Delete all rows in the table \n", "    conn.execute(table.delete())\n", "\n", "    # Insert the dataframe into the database in one bulk insert\n", "    conn.execute(table.insert(), fixed_records)\n", "    conn.close()\n", "    \n", "# Call the function to insert the data for each table\n", "populate_table(engine, Demographics.__table__)"], ["# Use a basic query to validate that the data was inserted correctly for table `demographics`\n", "engine.execute(\"SELECT * FROM demographics LIMIT 7\").fetchall()"], [], []]}, {"database_creation_crashes-checkpoint.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Create OS paths for each data set\n", "y2010 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2010 Speed Related Crashes Data.csv\")\n", "y2011 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2011 Speed Related Crashes Data.csv\")\n", "y2012 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2012 Speed Related Crashes Data.csv\")\n", "y2013 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2013 Speed Related Crashes Data.csv\")\n", "y2014 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2014 Speed Related Crashes Data.csv\")\n", "y2015 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2015 Speed Related Crashes Data.csv\")\n", "y2016 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2016 Speed Related Crashes Data.csv\")\n", "y2017 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2017 Speed Related Crashes Data.csv\")\n", "y2018 = os.path.join(\"Texas Speed Related Incidents Data Set\",\"TX 2018 Speed Related Crashes Data.csv\")\n"], ["# Create data frames for each data set via read_csv\n", "y2010_df = pd.read_csv(y2010, encoding = \"utf-8\", low_memory = False)\n", "y2011_df = pd.read_csv(y2011, encoding = \"utf-8\", low_memory = False)\n", "y2012_df = pd.read_csv(y2012, encoding = \"utf-8\", low_memory = False)\n", "y2013_df = pd.read_csv(y2013, encoding = \"utf-8\", low_memory = False)\n", "y2014_df = pd.read_csv(y2014, encoding = \"utf-8\", low_memory = False)\n", "y2015_df = pd.read_csv(y2015, encoding = \"utf-8\", low_memory = False)\n", "y2016_df = pd.read_csv(y2016, encoding = \"utf-8\", low_memory = False)\n", "y2017_df = pd.read_csv(y2017, encoding = \"utf-8\", low_memory = False)\n", "y2018_df = pd.read_csv(y2018, encoding = \"utf-8\", low_memory = False)"], ["y2018_df.head()"], ["# Append all the data sets into a single data set\n", "speedlimit_crash_df = y2010_df.append(y2011_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2012_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2013_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2014_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2015_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2016_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2017_df)\n", "speedlimit_crash_df = speedlimit_crash_df.append(y2018_df)\n", "speedlimit_crash_df.head()"], ["# Drop rows from teh data set with duplicate Crash ID's\n", "speedlimit_crash_df = speedlimit_crash_df.drop_duplicates(['Crash ID'], keep='first')\n", "speedlimit_crash_df = speedlimit_crash_df.reset_index(drop=True)\n", "speedlimit_crash_df.head(2)"], ["# Rename column names to be ORM class/table friendly\n", "speedlimit_crash_df = speedlimit_crash_df.rename(columns={'Crash ID': 'crash_id','Crash Death Count': 'crash_death_count', 'Crash Severity': 'crash_severity', 'Crash Time': 'crash_time', 'Crash Total Injury Count': 'crash_total_injury_count', 'Crash Year': 'crash_year', 'Day of Week': 'day_of_week', 'Manner of Collision': 'manner_of_collision', 'Population Group': 'population_group', 'Road Class': 'road_class', 'Speed Limit': 'speed_limit', 'Weather Condition': 'weather_condition', 'Vehicle Color': 'vehicle_color', 'Person Age': 'person_age', 'Person Ethnicity': 'person_ethnicity', 'Person Gender': 'person_gender', 'Person Type': 'person_type'})\n", "\n", "# Turn all columns headers to lowercase\n", "speedlimit_crash_df.columns = speedlimit_crash_df.columns.str.lower()\n", "\n", "speedlimit_crash_df.head(20)"], ["# Replace \n", "for column in speedlimit_crash_df.columns[1:]:\n", "    if (column == \"crash_death_count\") or (column == \"crash_time\") or (column == \"crash_total_injury_count\") or (column == \"crash_year\") or (column == \"latitude\") or (column == \"longitude\") or (column == \"speed_limit\") or (column == \"person_age\"):\n", "         speedlimit_crash_df[column] = speedlimit_crash_df[column].replace(\"No Data\", 0)\n", "    else:\n", "        speedlimit_crash_df[column] = speedlimit_crash_df[column].replace(\"No Data\", \"\")\n", "\n", "#speedlimit_crash_df.iloc[:,0:11]\n", "speedlimit_crash_df.head()"], ["# Number of rows in the df\n", "len(speedlimit_crash_df.index)"], ["# Create new data frame with crash totals by gender\n", "\n", "by_gender = y2010_df[y2010_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender = by_gender[by_gender[\"Person Gender\"] != \"No Data\"]\n", "by_gender = by_gender.groupby(\"Person Gender\").count()\n", "by_gender = by_gender[[\"Crash ID\"]]\n", "by_gender.columns = [\"crashes_2010\"]\n", "\n", "by_gender11 = y2011_df[y2011_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender11 = by_gender11[by_gender11[\"Person Gender\"] != \"No Data\"]\n", "by_gender11 = by_gender11.groupby(\"Person Gender\").count()\n", "by_gender11 = by_gender11[[\"Crash ID\"]]\n", "by_gender11.columns = [\"crashes_2011\"]\n", "\n", "by_gender12 = y2012_df[y2012_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender12 = by_gender12[by_gender12[\"Person Gender\"] != \"No Data\"]\n", "by_gender12 = by_gender12.groupby(\"Person Gender\").count()\n", "by_gender12 = by_gender12[[\"Crash ID\"]]\n", "by_gender12.columns = [\"crashes_2012\"]\n", "\n", "by_gender13 = y2013_df[y2013_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender13 = by_gender13[by_gender13[\"Person Gender\"] != \"No Data\"]\n", "by_gender13 = by_gender13.groupby(\"Person Gender\").count()\n", "by_gender13 = by_gender13[[\"Crash ID\"]]\n", "by_gender13.columns = [\"crashes_2013\"]\n", "\n", "by_gender14 = y2014_df[y2014_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender14 = by_gender14[by_gender14[\"Person Gender\"] != \"No Data\"]\n", "by_gender14 = by_gender14.groupby(\"Person Gender\").count()\n", "by_gender14 = by_gender14[[\"Crash ID\"]]\n", "by_gender14.columns = [\"crashes_2014\"]\n", "\n", "by_gender15 = y2015_df[y2015_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender15 = by_gender15[by_gender15[\"Person Gender\"] != \"No Data\"]\n", "by_gender15 = by_gender15.groupby(\"Person Gender\").count()\n", "by_gender15 = by_gender15[[\"Crash ID\"]]\n", "by_gender15.columns = [\"crashes_2015\"]\n", "\n", "by_gender16 = y2016_df[y2016_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender16 = by_gender16[by_gender16[\"Person Gender\"] != \"No Data\"]\n", "by_gender16 = by_gender16.groupby(\"Person Gender\").count()\n", "by_gender16 = by_gender16[[\"Crash ID\"]]\n", "by_gender16.columns = [\"crashes_2016\"]\n", "\n", "by_gender17 = y2017_df[y2017_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender17 = by_gender17[by_gender17[\"Person Gender\"] != \"No Data\"]\n", "by_gender17 = by_gender17.groupby(\"Person Gender\").count()\n", "by_gender17 = by_gender17[[\"Crash ID\"]]\n", "by_gender17.columns = [\"crashes_2017\"]\n", "\n", "by_gender18 = y2018_df[y2018_df[\"Person Gender\"] != \"Unknown\"]\n", "by_gender18 = by_gender18[by_gender18[\"Person Gender\"] != \"No Data\"]\n", "by_gender18 = by_gender18.groupby(\"Person Gender\").count()\n", "by_gender18 = by_gender18[[\"Crash ID\"]]\n", "by_gender18.columns = [\"crashes_2018\"]\n", "\n", "by_gender16"], ["# Joining dataframes together\n", "by_gender[\"crashes_2011\"] = by_gender11[\"crashes_2011\"]\n", "by_gender[\"crashes_2012\"] = by_gender12[\"crashes_2012\"]\n", "by_gender[\"crashes_2013\"] = by_gender13[\"crashes_2013\"]\n", "by_gender[\"crashes_2014\"] = by_gender14[\"crashes_2014\"]\n", "by_gender[\"crashes_2015\"] = by_gender15[\"crashes_2015\"]\n", "by_gender[\"crashes_2016\"] = by_gender16[\"crashes_2016\"]\n", "by_gender[\"crashes_2017\"] = by_gender17[\"crashes_2017\"]\n", "by_gender[\"crashes_2018\"] = by_gender18[\"crashes_2018\"]\n", "\n", "# Reset index and rename for clarity\n", "by_gender.reset_index(inplace=True)\n", "by_gender.head()"], ["by_gender = by_gender.transpose()\n", "by_gender.reset_index(inplace=True)\n", "by_gender.columns = [\"year\", \"female\", \"male\"]\n", "by_gender = by_gender.drop(by_gender.index[0])\n", "by_gender[\"year\"] = by_gender[\"year\"].map(lambda x: str(x)[8:])\n", "by_gender"], ["# Rename the df\n", "total_crashes_by_gender = by_gender"], ["# Same df creation process with grouping by race\n", "by_race = y2010_df[y2010_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race = by_race[by_race[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race = by_race.groupby(\"Person Ethnicity\").count()\n", "by_race = by_race[[\"Crash ID\"]]\n", "by_race.columns = [\"crashes_2010\"]\n", "\n", "by_race11 = y2011_df[y2011_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race11 = by_race11[by_race11[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race11 = by_race11.groupby(\"Person Ethnicity\").count()\n", "by_race11 = by_race11[[\"Crash ID\"]]\n", "by_race11.columns = [\"crashes_2011\"]\n", "\n", "by_race12 = y2012_df[y2012_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race12 = by_race12[by_race12[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race12 = by_race12.groupby(\"Person Ethnicity\").count()\n", "by_race12 = by_race12[[\"Crash ID\"]]\n", "by_race12.columns = [\"crashes_2012\"]\n", "\n", "by_race13 = y2013_df[y2013_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race13 = by_race13[by_race13[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race13 = by_race13.groupby(\"Person Ethnicity\").count()\n", "by_race13 = by_race13[[\"Crash ID\"]]\n", "by_race13.columns = [\"crashes_2013\"]\n", "\n", "by_race14 = y2014_df[y2014_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race14 = by_race14[by_race14[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race14 = by_race14.groupby(\"Person Ethnicity\").count()\n", "by_race14 = by_race14[[\"Crash ID\"]]\n", "by_race14.columns = [\"crashes_2014\"]\n", "\n", "by_race15 = y2015_df[y2015_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race15 = by_race15[by_race15[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race15 = by_race15.groupby(\"Person Ethnicity\").count()\n", "by_race15 = by_race15[[\"Crash ID\"]]\n", "by_race15.columns = [\"crashes_2015\"]\n", "\n", "by_race16 = y2016_df[y2016_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race16 = by_race16[by_race16[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race16 = by_race16.groupby(\"Person Ethnicity\").count()\n", "by_race16 = by_race16[[\"Crash ID\"]]\n", "by_race16.columns = [\"crashes_2016\"]\n", "\n", "by_race17 = y2017_df[y2017_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race17 = by_race17[by_race17[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race17 = by_race17.groupby(\"Person Ethnicity\").count()\n", "by_race17 = by_race17[[\"Crash ID\"]]\n", "by_race17.columns = [\"crashes_2017\"]\n", "\n", "by_race18 = y2018_df[y2018_df[\"Person Ethnicity\"] != \"Unknown\"]\n", "by_race18 = by_race18[by_race18[\"Person Ethnicity\"] != \"No Data\"]\n", "by_race18 = by_race18.groupby(\"Person Ethnicity\").count()\n", "by_race18 = by_race18[[\"Crash ID\"]]\n", "by_race18.columns = [\"crashes_2018\"]\n", "\n", "\n", "by_race16"], ["# Joining dataframes together\n", "by_race[\"crashes_2011\"] = by_race11[\"crashes_2011\"]\n", "by_race[\"crashes_2012\"] = by_race12[\"crashes_2012\"]\n", "by_race[\"crashes_2013\"] = by_race13[\"crashes_2013\"]\n", "by_race[\"crashes_2014\"] = by_race14[\"crashes_2014\"]\n", "by_race[\"crashes_2015\"] = by_race15[\"crashes_2015\"]\n", "by_race[\"crashes_2016\"] = by_race16[\"crashes_2016\"]\n", "by_race[\"crashes_2017\"] = by_race17[\"crashes_2017\"]\n", "by_race[\"crashes_2018\"] = by_race18[\"crashes_2018\"]\n", "\n", "# Reset index and rename for clarity\n", "by_race.reset_index(inplace=True)\n", "by_race"], ["by_race = by_race.transpose()\n", "by_race.reset_index(inplace=True)\n", "by_race.columns = [\"year\", \"native_american\", \"asian\", \"black\", \"hispanic\", \"other\", \"white\"]\n", "by_race = by_race.drop(by_race.index[0])\n", "by_race[\"year\"] = by_race[\"year\"].map(lambda x: str(x)[8:])\n", "by_race"], ["# Rename df\n", "total_crashes_by_race = by_race"], ["# Same df creation process with grouping by race\n", "by_county = y2010_df[y2010_df[\"County\"] != \"Unknown\"]\n", "by_county = by_county[by_county[\"County\"] != \"No Data\"]\n", "by_county = by_county.groupby(\"County\").count()\n", "by_county = by_county[[\"Crash ID\"]]\n", "by_county.columns = [\"crashes_2010\"]\n", "\n", "by_county11 = y2011_df[y2011_df[\"County\"] != \"Unknown\"]\n", "by_county11 = by_county11[by_county11[\"County\"] != \"No Data\"]\n", "by_county11 = by_county11.groupby(\"County\").count()\n", "by_county11 = by_county11[[\"Crash ID\"]]\n", "by_county11.columns = [\"crashes_2011\"]\n", "\n", "by_county12 = y2012_df[y2012_df[\"County\"] != \"Unknown\"]\n", "by_county12 = by_county12[by_county12[\"County\"] != \"No Data\"]\n", "by_county12 = by_county12.groupby(\"County\").count()\n", "by_county12 = by_county12[[\"Crash ID\"]]\n", "by_county12.columns = [\"crashes_2012\"]\n", "\n", "by_county13 = y2013_df[y2013_df[\"County\"] != \"Unknown\"]\n", "by_county13 = by_county13[by_county13[\"County\"] != \"No Data\"]\n", "by_county13 = by_county13.groupby(\"County\").count()\n", "by_county13 = by_county13[[\"Crash ID\"]]\n", "by_county13.columns = [\"crashes_2013\"]\n", "\n", "by_county14 = y2014_df[y2014_df[\"County\"] != \"Unknown\"]\n", "by_county14 = by_county14[by_county14[\"County\"] != \"No Data\"]\n", "by_county14 = by_county14.groupby(\"County\").count()\n", "by_county14 = by_county14[[\"Crash ID\"]]\n", "by_county14.columns = [\"crashes_2014\"]\n", "\n", "by_county15 = y2015_df[y2015_df[\"County\"] != \"Unknown\"]\n", "by_county15 = by_county15[by_county15[\"County\"] != \"No Data\"]\n", "by_county15 = by_county15.groupby(\"County\").count()\n", "by_county15 = by_county15[[\"Crash ID\"]]\n", "by_county15.columns = [\"crashes_2015\"]\n", "\n", "by_county16 = y2016_df[y2016_df[\"County\"] != \"Unknown\"]\n", "by_county16 = by_county16[by_county16[\"County\"] != \"No Data\"]\n", "by_county16 = by_county16.groupby(\"County\").count()\n", "by_county16 = by_county16[[\"Crash ID\"]]\n", "by_county16.columns = [\"crashes_2016\"]\n", "\n", "by_county17 = y2017_df[y2017_df[\"County\"] != \"Unknown\"]\n", "by_county17 = by_county17[by_county17[\"County\"] != \"No Data\"]\n", "by_county17 = by_county17.groupby(\"County\").count()\n", "by_county17 = by_county17[[\"Crash ID\"]]\n", "by_county17.columns = [\"crashes_2017\"]\n", "\n", "by_county18 = y2018_df[y2018_df[\"County\"] != \"Unknown\"]\n", "by_county18 = by_county18[by_county18[\"County\"] != \"No Data\"]\n", "by_county18 = by_county18.groupby(\"County\").count()\n", "by_county18 = by_county18[[\"Crash ID\"]]\n", "by_county18.columns = [\"crashes_2018\"]\n", "\n", "by_county17.head()"], ["# Joining dataframes together\n", "by_county[\"crashes_2011\"] = by_county11[\"crashes_2011\"]\n", "by_county[\"crashes_2012\"] = by_county12[\"crashes_2012\"]\n", "by_county[\"crashes_2013\"] = by_county13[\"crashes_2013\"]\n", "by_county[\"crashes_2014\"] = by_county14[\"crashes_2014\"]\n", "by_county[\"crashes_2015\"] = by_county15[\"crashes_2015\"]\n", "by_county[\"crashes_2016\"] = by_county16[\"crashes_2016\"]\n", "by_county[\"crashes_2017\"] = by_county17[\"crashes_2017\"]\n", "by_county[\"crashes_2018\"] = by_county18[\"crashes_2018\"]\n", "\n", "# Reset index and rename for clarity\n", "by_county.reset_index(inplace=True)\n", "by_county.columns = by_county.columns.str.lower()\n", "by_county.head()"], ["# Rename df\n", "total_crashes_by_county = by_county"], ["# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["# Create Engine\n", "engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["# Use `declarative_base` from SQLAlchemy to model the 'crashes' table as an ORM class\n", "# Declare a Base object here\n", "Base = declarative_base()"], ["# Define the ORM class for `Crashes`\n", "class Crashes(Base):\n", "    \n", "    __tablename__ = 'crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    crash_id = Column(Integer, unique=True)\n", "    agency = Column(Text)\n", "    city = Column(Text)\n", "    county = Column(Text)\n", "    crash_death_count = Column(Integer)\n", "    crash_severity = Column(Text)\n", "    crash_time = Column(Integer)\n", "    crash_total_injury_count = Column(Integer)\n", "    crash_year = Column(Integer)\n", "    day_of_week = Column(Text)\n", "    latitude = Column(Float)\n", "    longitude = Column(Float)\n", "    manner_of_collision = Column(Text)\n", "    population_group = Column(Text)\n", "    road_class = Column(Text)\n", "    speed_limit = Column(Integer)\n", "    weather_condition = Column(Text)\n", "    vehicle_color = Column(Text)\n", "    person_age = Column(Integer)\n", "    person_ethnicity = Column(Text)\n", "    person_gender = Column(Text)\n", "    person_type = Column(Text)\n", "    \n", "    \n", "    def __repr__(self):\n", "        return f\"id={self.crash_id}, name={self.agency}\""], ["class Gender_crashes(Base):\n", "    \n", "    __tablename__ = 'gender_crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    female = Column(Integer)\n", "    male = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.gender}: 2010 - {self.stops_2010}\"\n", "\n", "class Race_crashes(Base):\n", "    \n", "    __tablename__ = 'race_crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    native_american = Column(Integer)\n", "    asian = Column(Integer)\n", "    black = Column(Integer)\n", "    hispanic = Column(Integer)\n", "    other = Column(Integer)\n", "    white = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.race}: 2010 - {self.stops_2010}\"\n", "\n", "class County_crashes(Base):\n", "    \n", "    __tablename__ = 'county_crashes'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    county = Column(Text)\n", "    crashes_2010 = Column(Integer)\n", "    crashes_2011 = Column(Integer)\n", "    crashes_2012 = Column(Integer)\n", "    crashes_2013 = Column(Integer)\n", "    crashes_2014 = Column(Integer)\n", "    crashes_2015 = Column(Integer)\n", "    crashes_2016 = Column(Integer)\n", "    crashes_2017 = Column(Integer)\n", "    crashes_2018 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.county}: 2010 - {self.stops_2010}\""], ["# Use `create_all` to create the tables\n", "Base.metadata.create_all(engine)\n"], ["# Verify that the table names exist in the database\n", "engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('race_crashes')"], ["# connect to the database\n", "conn = engine.connect()\n", "\n", "# Orient ='records' creates a list of data to write\n", "gender_data = total_crashes_by_gender.to_dict(orient='records')\n", "race_data = total_crashes_by_race.to_dict(orient='records')\n", "county_data = total_crashes_by_county.to_dict(orient='records')\n", "\n", "# Insert the dataframe into the database in one bulk insert\n", "conn.execute(Gender_crashes.__table__.delete())\n", "conn.execute(Gender_crashes.__table__.insert(), gender_data)\n", "conn.execute(Race_crashes.__table__.delete())\n", "conn.execute(Race_crashes.__table__.insert(), race_data)\n", "conn.execute(County_crashes.__table__.delete())\n", "conn.execute(County_crashes.__table__.insert(), county_data)\n", "\n", "conn.close()"], ["# Testing the county_crashes table via engine \n", "engine.execute(\"SELECT * FROM county_crashes LIMIT 10\").fetchall()"], ["# Use Pandas to Bulk insert each data frame into their appropriate table\n", "def populate_table(engine, table):\n", "    \"\"\"Populates a table from a Pandas DataFrame.\"\"\"\n", "    \n", "    # connect to the database\n", "    conn = engine.connect()\n", "    \n", "    # Orient='records' creates a list of data to write\n", "    data = speedlimit_crash_df.to_dict(orient='records')\n", "\n", "    # Optional: Delete all rows in the table \n", "    conn.execute(table.delete())\n", "\n", "    # Insert the dataframe into the database in one bulk insert\n", "    conn.execute(table.insert(), data)\n", "    conn.close()\n", "    \n", "# Call the function to insert the data for each table\n", "populate_table(engine, Crashes.__table__)"], ["# Use a basic query to validate that the data was inserted correctly for table `crashes`\n", "engine.execute(\"SELECT * FROM crashes LIMIT 5\").fetchall()"], ["# Count the number of records in the 'crashes' table\n", "engine.execute(\"SELECT COUNT(*) FROM crashes\").fetchall()"]]}, {"database_creation_stops-checkpoint.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Read CSVs into dataframes\n", "stops10 = pd.read_csv(\"stops_2010.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops11 = pd.read_csv(\"stops_2011.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops12 = pd.read_csv(\"stops_2012.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops13 = pd.read_csv(\"stops_2013.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops14 = pd.read_csv(\"stops_2014.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops15 = pd.read_csv(\"stops_2015.csv\", encoding=\"utf-8\", low_memory=False)"], ["# Cut down to desired columns\n", "stops10 = stops10[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']]\n", "stops11 = stops11[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops12 = stops12[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops13 = stops13[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops14 = stops14[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops15 = stops15[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] "], ["# Clean column names\n", "stops10.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops11.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops12.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops13.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops14.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops15.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']"], ["# Group by date for total stops per day of year\n", "by_date10 = stops10.groupby(\"date\").count()\n", "by_date10 = by_date10[[\"county\"]]\n", "by_date10.columns = [\"stops_2010\"]\n", "\n", "# Reset index and remove year to leave just month/day\n", "by_date10.reset_index(inplace=True) \n", "by_date10[\"date\"] = by_date10[\"date\"].map(lambda x: str(x)[5:])\n", "\n", "# Group other dfs by date and reset index so they can be joined\n", "by_date11 = stops11.groupby(\"date\").count()\n", "by_date11.reset_index(inplace=True)\n", "by_date11 = by_date11[[\"county\"]]\n", "by_date11.columns = [\"stops_2011\"]\n", "\n", "by_date12 = stops12.groupby(\"date\").count()\n", "by_date12.reset_index(inplace=True)\n", "by_date12 = by_date12[[\"county\"]]\n", "by_date12.columns = [\"stops_2012\"]\n", "\n", "by_date13 = stops13.groupby(\"date\").count()\n", "by_date13.reset_index(inplace=True)\n", "by_date13 = by_date13[[\"county\"]]\n", "by_date13.columns = [\"stops_2013\"]\n", "\n", "by_date14 = stops14.groupby(\"date\").count()\n", "by_date14.reset_index(inplace=True)\n", "by_date14 = by_date14[[\"county\"]]\n", "by_date14.columns = [\"stops_2014\"]\n", "\n", "by_date15 = stops15.groupby(\"date\").count()\n", "by_date15.reset_index(inplace=True)\n", "by_date15 = by_date15[[\"county\"]]\n", "by_date15.columns = [\"stops_2015\"]"], ["# Add columns together for single dataframe\n", "by_date10[\"stops_2011\"] = by_date11[\"stops_2011\"]\n", "by_date10[\"stops_2012\"] = by_date12[\"stops_2012\"]\n", "by_date10[\"stops_2013\"] = by_date13[\"stops_2013\"]\n", "by_date10[\"stops_2014\"] = by_date14[\"stops_2014\"]\n", "by_date10[\"stops_2015\"] = by_date15[\"stops_2015\"]\n", "\n", "# Rename for clarity\n", "total_stops_by_year = by_date10\n", "total_stops_by_year.head()"], ["# Same stuff with grouping by gender\n", "by_gender = stops10.groupby(\"gender\").count()\n", "by_gender = by_gender[[\"outcome\"]]\n", "by_gender.columns = [\"stops_2010\"]\n", "\n", "by_gender11 = stops11.groupby(\"gender\").count()\n", "by_gender11 = by_gender11[[\"outcome\"]]\n", "by_gender11.columns = [\"stops_2011\"]\n", "\n", "by_gender12 = stops12.groupby(\"gender\").count()\n", "by_gender12 = by_gender12[[\"outcome\"]]\n", "by_gender12.columns = [\"stops_2012\"]\n", "\n", "by_gender13 = stops13.groupby(\"gender\").count()\n", "by_gender13 = by_gender13[[\"outcome\"]]\n", "by_gender13.columns = [\"stops_2013\"]\n", "\n", "by_gender14 = stops14.groupby(\"gender\").count()\n", "by_gender14 = by_gender14[[\"outcome\"]]\n", "by_gender14.columns = [\"stops_2014\"]\n", "\n", "by_gender15 = stops15.groupby(\"gender\").count()\n", "by_gender15 = by_gender15[[\"outcome\"]]\n", "by_gender15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_gender[\"stops_2011\"] = by_gender11[\"stops_2011\"]\n", "by_gender[\"stops_2012\"] = by_gender12[\"stops_2012\"]\n", "by_gender[\"stops_2013\"] = by_gender13[\"stops_2013\"]\n", "by_gender[\"stops_2014\"] = by_gender14[\"stops_2014\"]\n", "by_gender[\"stops_2015\"] = by_gender15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_gender.reset_index(inplace=True)\n", "total_stops_by_gender = by_gender\n", "total_stops_by_gender.head()"], ["# Same stuff with grouping by race\n", "by_race = stops10.groupby(\"race\").count()\n", "by_race = by_race[[\"outcome\"]]\n", "by_race.columns = [\"stops_2010\"]\n", "\n", "by_race11 = stops11.groupby(\"race\").count()\n", "by_race11 = by_race11[[\"outcome\"]]\n", "by_race11.columns = [\"stops_2011\"]\n", "\n", "by_race12 = stops12.groupby(\"race\").count()\n", "by_race12 = by_race12[[\"outcome\"]]\n", "by_race12.columns = [\"stops_2012\"]\n", "\n", "by_race13 = stops13.groupby(\"race\").count()\n", "by_race13 = by_race13[[\"outcome\"]]\n", "by_race13.columns = [\"stops_2013\"]\n", "\n", "by_race14 = stops14.groupby(\"race\").count()\n", "by_race14 = by_race14[[\"outcome\"]]\n", "by_race14.columns = [\"stops_2014\"]\n", "\n", "by_race15 = stops15.groupby(\"race\").count()\n", "by_race15 = by_race15[[\"outcome\"]]\n", "by_race15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_race[\"stops_2011\"] = by_race11[\"stops_2011\"]\n", "by_race[\"stops_2012\"] = by_race12[\"stops_2012\"]\n", "by_race[\"stops_2013\"] = by_race13[\"stops_2013\"]\n", "by_race[\"stops_2014\"] = by_race14[\"stops_2014\"]\n", "by_race[\"stops_2015\"] = by_race15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_race.reset_index(inplace=True)\n", "total_stops_by_race = by_race\n", "total_stops_by_race.head()"], ["# Same stuff with grouping by county\n", "by_county = stops10.groupby(\"county\").count()\n", "by_county = by_county[[\"outcome\"]]\n", "by_county.columns = [\"stops_2010\"]\n", "\n", "by_county11 = stops11.groupby(\"county\").count()\n", "by_county11 = by_county11[[\"outcome\"]]\n", "by_county11.columns = [\"stops_2011\"]\n", "\n", "by_county12 = stops12.groupby(\"county\").count()\n", "by_county12 = by_county12[[\"outcome\"]]\n", "by_county12.columns = [\"stops_2012\"]\n", "\n", "by_county13 = stops13.groupby(\"county\").count()\n", "by_county13 = by_county13[[\"outcome\"]]\n", "by_county13.columns = [\"stops_2013\"]\n", "\n", "by_county14 = stops14.groupby(\"county\").count()\n", "by_county14 = by_county14[[\"outcome\"]]\n", "by_county14.columns = [\"stops_2014\"]\n", "\n", "by_county15 = stops15.groupby(\"county\").count()\n", "by_county15 = by_county15[[\"outcome\"]]\n", "by_county15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_county[\"stops_2011\"] = by_county11[\"stops_2011\"]\n", "by_county[\"stops_2012\"] = by_county12[\"stops_2012\"]\n", "by_county[\"stops_2013\"] = by_county13[\"stops_2013\"]\n", "by_county[\"stops_2014\"] = by_county14[\"stops_2014\"]\n", "by_county[\"stops_2015\"] = by_county15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_county.reset_index(inplace=True)\n", "total_stops_by_county = by_county\n", "total_stops_by_county.head()"], [], ["# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["Base = declarative_base()"], ["class Date_stops(Base):\n", "    \n", "    __tablename__ = 'date_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    date = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.date}: 2010 - {self.stops_2010}\"\n", "    \n", "class Gender_stops(Base):\n", "    \n", "    __tablename__ = 'gender_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    gender = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.gender}: 2010 - {self.stops_2010}\"\n", "\n", "class Race_stops(Base):\n", "    \n", "    __tablename__ = 'race_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    race = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.race}: 2010 - {self.stops_2010}\"\n", "\n", "class County_stops(Base):\n", "    \n", "    __tablename__ = 'county_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    county = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.county}: 2010 - {self.stops_2010}\""], ["Base.metadata.create_all(engine)"], ["engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('county_stops')"], ["# connect to the database\n", "conn = engine.connect()\n", "\n", "# Orient='records' creates a list of data to write\n", "date_data = total_stops_by_year.to_dict(orient='records')\n", "gender_data = total_stops_by_gender.to_dict(orient='records')\n", "race_data = total_stops_by_race.to_dict(orient='records')\n", "county_data = total_stops_by_county.to_dict(orient='records')\n", "\n", "# Insert the dataframe into the database in one bulk insert\n", "conn.execute(Date_stops.__table__.insert(), date_data)\n", "conn.execute(Gender_stops.__table__.insert(), gender_data)\n", "conn.execute(Race_stops.__table__.insert(), race_data)\n", "conn.execute(County_stops.__table__.insert(), county_data)\n", "\n", "conn.close()"], ["engine.execute(\"SELECT * FROM date_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM gender_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM race_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM county_stops LIMIT 5\").fetchall()"], []]}, {"database_creation_stops_2-checkpoint.ipynb": [["# Import initial dependencies\n", "import os\n", "import pandas as pd\n", "import numpy as np"], ["# Read CSVs into dataframes\n", "stops10 = pd.read_csv(\"stops_2010.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops11 = pd.read_csv(\"stops_2011.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops12 = pd.read_csv(\"stops_2012.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops13 = pd.read_csv(\"stops_2013.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops14 = pd.read_csv(\"stops_2014.csv\", encoding=\"utf-8\", low_memory=False)\n", "stops15 = pd.read_csv(\"stops_2015.csv\", encoding=\"utf-8\", low_memory=False)"], ["# Cut down to desired columns\n", "stops10 = stops10[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']]\n", "stops11 = stops11[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops12 = stops12[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops13 = stops13[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops14 = stops14[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] \n", "stops15 = stops15[['stop_date', 'county_name', 'driver_gender',\n", "        'driver_race', 'search_conducted', \n", "        'contraband_found', 'stop_outcome', 'officer_id']] "], ["# Clean column names\n", "stops10.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops11.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops12.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops13.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops14.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']\n", "stops15.columns = ['date', 'county', 'gender', 'race',\n", "       'searched', 'contraband', 'outcome', 'officer_id']"], ["# Group by date for total stops per day of year\n", "by_date10 = stops10.groupby(\"date\").count()\n", "by_date10 = by_date10[[\"county\"]]\n", "by_date10.columns = [\"stops_2010\"]\n", "\n", "# Reset index and remove year to leave just month/day\n", "by_date10.reset_index(inplace=True) \n", "by_date10[\"date\"] = by_date10[\"date\"].map(lambda x: str(x)[5:])\n", "\n", "# Group other dfs by date and reset index so they can be joined\n", "by_date11 = stops11.groupby(\"date\").count()\n", "by_date11.reset_index(inplace=True)\n", "by_date11 = by_date11[[\"county\"]]\n", "by_date11.columns = [\"stops_2011\"]\n", "\n", "by_date12 = stops12.groupby(\"date\").count()\n", "by_date12.reset_index(inplace=True)\n", "by_date12 = by_date12[[\"county\"]]\n", "by_date12.columns = [\"stops_2012\"]\n", "\n", "by_date13 = stops13.groupby(\"date\").count()\n", "by_date13.reset_index(inplace=True)\n", "by_date13 = by_date13[[\"county\"]]\n", "by_date13.columns = [\"stops_2013\"]\n", "\n", "by_date14 = stops14.groupby(\"date\").count()\n", "by_date14.reset_index(inplace=True)\n", "by_date14 = by_date14[[\"county\"]]\n", "by_date14.columns = [\"stops_2014\"]\n", "\n", "by_date15 = stops15.groupby(\"date\").count()\n", "by_date15.reset_index(inplace=True)\n", "by_date15 = by_date15[[\"county\"]]\n", "by_date15.columns = [\"stops_2015\"]"], ["# Add columns together for single dataframe\n", "by_date10[\"stops_2011\"] = by_date11[\"stops_2011\"]\n", "by_date10[\"stops_2012\"] = by_date12[\"stops_2012\"]\n", "by_date10[\"stops_2013\"] = by_date13[\"stops_2013\"]\n", "by_date10[\"stops_2014\"] = by_date14[\"stops_2014\"]\n", "by_date10[\"stops_2015\"] = by_date15[\"stops_2015\"]\n", "\n", "# Rename for clarity\n", "total_stops_by_year = by_date10\n", "total_stops_by_year.head()"], ["# Same stuff with grouping by gender\n", "by_gender = stops10.groupby(\"gender\").count()\n", "by_gender = by_gender[[\"outcome\"]]\n", "by_gender.columns = [\"stops_2010\"]\n", "\n", "by_gender11 = stops11.groupby(\"gender\").count()\n", "by_gender11 = by_gender11[[\"outcome\"]]\n", "by_gender11.columns = [\"stops_2011\"]\n", "\n", "by_gender12 = stops12.groupby(\"gender\").count()\n", "by_gender12 = by_gender12[[\"outcome\"]]\n", "by_gender12.columns = [\"stops_2012\"]\n", "\n", "by_gender13 = stops13.groupby(\"gender\").count()\n", "by_gender13 = by_gender13[[\"outcome\"]]\n", "by_gender13.columns = [\"stops_2013\"]\n", "\n", "by_gender14 = stops14.groupby(\"gender\").count()\n", "by_gender14 = by_gender14[[\"outcome\"]]\n", "by_gender14.columns = [\"stops_2014\"]\n", "\n", "by_gender15 = stops15.groupby(\"gender\").count()\n", "by_gender15 = by_gender15[[\"outcome\"]]\n", "by_gender15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_gender[\"stops_2011\"] = by_gender11[\"stops_2011\"]\n", "by_gender[\"stops_2012\"] = by_gender12[\"stops_2012\"]\n", "by_gender[\"stops_2013\"] = by_gender13[\"stops_2013\"]\n", "by_gender[\"stops_2014\"] = by_gender14[\"stops_2014\"]\n", "by_gender[\"stops_2015\"] = by_gender15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_gender.reset_index(inplace=True)\n", "by_gender.head()"], ["by_gender = by_gender.transpose()\n", "by_gender.reset_index(inplace=True)\n", "by_gender.columns = [\"year\", \"male\", \"female\"]\n", "by_gender = by_gender.drop(by_gender.index[0])\n", "by_gender[\"year\"] = by_gender[\"year\"].map(lambda x: str(x)[6:])\n", "by_gender"], ["total_stops_by_gender = by_gender"], ["# Same stuff with grouping by race\n", "by_race = stops10.groupby(\"race\").count()\n", "by_race = by_race[[\"outcome\"]]\n", "by_race.columns = [\"stops_2010\"]\n", "\n", "by_race11 = stops11.groupby(\"race\").count()\n", "by_race11 = by_race11[[\"outcome\"]]\n", "by_race11.columns = [\"stops_2011\"]\n", "\n", "by_race12 = stops12.groupby(\"race\").count()\n", "by_race12 = by_race12[[\"outcome\"]]\n", "by_race12.columns = [\"stops_2012\"]\n", "\n", "by_race13 = stops13.groupby(\"race\").count()\n", "by_race13 = by_race13[[\"outcome\"]]\n", "by_race13.columns = [\"stops_2013\"]\n", "\n", "by_race14 = stops14.groupby(\"race\").count()\n", "by_race14 = by_race14[[\"outcome\"]]\n", "by_race14.columns = [\"stops_2014\"]\n", "\n", "by_race15 = stops15.groupby(\"race\").count()\n", "by_race15 = by_race15[[\"outcome\"]]\n", "by_race15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_race[\"stops_2011\"] = by_race11[\"stops_2011\"]\n", "by_race[\"stops_2012\"] = by_race12[\"stops_2012\"]\n", "by_race[\"stops_2013\"] = by_race13[\"stops_2013\"]\n", "by_race[\"stops_2014\"] = by_race14[\"stops_2014\"]\n", "by_race[\"stops_2015\"] = by_race15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_race.reset_index(inplace=True)\n", "by_race.head()"], ["by_race = by_race.transpose()\n", "by_race.reset_index(inplace=True)\n", "by_race.columns = [\"year\", \"asian\", \"black\", \"hispanic\", \"other\", \"white\"]\n", "by_race = by_race.drop(by_race.index[0])\n", "by_race[\"year\"] = by_race[\"year\"].map(lambda x: str(x)[6:])\n", "by_race"], ["total_stops_by_race = by_race"], ["# Same stuff with grouping by county\n", "by_county = stops10.groupby(\"county\").count()\n", "by_county = by_county[[\"outcome\"]]\n", "by_county.columns = [\"stops_2010\"]\n", "\n", "by_county11 = stops11.groupby(\"county\").count()\n", "by_county11 = by_county11[[\"outcome\"]]\n", "by_county11.columns = [\"stops_2011\"]\n", "\n", "by_county12 = stops12.groupby(\"county\").count()\n", "by_county12 = by_county12[[\"outcome\"]]\n", "by_county12.columns = [\"stops_2012\"]\n", "\n", "by_county13 = stops13.groupby(\"county\").count()\n", "by_county13 = by_county13[[\"outcome\"]]\n", "by_county13.columns = [\"stops_2013\"]\n", "\n", "by_county14 = stops14.groupby(\"county\").count()\n", "by_county14 = by_county14[[\"outcome\"]]\n", "by_county14.columns = [\"stops_2014\"]\n", "\n", "by_county15 = stops15.groupby(\"county\").count()\n", "by_county15 = by_county15[[\"outcome\"]]\n", "by_county15.columns = [\"stops_2015\"]"], ["# Joining dataframes together\n", "by_county[\"stops_2011\"] = by_county11[\"stops_2011\"]\n", "by_county[\"stops_2012\"] = by_county12[\"stops_2012\"]\n", "by_county[\"stops_2013\"] = by_county13[\"stops_2013\"]\n", "by_county[\"stops_2014\"] = by_county14[\"stops_2014\"]\n", "by_county[\"stops_2015\"] = by_county15[\"stops_2015\"]\n", "\n", "# Reset index and rename for clarity\n", "by_county.reset_index(inplace=True)\n", "by_county.head()"], ["total_stops_by_county = by_county"], ["# Import SQLAlchemy dependencies\n", "import sqlalchemy\n", "from sqlalchemy import create_engine, MetaData, inspect\n", "from sqlalchemy.ext.declarative import declarative_base\n", "from sqlalchemy import Column, Integer, String, Numeric, Text, Float, ForeignKey\n", "from sqlalchemy.orm import sessionmaker, relationship"], ["engine = create_engine(\"sqlite:///speeding.sqlite\")"], ["Base = declarative_base()"], ["class Date_stops(Base):\n", "    \n", "    __tablename__ = 'date_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    date = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.date}: 2010 - {self.stops_2010}\"\n", "    \n", "class Gender_stops(Base):\n", "    \n", "    __tablename__ = 'gender_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    male = Column(Integer)\n", "    female = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.gender}: 2010 - {self.stops_2010}\"\n", "\n", "class Race_stops(Base):\n", "    \n", "    __tablename__ = 'race_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    year = Column(Text)\n", "    asian = Column(Integer)\n", "    black = Column(Integer)\n", "    hispanic = Column(Integer)\n", "    other = Column(Integer)\n", "    white = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.race}: 2010 - {self.stops_2010}\"\n", "\n", "class County_stops(Base):\n", "    \n", "    __tablename__ = 'county_stops'\n", "    \n", "    id = Column(Integer, primary_key=True, autoincrement=True)\n", "    county = Column(Text)\n", "    stops_2010 = Column(Integer)\n", "    stops_2011 = Column(Integer)\n", "    stops_2012 = Column(Integer)\n", "    stops_2013 = Column(Integer)\n", "    stops_2014 = Column(Integer)\n", "    stops_2015 = Column(Integer)\n", "    \n", "    def __repr__(self):\n", "        return f\"{self.county}: 2010 - {self.stops_2010}\""], ["Base.metadata.create_all(engine)"], ["engine.table_names()"], ["inspector = inspect(engine)\n", "inspector.get_columns('race_stops')"], ["# connect to the database\n", "conn = engine.connect()\n", "\n", "# Orient='records' creates a list of data to write\n", "date_data = total_stops_by_year.to_dict(orient='records')\n", "gender_data = total_stops_by_gender.to_dict(orient='records')\n", "race_data = total_stops_by_race.to_dict(orient='records')\n", "county_data = total_stops_by_county.to_dict(orient='records')\n", "\n", "# Insert the dataframe into the database in one bulk insert\n", "#conn.execute(Date_stops.__table__.delete())\n", "conn.execute(Date_stops.__table__.insert(), date_data)\n", "#conn.execute(Gender_stops.__table__.delete())\n", "conn.execute(Gender_stops.__table__.insert(), gender_data)\n", "#conn.execute(Race_stops.__table__.delete())\n", "conn.execute(Race_stops.__table__.insert(), race_data)\n", "#conn.execute(County_stops.__table__.delete())\n", "conn.execute(County_stops.__table__.insert(), county_data)\n", "\n", "conn.close()"], ["engine.execute(\"SELECT * FROM date_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM gender_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM race_stops LIMIT 5\").fetchall()"], ["engine.execute(\"SELECT * FROM county_stops LIMIT 5\").fetchall()"], []]}], "maf5509/accident_capstone": [{"Untitled.ipynb": [["# Prep Course Capstone Project - FARS Road Accident Study \n", "April 2018\n", "by Mark Ferguson\n", "\n", "The aim of this project is to analyze variations in road safety in the U.S. This will be done using a dataset containing details of fatal road accidents obtained from the National Hightway Traffic Safety Administration: <div class=\"alert alert-block alert-success\">(http://nhtsa.gov)</div>\n", "\n", "### Available Data\n", "\n", "The data examined are from the NHTSA's road fatality database, the Fatality Analysis Reporting System (FARS). This study concentrates on 2016 data, although there were datasets in the FARS repository going back as far as 1975. The repository can be found at: <div class=\"alert alert-block alert-success\">ftp://ftp.nhtsa.dot.gov/fars/</div> \n", "\n", "Each row in the dataset represents a particular fatal accident. Columns include the state in which the accident occurred, the number and types of vehicles involved, number of pedestrians involved, the time of day, the weather conditions, whether a driver was reported as being drunk at the scene, number of fatalities, and many others. The columns in the dataset are mostly coded; some of the columns' meanings could be deduced intuitively, whereas others had to be looked up. For this, it was necessary to consult the NHTSA's manual, which is located at: <div class=\"alert alert-block alert-success\">ftp://ftp.nhtsa.dot.gov/fars/FARS-DOC/Analytical%20User%20Guide/FARS%20Analytical%20Users%20Manual%201975-2016-822447.pdf</div>\n", "***\n", "### What Can We Determine from the Data?\n", "\n", "From an examination of the data, it was decided that a number of questions could be addressed. These are:\n", "\n", "1. In which states are fatal road accidents most and least likely to occur? Are any states particularly safe or dangerous?\n", "2. What is the correlation between alcohol (or more specifically, drunk-driving) and fatal accidents?\n", "3. What is the distribution of fatal accidents over time (i.e. during the week)?\n", "4. At what times of the day are fatal accidents more or less likely to occur?\n", "5. What would be some worthwhile directions for further investigation?\n", "\n", "The python code and table manipulation used are shown below. [The results of the analysis, related plots, and discussion start here.](#Results)"], ["### Import Modules"], ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import math\n", "import seaborn as sns\n", "from pylab import *\n", "warnings.filterwarnings('ignore')\n", "%matplotlib inline\n", "\n", "plt.style.use(['ggplot'])\n", "plt.rcParams['figure.figsize'] = (20.0, 10.0)"], ["print(plt.style.available)"], ["#Import raw data\n", "table = pd.read_csv(\"C://Users//fergu//accident.csv\")"], ["table.describe()"], ["# DISCARD\n", "#states_proper = pd.read_csv('C://Users//fergu//States_Proper.csv')\n", "#states_proper.columns = ['NUM', 'NAM', 'COD']"], ["# DISCARD\n", "#print(states_proper.head())"], ["table['NAME_FIN'] = 0\n", "table['CODE_FIN'] = 0\n", "table.head()"], ["#DISCARD\n", "#state_counts = table['STATE'].value_counts()"], ["states = list(table['STATE']) "], ["# Create list of unique state codes\n", "states_final = np.unique(states)\n", "print(states_final)"], ["states_dict = {}\n", "for item in states_final:\n", "     states_dict[item] = 0\n", "print(states_dict)"], ["states = pd.read_csv('C://Users//fergu//States.csv', na_values = 'NaN')"], ["states.columns = ['code', 'state', 'wildcard']"], ["# State names not properly formatted\n", "states[33:34]"], ["# Add the second word of state names to the first column, e.g. 'Rhode' becomes 'Rhode Island'\n", "\n", "for i in range(0, len(states.wildcard) - 1):        \n", "       if type(states.wildcard[i]) != float:\n", "           states.state[i] += ' ' + str(states.wildcard[i])        "], ["states[33:34]"], ["# DISCARD\n", "#print(states_dict)"], ["# Create dictionary with keys corresponding to the numbered codes in the original import file \n", "\n", "for key in states_dict:\n", "    for i in range(0, len(states.code)):\n", "        if key == states['code'][i]:\n", "            states_dict[key] = states['state'][i]\n", "            break\n", "print(states_dict)\n", "print(len(states.code))"], ["table['ST_NAME'] = ''\n", "table.head()"], ["state_codes = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']"], ["# Create second dictionary with state abbreviations instead of full names\n", "\n", "states_dict2 = {}\n", "for i in range (0, len(state_codes)):\n", "    states_dict2[i] = state_codes[i]\n", "print(states_dict2)\n", "print(states_dict)"], ["## Third dictionary contains full state names and abbreviations. This will be used to populate new column with abbreviations\n", "\n", "def change_keys(dict1, dict2):\n", "    list1 = []\n", "    list2 = []\n", "    dict_final = {}\n", "    for key in dict1:\n", "        list1.append(dict1[key])\n", "    for key in dict2:\n", "        list2.append(dict2[key])\n", "    for i in range(0, len(list1)):\n", "        dict_final[str(list1[i])] = str(list2[i])\n", "#       dict_final[str([list1][i])] = str(list2[i])\n", "    return dict_final\n", "\n", "dict_final = change_keys(states_dict, states_dict2)\n", "print(dict_final)"], ["# Write to new column 'ST_NAME' showing full state names, using dict_final created earlier\n", "state_list = []\n", "for i in range(0, len(table['STATE'])):\n", "    state_list.append(states_dict[table['STATE'][i]])\n", "table['ST_NAME'] = state_list\n", "table.head()    "], ["# Write to new column 'CODE' showing state abbreviations, also using dict_final\n", "state_list2 = []\n", "for j in range(0, len(table['ST_NAME'])):\n", "    state_list2.append(dict_final[table['ST_NAME'][j]])\n", "table['CODE'] = state_list2\n", "table['TOTAL'] = 1\n", "table.head()"], ["# Create binary column, DRUNK_YES - 0 or 1. Was a drunk driver involved or not.\n", "drunk_list = []\n", "for i in range(0, len(table['DRUNK_DR'])):\n", "    if table['DRUNK_DR'][i] == 0:\n", "        drunk_list.append(0)\n", "    else:\n", "        drunk_list.append(1)\n", "table['DRUNK_YES'] = drunk_list"], ["table.head()"], ["test = table.groupby(['CODE'])"], ["test_bar = test.agg('sum')"], ["# This aggregation tells us the total number of fatal accidents, by state, that involved drunk driver(s)\n", "# Also add column 'NOT DRUNK' to compare no. of accidents where a drunk driver was not reported\n", "test_bar2 = test_bar.reset_index()\n", "test_bar2['NOT_DRUNK'] = test_bar2['TOTAL'] - test_bar2['DRUNK_YES']\n", "test_bar2.head()"], ["***\n", "## Results\n", "\n", "Having sufficiently cleaned the data, we were able to produce some plots and address the [questions raised in the introduction](#What-Can-We-Determine-from-the-Data?). The first plot below shows the number of fatal\n", "road accidents that occurred in 2016, by state. As we can, see, Texas, California and Florida had the most fatal traffic accidents by a wide margin. This could be explained in part by the fact that these states are some of the most populous states in the US. This theory will\n", "be looked at and tested in the following sections.\n"], ["### Drunk Driving vs Non Drunk Driving Accidents, State by State\n", "\n", "An obvious place to start was the raw numbers of fatal accidents, and to split them between drunk- and non-drunk driving cases:"], ["x = np.arange(len(test_bar2['CODE']))\n", "y1 = test_bar2['DRUNK_YES']\n", "y2 = test_bar2['NOT_DRUNK']\n", "width = 0.6\n", "p1=plt.bar(x, y1, width)\n", "p2=plt.bar(x, y2, width, bottom=y1)\n", "plt.xticks(x, test_bar2['CODE'])\n", "plt.ylabel(\"Number of Fatal Accidents\")\n", "plt.title(\"2016 Fatal Traffic Accidents by State, Split by Alcohol Involvement\")\n", "plt.legend((p1[0], p2[0]), ('Alcohol-Related', 'Non-Alcohol-Related'))\n", "plt.show()"], ["### Findings from Alcohol vs. Non Alcohol Related Data\n", "\n", "<div class=\"alert alert-block alert-info\"> \n", "We are defining an \"alcohol-related accident\" as one in which one or more drivers were reported as \"drunk\". Presumably this would be reported as such only if the person's Blood Alcohol Concentration (BAC) exceeded the legal threshold; but it is possible that alcohol may have influenced the outcome of an accident even if the driver was not reported as drunk. The nature of the FARS data gives us no way to quantify this. The timing of the blood alcohol testing is also unknowable; the person's BAC may be below the limit at the time they are tested, even if it was above it when the accident occurred.\n", "</div>\n", "\n", "-  Most fatal traffic accidents are not alcohol-related.\n", "-  However, in some of the smaller states, e.g. Alaska, North Dakota and Vermont, the proportion of alcohol related accidents is much higher.\n", "-  Since those states have smaller poulation sizes, more data would be needed (perhaps multi-year data) to investigate this further.\n", "-  By eye, New York's number of fatal accidents seems much lower than would expected for such a large state. \n", "-  This apparent anomaly might be explained by more and better public transport options in NY.\n", "-  To compare states against one another properly, we need to look at some detailed population data."], ["### Comparison of State Totals vs. US Population Data\n", "\n", "As indicated, the high numbers of fatal accidents in TX, FL and CA could perhaps be because of higher populations \n", "in those states. We therefore check the numbers against estimated 2017 US population data:\n", "    \n", "    \n", "<div class=\"alert alert-block alert-success\">(https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population_growth_rate)</div>    \n", "    \n", "...and in fact find this to be a reasonable supposition, as the pie chart below shows."], ["# Create new table with population data by state\n", "\n", "state_pop = pd.read_csv('C://Users//fergu//us_pop_by_state.csv')\n", "state_pop2017 = state_pop[['State', '2017 estimate']]\n", "state_pop2017.head()"], ["threshold = 4.0  # Set threshold above which states will show individually\n", "biggest_states = []\n", "pop_biggest = []\n", "others = []\n", "pop_others = []\n", "others_total = 0\n", "\n", "# If (state pop / total pop) is greater than threshold, state qualifies as one of the 'biggest states'\n", "# If not, state is classified as one of the 'others'\n", "for i in range(0, len(state_pop2017['2017 estimate'])):\n", "    if state_pop2017['2017 estimate'][i] / state_pop2017['2017 estimate'].sum() > (threshold / 100):\n", "        biggest_states.append(state_pop2017['State'][i])\n", "        pop_biggest.append(state_pop2017['2017 estimate'][i])\n", "    else:\n", "        others.append(state_pop2017['State'][i])\n", "        pop_others.append(state_pop2017['2017 estimate'][i])\n", "        \n", "others_total = sum(pop_others)\n", "all_states = []\n", "all_pop = []\n", "\n", "# Add each biggest state and its pop to the all_states list\n", "for j in range(0, len(biggest_states)):\n", "    all_states.append(biggest_states[j])\n", "    all_pop.append(pop_biggest[j])\n", "\n", "# Add all other combined states and their combined pop to the all_states list    \n", "all_states.append('All Others')\n", "all_pop.append(others_total)\n", "\n", "# Create new table housing all the pop data: biggest states individually, and all others combined\n", "state_popfinal = pd.DataFrame()\n", "state_popfinal['State'] = all_states\n", "state_popfinal['2017 estimate'] = all_pop\n", "state_popfinal\n", " \n", "# Shows the biggest states (as defined at top) as exploded slices:    \n", "state_popfinal['explode']=0.0\n", "expl_ind = 0.25      \n", "for k in range(0, len(state_popfinal)):\n", "    if state_popfinal['State'][k] in biggest_states:\n", "        state_popfinal['explode'][k] = expl_ind\n", "\n", "e = state_popfinal['explode']  \n", "\n", "pie(state_popfinal['2017 estimate'],labels=state_popfinal['State'],explode=e,autopct='%1.0f%%')\n", "plt.title('Estimated 2017 US Population by State')\n", "font = {'family' : 'normal',\n", "        'weight' : 'normal',\n", "        'size'   : 14}\n", "\n", "matplotlib.rcParams['axes.titlepad'] = 40\n", "matplotlib.rc('font', **font)\n", "plt.show()"], ["### Findings In the Light of the State-by-State Population Data \n", "\n", "-  The four most populous states in the US are in fact TX, NY, FL and CA. \n", "-  Traffic accident data may correlate better with number of cars owned per head or per household, or with miles driven, than with population. \n", "\n", "We will next look at the 2016 fatality data adjusted for total miles driven, and then briefly at law enforcement policy, to see how this might be influencing the rate at which fatal traffic accidents occur."], ["### A Closer Look At Road Fatalities vs. Passenger Miles Driven\n", "\n", "The data below give a more meaningful way to compare the data between states. The following chart shows the data adjusted for population and, most important, total miles driven:"], ["adjust_fig = pd.read_csv('C://Users//fergu/acc_adjstate.csv') # Import data from IIHS (see discussion below)\n", "adjust_fig.columns = ['state', 'pop', 'miles', 'crash_fat', 'fatal', 'deaths_per_100k_ppl', 'deaths_per_100mm_mi']\n", "\n", "adjust_state = list(adjust_fig['state'])\n", "\n", "adjust_fig['state'][adjust_state.index('District of Columbia')] = 'DC' # Change entry from 'District of Columbia' to 'DC' \n", "\n", "# Add new column with state name abbreviations\n", "# Remove a trailing space on some state name entries\n", "temp_list = []\n", "for i in range(0, len(adjust_fig['state'])):\n", "    if adjust_fig['state'][i][-1] == ' ':\n", "                  temp_list.append(dict_final[adjust_fig['state'][i][:-1]])\n", "    else:\n", "                  temp_list.append(dict_final[adjust_fig['state'][i]])\n", "    \n", "adjust_fig['state2'] = temp_list\n", "adjust_fig['pop_10k'] = adjust_fig['pop'] / 10000 # Divide pop by 10000 for best sizing of dots on chart\n", "adjust_fig.head()\n"], ["mi = adjust_fig['miles']\n", "fa = adjust_fig['crash_fat']\n", "pop_10k = adjust_fig['pop_10k']\n", "plt.scatter(mi, fa, sizes=pop_10k, color='green', alpha=0.5)\n", "s2 = list(adjust_fig['state2'])\n", "for item in state_codes:\n", "    plt.text(adjust_fig['miles'][s2.index(item)] - 2000, adjust_fig['crash_fat'][s2.index(item)], str(item))\n", "plt.title('2016 Fatal Accidents by State vs. Total Passenger Miles Driven')\n", "plt.xlabel('Total Passenger Miles Driven (millions)')\n", "plt.ylabel('Number of Fatal Accidents')\n", "plt.show()"], ["### Conclusions on State Totals vs. Population\n", "\n", "The plot above shows the number of 2016 fatal accidents (y-axis) vs millions of passenger miles driven (x-axis). The dots are sized according to the populations of the states. These data were obtained from the Insurance Institute for Highway Safety:\n", "\n", "<div class=\"alert alert-block alert-success\">http://www.iihs.org/iihs/topics/t/general-statistics/fatalityfacts/state-by-state-overview</div>\n", "\n", "-  The plot shows a clear increase in number of fatal accidents with increasing miles driven,as would be expected.\n", "-  With the rising trend in fatal accidents vs. miles driven, the increasing sizes of the dots shows a correlation between population and both miles driven and number of fatal accidents.\n", "-  New York's low number of accidents in relation to its population can be understood in the light of its relatively low number of miles driven, [as theorized earlier](#Findings-from-Alcohol-vs.-Non-Alcohol-Related-Data). In fact, the state even falls some way below the number one would expect if looking at the 'national trend.'\n", "-  California falls far below the national trend, with a much lower number of fatal accidents than its number of passenger miles would suggest.\n", "-  An alternative view would be that it is Florida and Texas that are ABOVE the number of fatal accidents that would be expected.\n", "- How best to plot and extrapolate the trendline will probably be a topic covered later in this course.\n"], ["### California vs. Texas: A Comparison\n", "\n", "\n", "Clearly the California data represent quite a large and significant anomaly from the national trend of fatal accidents vs. total miles driven. The Texas A&M Transportation Institute has highlighted the disparity in recent years between Texas and California Road Fatalities in relation to the states' populations, and has pointed to some possible reasons for it:\n", "\n", "<div class=\"alert alert-block alert-success\">http://ftp.dot.state.tx.us/pub/txdot-info/trf/trafficsafety/engineering/comparative-analysis.pdf</div>\n", "\n", "This report highlighted the following factors in explaining this disparity:\n", "\n", "-  A strong Motorcycle Safety campaign in California over the last decade, plus a strict helmet law. According to the Institute, motorcyclists are 26 times more likely than passenger car occupants to die in motor vehicle crashes.\n", "\n", "-  California's stricter stance on cell phone use while driving, with a ban introduced in 2008. Texas, by contrast, did not ban phone use while driving until 2017, as reported by the Fort Worth Star-Telegram:\n", "\n", "<div class=\"alert alert-block alert-success\">http://www.star-telegram.com/news/politics-government/state-politics/article170457212.html</div>\n", "\n", "The FARS data do not show how many accidents could be considered to have involved a distracted driver. Clearly there are any number of factors that may also be contributing to the disparity between these states. The design and layout of the roads, traffic density, driving speed, driving education, and quality of medical care are just a few of the many factors that the report does not consider.\n"], ["### The Effect of Weather\n", "\n", "The FARS table also contained a field showing the reported weather condition. Below we show the breakdown of these conditions:"], ["# Scatter plot for weather conds\n", "\n", "accept_weath = [1,2,3,4,5,6,7,10,11,12]\n", "weather_names = ['Clear', 'Rain', 'Sleet/Hail', 'Snow', 'Fog/Smoke', 'Severe Crosswind', 'Blwng Debris', \n", "                 'Cloudy', 'Blwng Snow', 'Frzg Rain/Drizzle']\n", "\n", "# Create dictionary with coded numbers and descriptions of weather conditions\n", "weather_dict = {}\n", "for i in range(0, len(accept_weath)):\n", "    weather_dict[accept_weath[i]] = weather_names[i]\n", "\n", "weather_list = []\n", "weather_count = []\n", "\n", "table_w = table\n", "table_w['WEATHER_COUNTS'] = 0\n", "\n", "# Populate weather_list with the name of reported weather condition, read from the dictionary: \n", "for i in range(0, len(table_w['WEATHER'])):\n", "    if table_w['WEATHER'][i] in accept_weath:\n", "        weather_list.append(weather_dict[table_w['WEATHER'][i]])\n", "        \n", "# Create new table and add new column with weather condition as words instead of numbers, read from weather_list#\n", "# Add 'COUNT' column for aggregation purposes\n", "table_wx = pd.DataFrame()\n", "table_wx['WEATHER_NAME'] = weather_list\n", "table_wx['COUNT'] = 1\n", "table_wx.head()\n", "table_wxagg = table_wx.groupby('WEATHER_NAME').sum().reset_index()\n", "table_wxagg\n", "\n", "# Convert relevant columns to lists and plot bar chart from these\n", "w_name = list(table_wxagg['WEATHER_NAME'])\n", "y = list(table_wxagg['COUNT'])\n", "x = np.arange(len(w_name))\n", "plt.bar(x,y)\n", "\n", "#labels = ['a','b','c','d','e','f','g','h','i','j']\n", "plt.xticks(x,w_name)\n", "\n", "\n", "plt.title('2016 Fatal Accidents by Weather According to Numbered System')\n", "plt.ylabel('Number of Fatal Accidents')\n", "plt.xlabel('Primary Reported Weather Condition')\n", "plt.show()"], ["### Some Notes on The Effect of Weather\n", "\n", "-  The majority of fatal accidents were reported as occurring in clear conditions.\n", "-  A surprisingly low number of accidents occurred in adverse conditions.\n", "-  The low number of fatal accidents in bad weather could be because of drivers taking extra care due to the conditions."], ["# From hour and minute columns, make a single, fractional time in hours\n", "# So from ['Hour'] = 9 and ['Minute'] = 45 we get ['TIME_FRAC'] = 9.75  \n", "table['TIME_FRAC'] = 0\n", "updated_table = table[table['HOUR'] <= 23] \n", "updated_table = table[table['MINUTE'] <= 59]\n", "updated_table['TIME_FRAC'] = updated_table['HOUR'] + updated_table['MINUTE'] / 60"], ["updated_table.head()"], ["# Define daytime ['DAY_HR'] from 6am to 6pm as most intuitive division between day and night\n", "\n", "updated_table2 = updated_table.reset_index()\n", "\n", "list_time = []\n", "for i in range(0, len(updated_table2['TIME_FRAC'])):\n", "    if updated_table2['TIME_FRAC'][i] < 6.0:\n", "        list_time.append(updated_table2['TIME_FRAC'][i] + 18.0)\n", "    else:\n", "        list_time.append(updated_table2['TIME_FRAC'][i] - 6)\n", "    \n", "updated_table2['DAY_HR'] = list_time"], ["updated_table2['DAY_HR'] = updated_table2['DAY_HR'].round(2)\n", "updated_table2['TIME_FRAC'] = updated_table2['TIME_FRAC'].round(2)"], ["updated_table2.head()"], ["# Separate into two tables, \"drunk\" and \"sober\", and sample each of these\n", "updated_table2_dru = updated_table2[updated_table2['DRUNK_YES'] == 1]\n", "updated_table2_sob = updated_table2[updated_table2['DRUNK_YES'] == 0]\n", "\n", "sample_dru = np.random.choice(updated_table2_dru['DAY_HR'], 1000)\n", "sample_sob = np.random.choice(updated_table2_dru['DAY_HR'], 1000)"], ["### Do Alcohol- and Non-Alcohol-Related Accidents Occur At Different Times of Day?\n", "\n", "We have created sub-tables that isolate the accidents in which one or more drunk drivers were reported \n", "to have been involved, in order to see any differences between the two datasets. The results of this \n", "are shown in the plot below. \n", "\n", "<div class=\"alert alert-block alert-info\"> \n", "Note that for this analysis and for the plots that follow, the 'day' was defined as being from 0:00 to 23:59 as per the standard 24-hour clock. However, the t-test will define the day as 06:00 to 17:59 since that is the more conventional way in which the daytime is considered.\n", "</div>\n", "\n", "The conclusions arising from this analysis are as follows:\n", " \n", "-  Here, the median is considered a better measure of central tendency than the mean, since it gives no greater numerical weighting to later times than earlier ones. \n", "-  [As highlighted earlier](#Findings-from-Alcohol-vs.-Non-Alcohol-Related-Data), the majority of accidents do not involve drunk drivers.\n", "-  The median drunk driving accident occurred at around 5 pm.\n", "-  The median non-drunk driving accident occurred at around 10 pm."], ["plt.hist(updated_table2_dru['TIME_FRAC'], bins=24, color='r', alpha=0.5)\n", "plt.hist(updated_table2_sob['TIME_FRAC'], bins=24, color='y', alpha=0.5)\n", "plt.title('Histograms Show Time of Day for Drunk- and Non-Drunk-Driving Accidents')\n", "plt.xlabel('Time of Day (0 = Midnight)')\n", "plt.ylabel('No. of Fatal Accidents')\n", "plt.legend(['Drunk', 'Non-Drunk'])\n", "dru_mean = updated_table2_dru['TIME_FRAC'].mean()\n", "sob_mean = updated_table2_sob['TIME_FRAC'].mean()\n", "dru_std = updated_table2_dru['TIME_FRAC'].std()\n", "sob_std = updated_table2_sob['TIME_FRAC'].std()\n", "\n", "print('Sober median time of occurrence: {}'.format(updated_table2_sob['TIME_FRAC'].median()))\n", "print('Drunk median time of occurrence: {}'.format(updated_table2_dru['TIME_FRAC'].median()))\n", "plt.show()"], ["boxtab_dru = updated_table2_dru.reset_index()\n", "boxtab_sob = updated_table2_sob.reset_index()\n", "\n", "plt.figure(figsize=(18,6))\n", "\n", "plt.subplot(1,2,1)\n", "plt.boxplot(boxtab_sob['TIME_FRAC'])\n", "plt.ylabel('Time of Day (Hr)')\n", "plt.title('Time of Accident Occurrence (Not Drunk)')\n", "\n", "plt.subplot(1,2,2)\n", "plt.boxplot(boxtab_dru['TIME_FRAC'])\n", "plt.ylabel('Time of Day (Hr)')\n", "plt.title('Time of Accident Occurrence (Drunk)')\n", "\n", "plt.show()"], ["### Interpretation of Accident Time Boxplots\n", "\n", "Looking at the boxplots, the following points are noteworthy here:\n", "\n", "-  The drunk data show a much bigger interquartile range, in particular a much spread between the first quartile and the median.\n", "-  This long lower median - Q1 range could be because a person who is drunk is more likely to be out (and apparently, sadly, driving) in the early hours of the morning than a sober person.\n", "-  The two datasets show a similar median at around 15:00.\n", "-  The range between third quartile and the median is approximately the same size in both datasets. Evening rush could be quite a dangerous time to be on the road due to a high volume of traffic on the road, high stress/inattention/fatigue levels and the fact that people are in a hurry to get home.  \n"], ["### t-test on Population Samples From Drunk and Non-Drunk Populations\n", "\n", "As shown by the code and tables below, a t-test on the samples from the drunk and non-drunk data yields a p-value of 0.51. The upper limit for statistical significance for a p-value is 0.05. Thus the difference in sample means is most likely due to sample noise, rather than to any statistically significant difference between the population means."], ["# Compute standard error of the combined samples\n", "def st_error(s1, n1, s2, n2):\n", "    return ((s1**2/n1) + (s2**2/n2))**0.5\n", "\n", "sample_dru_mean = sample_dru.mean()\n", "sample_sob_mean = sample_sob.mean()\n", "\n", "sample_dru_std = sample_dru.std()\n", "sample_sob_std = sample_sob.std()\n", "size = 1000\n", "\n", "a = sample_dru_mean-sample_sob_mean # y_bar\n", "b = (sample_dru_std**2/size + sample_sob_std**2/size)**0.5 # standard error of difference\n", "c = a / b\n", "\n", "print('The mean of the drunk sample is: {}'.format(sample_dru_mean))\n", "print('The mean of the sober sample is: {}'.format(sample_sob_mean))\n", "print('The y_bar value (difference between the sample means) is: {}'.format(a))\n", "print('y_bar / se: {}'.format(c))\n", "print('The combined standard error of these samples is {}'.format(b))"], ["from scipy.stats import ttest_ind\n", "np.random.seed(450)\n", "ttest_ind(sample_dru, sample_sob, equal_var=False)"], ["updated_table2.head()"], ["### Is There a Correlation Between Number of People/ Vehicles Involved In Crashes and the Time of Day?\n", "\n", "It was decided that in order to test for a link between the time of day, and the number of persons and vehicles involved in the accident. The next two scatter plots indicate the following:\n", "\n", "-  Although hard to detect, accidents involving large numbers of people tend to occur with greater frequency between the hours of 8 am and 10 pm.\n", "-  The data also indicate a greater frequency of accidents with larger numbers of vehicles involved within the same time interval.\n", "-  These trends are loosely matched by the non-drunk data (the majority of 2016 fatal accidents) which show a higher occurrence between approx. 1 pm and 9 pm. \n", "-  The pattern for lower numbers of vehicles is not possible to judge visually due to the much higher data density.\n", "\n", "<div class=\"alert alert-block alert-info\"> \n", "The plots below have their y-axes capped at 20, because the number of accidents involving greater than this number of persons and/ or vehicles is negligibly small. The upper quartile (75%) values in the following summary stats point to this. \n", "</div>"], ["updated_table2['PERSONS'].describe(), updated_table2['VE_TOTAL'].describe()"], ["### Does the number of Vehicles Involved Vary by Time of Day?"], ["x = updated_table2['TIME_FRAC']\n", "y = updated_table2['VE_TOTAL']\n", "plt.scatter(x,y)\n", "plt.xlim(0,24)\n", "plt.ylim(0,20)\n", "plt.xlabel('Time of Day (Hour)')\n", "plt.ylabel('Number of Vehicles Involved in Accident')\n", "plt.title('A Plot of Vehicles Involved vs. Time of Day')\n", "plt.show()"], ["### Does the number of People Involved Vary by Time of Day?"], ["x = updated_table2['TIME_FRAC']\n", "y = updated_table2['PERSONS']\n", "plt.scatter(x,y)\n", "plt.xlim(0,24)\n", "plt.ylim(0,20)\n", "plt.xlabel('Time of Day (Hour)')\n", "plt.ylabel('Number of Persons Involved in Accident')\n", "plt.title('A Plot of Persons Involved vs. Time of Day')\n", "plt.show()"], ["### t-test on Population Samples From Two Different Time Intervals During the Day\n", "\n", "To examine whether the number of persons involved varies according to the time of day, we define two time periods, 7am-12pm and 7pm-12am. We will sample these periods and perform a t-test on the data from these periods in order to assess whether the difference in their means reflects a statistically significant difference between the populations. "], ["# Designate each record as being within one of 3 time categories: 7am-12pm, 7pm-12am or other ('NO')\n", "intervals = []\n", "\n", "for k in range(0, len(updated_table2)):\n", "    if updated_table2['TIME_FRAC'][k] >= 7.0 and updated_table2['TIME_FRAC'][k] < 12.0:\n", "        intervals.append('7AM_12PM')\n", "    elif updated_table2['TIME_FRAC'][k] >= 19.0 and updated_table2['TIME_FRAC'][k] < 24.0:\n", "        intervals.append('7PM_12AM') \n", "    else:\n", "        intervals.append('NO')\n", "        \n", "updated_table2['INTERVAL'] = intervals\n", "\n", "table_am = updated_table2[updated_table2['INTERVAL'] == '7AM_12PM']\n", "table_pm = updated_table2[updated_table2['INTERVAL'] == '7PM_12AM']\n", "\n", "# Sample the two populations of interest (7-12 am and 7-12 pm)\n", "size = 1000\n", "sample_am = np.random.choice(table_am['PERSONS'], size)\n", "sample_pm = np.random.choice(table_pm['PERSONS'], size)\n", "\n", "se = (sample_am.std()**2 / size + sample_pm.std()**2 / size)**0.5 # Standard error of the combined samples\n", "y_bar = sample_am.mean() - sample_pm.mean()                       # Difference between the two sample means\n", "\n", "print('The mean of sample_am is: {}'.format(sample_am.mean()))\n", "print('The mean of sample_pm is: {}'.format(sample_pm.mean()))\n", "print('The y_bar value (difference between the sample means) is: {}'.format(sample_am.mean() - sample_pm.mean()))\n", "print('y_bar / se: {}'.format(y_bar / se))\n", "\n", "\n", "print('The combined standard error of these samples is {}'.format(se))\n", "\n", "np.random.seed(450)\n", "\n", "#Compute t-value:\n", "ttest_ind(sample_am, sample_pm, equal_var=False)\n"], ["### Analysis of Persons Involved, Time-of-Day t-test Results\n", "\n", "The p-value of 0.067 for this t-test falls just above the threshold for statistical significance. Recall what we are looking at: the number of vehicles involved, in fatal accidents in two samples taken from two populations at different times of the day (7am - 12pm, and 7pm - 12am). Thus, the 0.067 p-value indicates that the difference between the sample means could simply be a result of random variation in the individual sample populations, rather than a real difference between them."], ["### What Are The Best and Worst Times To Travel?\n", "\n", "Combining inferences from the histograms below, we can draw a conclusion as to the safest and most dangerous times to be on the road:\n", "    \n", "-  The most dangerous time to be on the road is Saturday between 6 - 7 pm.\n", "-  The safest time to be on the road is Tuesday between 4 - 5 am.\n", "-  Intuitively, these findings could be  a function of a disparity between the volumes of traffic on the road at these times. It would be interesting to be able to adjust for this. Unfortunately there were no data available on the variation in traffic density during the day or week.\n"], ["plt.figure(figsize=(18,6))\n", "\n", "plt.subplot(1,2,1)\n", "plt.hist(updated_table['DAY_WEEK'], bins=7, color='c')\n", "plt.xlabel('Day of the Week (1 = Sunday)')\n", "plt.ylabel('Number of 2016 Fatal Road Accidents')\n", "\n", "plt.subplot(1,2,2)\n", "plt.hist(updated_table['TIME_FRAC'], bins=24, color='violet')\n", "plt.xlabel('Time of Day (Midnight = 0)')\n", "plt.ylabel('Number of 2016 Fatal Road Accidents')\n", "plt.show()"], ["### Closing Remarks\n", "\n", "A study of this nature tends to raise as many questions as it answers. A number of questions arose during project that would be an interesting focus for further investigation. These include, but are not limited to:\n", "    \n", "-  How the likelihood of fatal accidents varies geographically within a state or county, i.e. are some zipcodes more dangerous than others.\n", "-  How the traffic density at given times affects fatal accident statistics.\n", "-  The influence of state laws and penalties on accident incidence [(briefly alluded to here)](#California-vs.-New-York:-A-Comparison).\n", "-  Whether the trends seen here for fatal accidents are matched by those for non-fatal accidents.\n", "-  Whether drivers involved in fatal accidents tend to have prior offences on their record.\n", "-  Are any socio-economic groups, professions or age groups over- or under-represented among drivers in fatal accidents?\n", "-  How has the proliferation of mobile phones and other technologies such as GPS affected the incidence of traffic accidents over the last 20 years? \n"]]}], "dariusmehri/Risk-Screening-Tool-to-Predict-Accidents-at-Construction-Sites": [{"Site+Safety+Analysis+2+AUTOMATION+2 (1).ipynb": [["READ PERMIT DATA"], ["from datetime import datetime\n", "print str(datetime.now())\n", "\n", "import pandas as pd\n", "#import logging\n", "pd.set_option('chained_assignment', None)\n", "\n", "from datetime import datetime\n", "print str(datetime.now())\n", "\n", "#logging.basicConfig(filename=\"I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\automation_log.log\",level=logging.DEBUG)\n", "#logging.debug('This message should go to the log file')\n", "#logging.warning('And this, too')\n", "\n", "#text_file = open(\"I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\automation_log.txt\", \"a\" )\n", "\n", "\n", "#string = str(datetime.now()) + \"...\" + \"Reading active permit data\"  + \"\\n\"\n", "#print string\n", "#text_file.write(\" \\n\")\n", "#text_file.write(\"******\\n\")\n", "#text_file.write(string)\n", "\n", "\n", "\n", "\n", "\n", "def PermitDataInput():\n", "    #import datetime\n", "    #import time\n", "    #from datetime import date\n", "    #from dateutil.relativedelta import relativedelta\n", "    #from dateutil import parser\n", "\n", "    #permits issued\n", "    df = pd.read_csv('I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\sourceData\\\\active permitsAuto.csv', low_memory=False)\n", "    \n", "    \n", "    \n", "    \n", "    #drop columns\n", "    df = df.drop('Borough Name', 1)\n", "    df = df.drop('Current Job Status', 1)\n", "    df = df.drop('Permit Filing Status (Raw)', 1)\n", "    df = df.drop('Permit Sequence Number Max Flag', 1)\n", "    df = df.drop('issue_DATE', 1)\n", "    df = df.drop('issue_YEAR', 1)\n", "    df = df.drop('Permit Filing Status', 1)\n", "    df = df.drop('Job-Permit', 1)\n", "    \n", "    #df = df.drop('Applicant Business Name', 1)\n", "    \n", "    dropList =   [\"Applicant First Name\", \"Applicant Last Name\", \"Applicant License Number\", \n", "                  \"Applicant License Type\", \"Applicant Middle Initial\"]   \n", "\n", "    df = df.drop(dropList, 1)\n", "    \n", "    \n", "  \n", "\n", "\n", "    print \"length before drop\", len(df)\n", "\n", "    before = len(df)\n", "\n", "    df = df.dropna(subset = ['expire_DATE'])\n", "\n", "    print \"length after drop\", len(df)\n", "\n", "    print \"number dropped\", before - len(df)\n", "\n", "    df = df.reset_index(drop=True)\n", "    \n", "    #ts = time.time()\n", "    #st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n", "    #print \"Current Date\", st\n", "  \n", "    #df['expire_DATE'] = pd.to_datetime(df['expire_DATE'])\n", "    #df = df.sort_values(by = 'expire_DATE', ascending=True).reset_index(drop=True)\n", "    \n", "    #df = df[(df['expire_DATE'] >= st) ]\n", "    \n", "    df = df.rename(columns={'Latitude Point': 'Latitude', 'Longitude Point': 'Longitude', 'Borough Name.1': 'Borough',\n", "                           'Applicant Business Name':'General Contractor'})\n", "\n", "    df = df.reindex_axis(sorted(df.columns), axis=1)\n", "    \n", "    \n", "    return df\n", "\n", "\n", "#print \"Reading BIS job filing data ...\"\n", "#df = PermitDataInput()\n", "\n", "#print df.dtypes\n", "\n", "\n", "#IMPORT DOB NOW DATA\n", "def DNDataInput():\n", "    df = pd.read_csv('I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\sourceData\\\\Active Permits - DOB NOWAuto.csv', low_memory=False)\n", "    \n", "    df[\"Permit Type\"] = \"EQ\"\n", "    df[\"Proposed Stories\"] = \"\"\n", "    df = df.rename(columns={'Work Type Name': 'Permit Sub Type', 'ExisitingBuildingStories':'Existing Stories',\n", "                           'Permit Expiration Date':'expire_DATE', 'JobType':'Job Type', \n", "                            'Business Name':'General Contractor'})\n", "    \n", "    \n", "    \n", "    df = df.drop('Sequence Number', 1)\n", "    df = df.drop('Permit Issued Date', 1)\n", "    df = df.drop('Filing Type', 1)\n", "    df = df.drop('Filing Status', 1)\n", "    \n", "    df = df.drop('Last Name', 1)\n", "    df = df.drop('First Name', 1)\n", "    df = df.drop('MiddleI nitial', 1)\n", "    #df = df.drop('Business Name', 1)\n", "    \n", "    df = df.drop('License', 1)\n", "    df = df.drop('License Number', 1)\n", "    \n", "    df['Permit Sub Type'] = df['Permit Sub Type'].str.replace('Side Walk Shed', 'SH')\n", "    df['Permit Sub Type'] = df['Permit Sub Type'].str.replace('Scaffold', 'SF')\n", "    df['Permit Sub Type'] = df['Permit Sub Type'].str.replace('Fence', 'FN')\n", "    \n", "    df = df.rename(columns={'Bin': 'BIN Number'})\n", "    df = df.reindex_axis(sorted(df.columns), axis=1)\n", "    \n", "    print \"len of dob now data\", len(df)\n", "\n", "    \n", "    return df\n", "\n", "#dobnow = DNDataInput()\n", "#print dobnow.dtypes\n", "\n", "\n", "def CODataInput():\n", "    df = pd.read_csv('I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\sourceData\\\\C of O Issued (BIS)Auto.csv', low_memory=False)\n", "    df = df[[\"J_BIN_NUMBER\", \"J_JOB_NUMBER\", \"AP_A_ISSUE_TYPE\", \"C of O Issue Date\"]]\n", "    df[\"J_BIN_NUMBER\"] = df[\"J_BIN_NUMBER\"].astype(str)\n", "    df[\"J_BIN_NUMBER\"] = df[\"J_BIN_NUMBER\"].map(str.strip)\n", "    df = df.rename(columns={'J_BIN_NUMBER': 'BIN Number', 'AP_A_ISSUE_TYPE': 'CO_TCO', 'J_JOB_NUMBER':'Job Number'})\n", "    \n", "    return df\n", "    \n", "#CREATE DATAFRAMES\n", "print \"Reading BIS job filing data ...\"\n", "#text_file.write(str(datetime.now()) + \"...\" + \"Reading BIS job filing data\"  + \"\\n\")\n", "df = PermitDataInput()\n", "print \"Reading DOB Now job filing data ...\"\n", "#text_file.write(str(datetime.now()) + \"...\" + \"Reading DOB Now job filing data\"  + \"\\n\")\n", "dobnow = DNDataInput()\n", "print \"Reading CO/TCO data for dictionary ...\"\n", "#text_file.write(str(datetime.now()) + \"...\" + \"Reading CO/TCO data for dictionary\"  + \"\\n\")\n", "co = CODataInput()\n", "\n", "\n", "\n", "df[\"BIN Number\"] = df[\"BIN Number\"].astype(str)\n", "df[\"BIN Number\"] = df[\"BIN Number\"].map(str.strip)\n", "dobnow[\"BIN Number\"] = dobnow[\"BIN Number\"].astype(str)\n", "dobnow[\"BIN Number\"] = dobnow[\"BIN Number\"].map(str.strip)\n", "\n", "#INSERT THE CORRECT LAT LON COORDINATES FROM BIS INTO DOBNOW\n", "latDic = df.set_index('BIN Number')['Latitude'].to_dict()\n", "lonDic = df.set_index('BIN Number')['Latitude'].to_dict()\n", "\n", "for i in range(0, len(dobnow)):\n", "    if dobnow[\"BIN Number\"][i] in latDic:\n", "        dobnow[\"Latitude\"][i] = latDic[dobnow[\"BIN Number\"][i]]\n", "    if dobnow[\"BIN Number\"][i] in lonDic:\n", "        dobnow[\"Longitude\"][i] = lonDic[dobnow[\"BIN Number\"][i]]\n", "\n", "\n", "frames = [df, dobnow]\n", "result = pd.concat(frames)\n", "\n", "df = result.reset_index(drop=True)\n", "df['expire_DATE'] = pd.to_datetime(df['expire_DATE'])\n", "df = df.sort_values(by = 'BIN Number', ascending=True).reset_index(drop=True)\n", "\n", "df[\"Borough\"] = df[\"Borough\"].astype(str)\n", "df[\"Borough\"] = df[\"Borough\"].map(str.upper)\n", "\n", "\n", "print \"lengh of combined\", len(df)\n", "\n"], ["AGGREGATE AND COUNT DATA"], ["def GetStories(dtf):    \n", "    existStories = dtf[dtf[\"Existing Stories\"] != 0]\n", "    existStories = existStories[\"Existing Stories\"].tolist()\n", "    existStories = filter(None, existStories)\n", "    #existStories = sorted(existStories, reverse=True)\n", "    #print existStories\n", "    \n", "    proposedStories = dtf[dtf[\"Proposed Stories\"] != 0]\n", "    proposedStories = proposedStories[\"Proposed Stories\"].tolist()\n", "    proposedStories = filter(None, proposedStories)\n", "    #proposedStories = sorted(proposedStories, reverse=True)\n", "    #print proposedStories\n", "\n", "    #stories\n", "    if len(proposedStories) > 0:\n", "        #if len(existStories) != 0:\n", "        stories = proposedStories[0]\n", "    elif len(existStories) > 0:\n", "        stories = existStories[0]\n", "    else:\n", "        stories = 0\n", "    return stories\n", "\n", "\n", "\n", "print \"Aggregating to BIN level ...\"\n", "#text_file.write(str(datetime.now()) + \"...\" + \"Aggregating to BIN level\"  + \"\\n\")\n", "\n", "\n", "qn = pd.DataFrame(columns=('BIN', 'Address', 'Borough', 'Community Board', 'Number Permits' \n", "                           'Number Stories', 'NB', 'ShedScafFence', 'Job Type List'  'Latitude', 'Longitude'))\n", "\n", "binList = sorted(list(set(df[\"BIN Number\"].tolist())))\n", "\n", "print \"Number of BINs\", len(binList)\n", "\n", "\n", "AggList = []\n", "\n", "df[\"Job Type\"] = df[\"Job Type\"].astype(str)\n", "df[\"Job Type\"] = df[\"Job Type\"].map(str.strip)\n", "\n", "df[\"Job Number\"] = df[\"Job Number\"].astype(str)\n", "df[\"Job Number\"] = df[\"Job Number\"].map(str.strip)\n", "\n", "df[\"Permit Type\"] = df[\"Permit Type\"].astype(str)\n", "df[\"Permit Type\"] = df[\"Permit Type\"].map(str.strip)\n", "\n", "df[\"Permit Sub Type\"] = df[\"Permit Sub Type\"].astype(str)\n", "df[\"Permit Sub Type\"] = df[\"Permit Sub Type\"].map(str.strip)\n", "\n", "for i in range(0, len(binList)):\n", "#for i in range(0, 5):    \n", "    if i%(len(binList)/10) == 0:\n", "        print i\n", "    \n", "    df2 = df[df[\"BIN Number\"] == binList[i]]\n", "    df2 = df2.reset_index(drop=True)\n", "    \n", "    df2 = df2.sort_values(by = 'expire_DATE', ascending=False).reset_index(drop=True)\n", "    \n", "    #print df2\n", "    \n", "    #COUNT NUMBER OF PERMITS\n", "    df2[\"Permit Agg\"] = df2[\"Job Type\"] + \" \" + df2[\"Permit Type\"] + \" \" + df2[\"Permit Sub Type\"]\n", "   \n", "    PermitAggList = list(set(df2[\"Permit Agg\"].tolist() ) )\n", "    if 'nan' in PermitAggList:\n", "        PermitAggList.remove('nan')\n", "    \n", "    numPermits = len(PermitAggList)\n", "    if len(PermitAggList) == 0:\n", "        print PermitAggList, binList[i]\n", "    \n", "    #print df2\n", "    #print\n", "    #print PermitAggList\n", "\n", "\n", "    \n", "    \n", "    \n", "    #DETERMIN NUMBER OF FLOORS, LOOK AT NB FIRST, THEN DM, THEN AFTER THE MOST RECENT\n", "    storiesNB = df2.copy()\n", "    storiesNB = storiesNB[storiesNB[\"Job Type\"] == \"NB\"]\n", "    storiesDM = df2.copy()\n", "    storiesDM = storiesDM[storiesDM[\"Job Type\"] == \"DM\"]\n", "    storiesA1 = df2.copy()\n", "    storiesA1 = storiesA1[storiesA1[\"Job Type\"] == \"A1\"]\n", "    \n", "    if len(storiesNB) > 0:\n", "        stories = GetStories(storiesNB)\n", "    elif len(storiesDM) > 0:\n", "        stories = GetStories(storiesDM) \n", "    elif len(storiesA1) > 0:\n", "        stories = GetStories(storiesA1)\n", "    else:\n", "        a2 = df2.copy()\n", "        stories = GetStories(a2)\n", "           \n", "        \n", "    #NB\n", "    #job_types = df2[\"Job Type\"].tolist()\n", "    #if \"NB\" in job_types:\n", "    #    nb = \"YES\"\n", "    #else:\n", "    #    nb = \"NO\"\n", "        \n", "    #shed, scaffold or fence\n", "    permitSub = df2[\"Permit Sub Type\"].tolist()\n", "    if ('SH' in permitSub or 'SF' in permitSub or 'FN' in permitSub):\n", "        ssf = \"YES\"\n", "    else:\n", "        ssf = \"NO\"\n", "        \n", "        \n", "    #JOB TYPE AND JOB NUMBER, MAJOR JOB TYPES ONLY\n", "    jobTypeList = [\"NB\", \"DM\", \"A1\"]\n", "    df3 = df2.copy()\n", "    df3 = df3[df3['Job Type'].isin(jobTypeList)]\n", "    \n", "    #df3 = df2.copy()\n", "    jt_number = []\n", "    contList = []\n", "    if len(df3) > 0:\n", "        df3[\"Job Type Number\"] = df3[\"Job Type\"] + \" \" + df3[\"Job Number\"]\n", "        jt_number = df3[\"Job Type Number\"].tolist()\n", "        contList = df3[\"General Contractor\"].tolist()\n", "        \n", "        \n", "    #GET JOB TYPE, THE HIGHEST IN HIEARCHY,JOB NUMBER AND CONTRACTOR\n", "    nb = df2.copy()\n", "    nb = nb[nb[\"Job Type\"] == \"NB\"]\n", "    nb = nb.reset_index(drop=True)\n", "    dm = df2.copy()\n", "    dm = dm[dm[\"Job Type\"] == \"DM\"]\n", "    dm = dm.reset_index(drop=True)\n", "    a1 = df2.copy()\n", "    a1 = a1[a1[\"Job Type\"] == \"A1\"]\n", "    a1 = a1.reset_index(drop=True)\n", "    a2 = df2.copy()\n", "    a2 = a2[a2[\"Job Type\"] == \"A2\"]\n", "    a2 = a2.reset_index(drop=True)\n", "    a3 = df2.copy()\n", "    a3 = a3[a3[\"Job Type\"] == \"A3\"]\n", "    a3 = a3.reset_index(drop=True)\n", "\n", "    \n", "    if len(nb) > 0:\n", "        jobtype = nb[\"Job Type\"][0]\n", "        jobnumber = nb[\"Job Number\"][0]\n", "        contractor = nb[\"General Contractor\"][0]\n", "    elif len(dm) > 0:\n", "        jobtype = dm[\"Job Type\"][0]\n", "        jobnumber = dm[\"Job Number\"][0]\n", "        contractor = dm[\"General Contractor\"][0]\n", "    elif len(a1) > 0:\n", "        jobtype = a1[\"Job Type\"][0]\n", "        jobnumber = a1[\"Job Number\"][0]\n", "        contractor = a1[\"General Contractor\"][0]\n", "    elif len(a2) > 0:\n", "        jobtype = a2[\"Job Type\"][0]\n", "        jobnumber = a2[\"Job Number\"][0]\n", "        contractor = a2[\"General Contractor\"][0]\n", "    else:\n", "        jobtype = a3[\"Job Type\"][0]\n", "        jobnumber = a3[\"Job Number\"][0]\n", "        contractor = a3[\"General Contractor\"][0]\n", "          \n", "     \n", "    \n", "    #print \"***************\"\n", "    #print df3\n", "    \n", "    \n", "    \n", "    \n", "    \n", "    AggList.append([binList[i],df2[\"Address\"][0], df2[\"Borough\"][0], df2[\"Community Board\"][0], \n", "                    numPermits, stories, ssf,jt_number,jobtype, jobnumber, contractor, contList, df2[\"Latitude\"][0], df2[\"Longitude\"][0] ] )\n", "    #print\n", "    #print df2\n", "    \n", "    \n", "data = pd.DataFrame(AggList)\n", "\n", "data = data.rename(columns={0: 'BIN', 1: 'Address', 2:'Borough', 3:'Community Board', 4:'Number of Permits',\n", "                            5:'Number of Stories',  6:'ScaffoldShedFence', 7:'Job Type and Number List', 8:'Job Type',\n", "                            9:'Job Number', 10:'Contractor', 11:'Contractor List',\n", "                            12:'Lat', 13:'Lon'})\n", "\n"], ["IMPORT ACCIDENT DATA AND INSERT INTO DATA"], ["pd.set_option('chained_assignment', None)\n", "\n", "#text_file.write(str(datetime.now()) + \"...\" + \"Import accident data and insert into dataframe\"  + \"\\n\")\n", "\n", "\n", "def AccidentDataInput():\n", "\n", "    #import datetime\n", "    #import time\n", "    #from datetime import date\n", "    #from dateutil.relativedelta import relativedelta\n", "    #from dateutil import parser\n", "\n", "    #permits issued\n", "    df = pd.read_csv('I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\sourceData\\\\construction related accidents_3 YRS PERIODAuto.csv', low_memory=False)\n", "    \n", "    df = df[[\"BIN\"]]\n", "    \n", "    print \"length before drop nan\", len(df)\n", "    df = df.dropna()\n", "    print \"length after drop nan\", len(df)\n", "    \n", "    df[\"BIN\"] = df[\"BIN\"].astype(int)\n", "    df[\"BIN\"] = df[\"BIN\"].astype(str)\n", "    df[\"BIN\"] = df[\"BIN\"].map(str.strip)\n", "\n", "\n", "\n", "    \n", "    df[\"Value\"] = 1\n", "    \n", "    df = df.groupby(['BIN']).sum()\n", "    df = df.add_suffix('').reset_index()\n", "    \n", "    df = df.set_index('BIN')['Value'].to_dict()\n", "    \n", "    return df\n", "\n", "AccDic = AccidentDataInput()\n", "\n", "#INSERT ACCIDENTS\n", "data[\"Accidents\"] = 0\n", "data[\"BIN\"] = data[\"BIN\"].astype(str)\n", "data[\"BIN\"] = data[\"BIN\"].map(str.strip)\n", "\n", "for i in range(0, len(data)):\n", "    if data[\"BIN\"][i] in AccDic:\n", "        data[\"Accidents\"][i] = AccDic[data[\"BIN\"][i]]\n", "        \n", "#data = data[[\"BIN\", \"Number of Permits\", \"Number of Stories\", \"NB\", \"ScaffoldShedFence\", \"Accidents\", \"Lat\", \"Lon\"]]\n", "\n", "#data = data.reindex_axis(sorted(df.columns), axis=1)"], ["INSERT CO/TCO DATA"], ["#keep only active job numbers\n", "\n", "#text_file.write(str(datetime.now()) + \"...\" + \"Insert CO and TCO data\"  + \"\\n\")\n", "\n", "\n", "count = 0\n", "\n", "df[\"Job Number\"] = df[\"Job Number\"].astype(str)\n", "co[\"Job Number\"] = co[\"Job Number\"].astype(str)\n", "\n", "dfJobNum = list(set(df[\"Job Number\"].tolist() ) )\n", "\n", "print \"Remove those CO/TCO not active ...\"\n", "print \"Length of co\", len(co)\n", "\n", "co = co[co['Job Number'].isin(dfJobNum)]\n", "co = co.reset_index(drop=True)\n", "\n", "print \"Length after drop co\", len(co)\n", "\n", "\n", "print \"Creating CO, TCO dictionary ...\"\n", "#includes co and tco\n", "data[\"CO/TCO\"] = \"NO\"\n", "data[\"CO/TCO Recent Date\"] = \"\"\n", "\n", "coDic = co.set_index('BIN Number')['CO_TCO'].to_dict()\n", "\n", "\n", "#create dictionary\n", "\n", "co_binList = list( set(co[\"BIN Number\"].tolist() ) )\n", "\n", "co['C of O Issue Date'] = pd.to_datetime(co['C of O Issue Date'])\n", "\n", "dateDic = {}\n", "\n", "print \"Creating date dictionary ...\"\n", "\n", "coG = co[[\"BIN Number\", \"C of O Issue Date\"]]\n", "#get the most recent date on the BIN\n", "coG = coG.groupby(['BIN Number'], sort=False)['C of O Issue Date'].max()\n", "\n", "#convert series to dataframe\n", "coG_df = pd.DataFrame(coG)\n", "\n", "coG_df = coG_df.add_suffix('').reset_index()\n", "\n", "coG_df[\"C of O Issue Date\"] = coG_df[\"C of O Issue Date\"].astype(str)\n", "\n", "coG_df\n", "\n", "\n", "dateDic = coG_df.set_index('BIN Number')['C of O Issue Date'].to_dict()\n", "\n", "\n", "\n", "\n", "print \"Inserting certificat of occupancy and recent date ... \"\n", "for i in range(0, len(data)):\n", "    if data[\"BIN\"][i] in coDic:\n", "        data[\"CO/TCO\"][i] = \"YES\" \n", "    \n", "        data[\"CO/TCO Recent Date\"][i] = dateDic[data[\"BIN\"][i]]\n", "        \n", "        count+=1\n", "            \n", "print \"Number that match\", count\n", "        \n", "        \n", "\n"], ["CREATE THE SCORES"], ["#text_file.write(str(datetime.now()) + \"...\" + \"Score the data\"  + \"\\n\")\n", "print \"Creating the scores...\"\n", "\n", "data['Score'] = 0\n", "for i in range(0, len(data)):\n", "    score = []\n", "    #number of permits\n", "    if (data[\"Number of Permits\"][i] >= 1 and data[\"Number of Permits\"][i] <= 4):\n", "        score.append(1)\n", "    if (data[\"Number of Permits\"][i] >= 5 and data[\"Number of Permits\"][i] <= 7):\n", "        score.append(2)\n", "    if (data[\"Number of Permits\"][i] >= 8 and data[\"Number of Permits\"][i] <= 10):\n", "        score.append(3)\n", "    if (data[\"Number of Permits\"][i] >= 11 ):\n", "        score.append(4)\n", "    \n", "    if  \"NB\" in data[\"Job Type\"][i] > 0:\n", "        score.append(1)\n", "    else:\n", "        score.append(0)\n", "        \n", "    if data[\"ScaffoldShedFence\"][i] > 0:\n", "        score.append(1)\n", "    else:\n", "        score.append(0)\n", "        \n", "    if data[\"Number of Stories\"][i] >= 10:\n", "        score.append(1)\n", "    else:\n", "        score.append(0)\n", "        \n", "    if data[\"Accidents\"][i] > 0:\n", "        score.append(1)\n", "    else:\n", "        score.append(0)\n", "        \n", "        \n", "    data['Score'][i] = sum(score)\n", "        \n"], ["OUTPUT ALL DATA"], ["#data.to_csv(\"I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\test.csv\", index=False)\n", "data.dtypes"], ["OUTPUT SUBSETTED DATA"], ["output = data.copy()\n", "\n", "#output[\"Job Type and Number List\"] = output[\"Job Type and Number List\"].astype(str)\n", "#output[\"Job Type and Number List\"] = output[\"Job Type and Number List\"].str.replace(\"'\",\"\")\n", "#output[\"Job Type and Number List\"] = output[\"Job Type and Number List\"].str.replace(\"[\", '')\n", "#output[\"Job Type and Number List\"] = output[\"Job Type and Number List\"].str.replace(\"]\", '')\n", "\n", "#output[\"Contractor List\"] = output[\"Contractor List\"].astype(str)\n", "#output[\"Contractor List\"] = output[\"Contractor List\"].str.replace(\"'\",\"\")\n", "#output[\"Contractor List\"] = output[\"Contractor List\"].str.replace(\"[\", '')\n", "#output[\"Contractor List\"] = output[\"Contractor List\"].str.replace(\"]\", '')\n", "\n", "#drop columns\n", "output = output.drop('Job Type and Number List', 1)\n", "output = output.drop('Contractor List', 1)\n", "\n", "output[\"Contractor\"] = output[\"Contractor\"].str.replace(',', '')\n", "output[\"Address\"] = output[\"Address\"].str.replace(',', '')\n", "\n", "#output[\"Job Type and Number\"] = output[\"Job Type and Number\"].str.replace('[^\\w\\s]','')\n", "\n", "#output[\"Job Type and Number List\"] = output[\"Job Type and Number List\"].map(str.strip)\n", "\n", "#remove low risk\n", "print \"length before drop low risk\", len(output)\n", "output = output[output[\"Score\"] > 4]\n", "output = output.reset_index(drop=True)\n", "print \"len after drop\", len(output)\n", "\n", "#remove low risk\n", "print \"drop CO/TCO\"\n", "output = output[output[\"CO/TCO\"] != \"YES\"]\n", "output = output.reset_index(drop=True)\n", "print \"len after drop\", len(output)\n", "\n"], ["#EXPORT AS CSV AND JSON FILE\n", "#text_file.write(str(datetime.now()) + \"...\" + \"Output the data\"  + \"\\n\")\n", "output.to_csv(\"I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\sourceData\\\\SiteSafetyAutomationOutput.csv\", index=False)\n", "#text_file.write(str(datetime.now()) + \"...\" + \"DONE\"  + \"\\n\")\n", "#text_file.close()\n", "\n", "#EXPORT JSON\n", "import csv\n", "import json\n", "reader = csv.DictReader(open('I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\sourceData\\\\SiteSafetyAutomationOutput.csv', 'rb'))\n", "\n", "dict_list = []\n", "for line in reader:\n", "    dict_list.append(line)\n", "\n", "    \n", "with open('I:\\\\BAT\\\\SITE SAFETY AUTOMATION\\\\sourceData\\\\SiteSafetyAutomationOutput.json', 'w') as outfile:\n", "    json.dump(dict_list, outfile)\n", "\n", "print str(datetime.now())\n", "\n", "print \"DONE\""]]}], "DeveloTI/accidentes": [{"MUAVDM_TIA_Actividad 2_Clasificacio\u0301n_NotebookBase.ipynb": [["<img src=\"https://www.unir.net/wp-content/uploads/2019/11/Unir_2021_logo.svg\" width=\"240\" height=\"240\" align=\"right\"/>"], ["<center><h1>T\u00e9cnicas de Inteligencia Artificial</header1></center>\n", "<left><h1>Actividad 2. Trabajando con redes neuronales y Deep Learning</header1></left>"], ["Presentado por: Nombres Apellidos  <br>\n", "Fecha: DD/MM/2022"], ["# Importaci\u00f3n de librerias necesarias"], [], ["# Parte II. Clasificaci\u00f3n"], ["#Para esta actividad se importar\u00e1n las siguientes librer\u00edas:\n"], ["## Cargar el Dataset\n", "\n", "Con al menos 1000 instancias, una variable/atributo de la salida, y que dependa de, al menos, 6 variables/atributos de entrada. "], ["#C\u00f3digo para cargar el Dataset\n", "#url = 'https://raw.githubusercontent.com/oscar-unir/TIA/main/datasets/car/Laboratorio_dataset_car.csv'\n"], ["## Descripci\u00f3n de la fuente del Dataset"], ["Haga una descripci\u00f3n de la fuente de dayos utilizada (Incluya los enlaces necesarios)."], ["## Explique el problema a resolver. \n", "Descripci\u00f3n del problema. Tipo de problema (justifique). Variable objetivo, variables de entrada. Utilidad de su posible soluci\u00f3n. Elementos adicionales que considere relevantes (no son necesarios contenidos te\u00f3ricos, sino explicar qu\u00e9 relaciones tratas de comprobar y con qu\u00e9 m\u00e9todos)."], ["## Caracterizaci\u00f3n del Dataset\n", "\n", "Realice una descripci\u00f3n de los datos con:\n", "\n", ">- N\u00famero de instancias en total.\n", ">- N\u00famero de atributos de entrada, su significado y tipo.\n", ">- N\u00famero de clases de la variable objetivo, indicando que representan dichas clases y el tipo de valor que toman.\n", ">- N\u00famero de instancias pertenecientes a cada clase en la variable objetivo.\n", ">- Estad\u00edsticas de la variable objetivo.\n", ">- Estad\u00edsticas los atributos en relaci\u00f3n con la variable objetivo.\n", "\n", "Se incorpor\u00e1 una peque\u00f1a descripci\u00f3n (EDA) del conjunto de datos utilizado. Se analiza el dataset proporcionando, se muestra al menos algunas de sus caracter\u00edsticas mediante tablas y al menos algunas de ellas en modo gr\u00e1fico (p.ej., histogramas, diagramas de dispersi\u00f3n, diagramas de cajas y bigotes, etc.)\n"], ["#C\u00f3digo que responde a la descripci\u00f3n anterior"], ["#C\u00f3digo que responde a la descripci\u00f3n anterior (incorpore las lineas de code necesarias. Describa cadas sentencia de c\u00f3digo)"], ["En un par de p\u00e1rrafos haga un resumen de los principales hallazagos encontrados:    "], ["## Preprocesamiento del dataset. Transformaciones previas necesarias para la modelaci\u00f3n"], ["#C\u00f3digo que realice las transformaciones necesarias para poder realizar los procesos de modelaci\u00f3n. Ej.One hot enconding"], ["## Divisi\u00f3n del dataset en datos de entrenamiento y datos de test "], ["#C\u00f3digo que realice la divisi\u00f3n en entrenamiento y test, de acuerdo con la estretgia de evaluaci\u00f3n planeada. Describa cu\u00e1l es."], ["## Propuesta de arquitectura de red neuronal"], ["Describe: \n", "\n", "+ las neuronas en la capa de entrada \n", "+ las capas intermedias \u2013 al menos dos \u2013 \n", "+ capa de salida\n", "+ funciones de activaci\u00f3n\n", "\n", "Al menos utiliza relu en algunas de las capas intermedias y utiliza softmax en la capa de salida."], ["# C\u00f3digo de la estructuraci\u00f3n de la red"], ["#C\u00f3digo de la inspecci\u00f3n del modelo de red"], ["##  Ajuste de modelo de Clasificaci\u00f3n RNA"], ["Mediante Python y utilizando al menos Keras sobre TensorFlow 2.0 (tensorflow.keras), entrena el modelo o modelos de red neuronal escogidos."], ["#C\u00f3digo de ajuste y entrenamiento"], ["## Evaluaci\u00f3n de modelo RNA\n", "\n", "Defina las estad\u00edsticas (m\u00e9tricas) de evaluaci\u00f3n, y dividiendo el dataset en datos de entrenamiento, validaci\u00f3n y datos de test prueba tu propuesta.\n"], ["### Visualice el progreso de entrenamiento del modelo y muestre las estad\u00edsticas de evaluaci\u00f3n para los conjuntos de entrenamiento y validaci\u00f3n.  "], ["#C\u00f3digo de evaluaci\u00f3n de la red propuesta (entrenamiento y validaci\u00f3n)"], ["### Eval\u00fae los resultados  para el conjunto de test."], ["#C\u00f3digo de evaluaci\u00f3n de la red propuesta (evaluaci\u00f3n conjunto de test)"], ["## Ajuste de modelos de clasificaci\u00f3n alternativos"], ["Elige al menos un m\u00e9todo de clasificaci\u00f3n no basado en redes neuronales (p.ej. regresi\u00f3n log\u00edstica, \u00e1rboles de decisi\u00f3n, reglas de clasificaci\u00f3n, random forest, SVM, etc)."], ["#C\u00f3digo de ajuste del modelo 1"], ["#C\u00f3digo de ajuste del modelo 2"], ["#C\u00f3digo para mostrar la evaluaci\u00f3n de los modelos"], ["Construya un o dos p\u00e1rrafos con los principales hallazgos. Incluye una explicaci\u00f3n de los par\u00e1metros que consideres relevantes en cada ejecuci\u00f3n."], ["## Comparaci\u00f3n del desempe\u00f1o de modelos\n", "\n", "Muestra los resultados obtenidos por los diferentes algoritmos escogidos de forma gr\u00e1fica y comparada/superpuesta."], ["#C\u00f3digo para mostrar la comparaci\u00f3n de m\u00e9tricas de desempe\u00f1o de las dos propuestas en tabla"], ["#C\u00f3digo para mostrar la comparaci\u00f3n de m\u00e9tricas de desempe\u00f1o de las dos propuestas en gr\u00e1fica"], ["Construya un p\u00e1rrafo con los principales hallazgos."], ["## Discusi\u00f3n de los resultados obtenidos y argumentos sobre c\u00f3mo se podr\u00edan mejorar de dichos resultados"], ["Realice en este espacio todo el an\u00e1lsis de resultados final incluyendo: \n", "+ Resultados comparados. Conclusiones objetivas y significantes con base a las diferentes m\u00e9tricas escogidas. \n", "+ Argumentos que describan con qu\u00e9 t\u00e9cnica se obtienen mejores resultados en base a las diferentes m\u00e9tricas que hayas escogido\n", "+ Explicaci\u00f3n de c\u00f3mo se podr\u00edan mejorar los resultados obtenidos por las redes neuronales, independientemente de que mejoren o no a los algoritmos no basados en redes neuronales."]]}, {"MUAVDM_TIA_Actividad 2_Regresio\u0301n_NotebookBase.ipynb": [["<img src=\"https://www.unir.net/wp-content/uploads/2019/11/Unir_2021_logo.svg\" width=\"240\" height=\"240\" align=\"right\"/>"], ["<center><h1>T\u00e9cnicas de Inteligencia Artificial</header1></center>\n", "<left><h1>Actividad 2. Trabajando con redes neuronales y Deep Learning</header1></left>"], ["Presentado por: Nombres Apellidos  <br>\n", "Fecha: DD/MM/2022"], ["# Importaci\u00f3n de librerias necesarias"], ["# Parte I. Regresi\u00f3n"], ["#Para esta actividad se importar\u00e1n las siguientes librer\u00edas:\n"], ["## Cargar el Dataset\n", "\n", "Con al menos 1000 instancias, una variable/atributo de la salida, y que dependa de, al menos, 6 variables/atributos de entrada. "], ["#C\u00f3digo para cargar el Dataset\n", "#url = 'https://raw.githubusercontent.com/oscar-unir/TIA/main/datasets/car/Laboratorio_dataset_car.csv'\n"], ["## Descripci\u00f3n de la fuente del Dataset"], ["Haga una descripci\u00f3n de la fuente de dayos utilizada (Incluya los enlaces necesarios)."], ["## Explique el problema a resolver. \n", "Descripci\u00f3n del problema. Tipo de problema (justifique). Variable objetivo, variables de entrada. Utilidad de su posible soluci\u00f3n. Elementos adicionales que considere relevantes (no son necesarios contenidos te\u00f3ricos, sino explicar qu\u00e9 relaciones tratas de comprobar y con qu\u00e9 m\u00e9todos)."], ["## Caracterizaci\u00f3n del Dataset\n", "\n", "Realice una descripci\u00f3n de los datos con:\n", "\n", ">- N\u00famero de instancias en total.\n", ">- N\u00famero de atributos de entrada, su significado y tipo.\n", ">- Estad\u00edsticas de la variable objetivo.\n", ">- Estad\u00edsticas los atributos en relaci\u00f3n con la variable objetivo.\n", "\n", "Se incorpor\u00e1 una peque\u00f1a descripci\u00f3n (EDA) del conjunto de datos utilizado. Se analiza el dataset proporcionando, se muestra al menos algunas de sus caracter\u00edsticas mediante tablas y al menos algunas de ellas en modo gr\u00e1fico (p.ej., histogramas, diagramas de dispersi\u00f3n, diagramas de cajas y bigotes, etc.)\n"], ["#C\u00f3digo que responde a la descripci\u00f3n anterior"], ["#C\u00f3digo que responde a la descripci\u00f3n anterior (incorpore las lineas de code necesarias. Describa cadas sentencia de c\u00f3digo)"], ["En un par de p\u00e1rrafos haga un resumen de los principales hallazagos encontrados:    "], ["## Preprocesamiento del dataset. Transformaciones previas necesarias para la modelaci\u00f3n"], ["#C\u00f3digo que realice las transformaciones necesarias para poder realizar los procesos de modelaci\u00f3n. Ej.One hot enconding"], ["## Divisi\u00f3n del dataset en datos de entrenamiento y datos de test "], ["#C\u00f3digo que realice la divisi\u00f3n en entrenamiento y test, de acuerdo con la estretgia de evaluaci\u00f3n planeada. Describa cu\u00e1l es."], ["## Propuesta de arquitectura de red neuronal"], ["Describe: \n", "\n", "+ las neuronas en la capa de entrada \n", "+ las capas intermedias \u2013 al menos dos \u2013 \n", "+ capa de salida\n", "+ funciones de activaci\u00f3n"], ["# C\u00f3digo de la estructuraci\u00f3n de la red"], ["#C\u00f3digo de la inspecci\u00f3n del modelo de red"], ["##  Ajuste de modelo de Regresi\u00f3n RNA"], ["Mediante Python y utilizando al menos Keras sobre TensorFlow 2.0 (tensorflow.keras), entrena el modelo o modelos de red neuronal escogidos."], ["#C\u00f3digo de ajuste y entrenamiento"], ["## Evaluaci\u00f3n de modelo RNA\n", "\n", "Defina las estad\u00edsticas (m\u00e9tricas) de evaluaci\u00f3n, y dividiendo el dataset en datos de entrenamiento, validaci\u00f3n y datos de test prueba tu propuesta.\n"], ["### Visualice el progreso de entrenamiento del modelo y muestre las estad\u00edsticas de evaluaci\u00f3n para los conjuntos de entrenamiento y validaci\u00f3n.  "], ["#C\u00f3digo de evaluaci\u00f3n de la red propuesta (entrenamiento y validaci\u00f3n)"], ["### Eval\u00fae los resultados  para el conjunto de test."], ["#C\u00f3digo de evaluaci\u00f3n de la red propuesta (evaluaci\u00f3n conjunto de test)"], ["## Ajuste de modelos de Regresi\u00f3n alternativos"], ["Elige al menos un m\u00e9todo de regresi\u00f3n no basado en redes neuronales (p.ej. regresi\u00f3n lineal, regresi\u00f3n polin\u00f3mica, regresi\u00f3n logar\u00edtmica, SVR, random forest regression, etc.)."], ["#C\u00f3digo de ajuste del modelo 1"], ["#C\u00f3digo de ajuste del modelo 2"], ["#C\u00f3digo para mostrar la evaluaci\u00f3n de los modelos"], ["Construya un o dos p\u00e1rrafos con los principales hallazgos. Incluye una explicaci\u00f3n de los par\u00e1metros que consideres relevantes en cada ejecuci\u00f3n."], ["## Comparaci\u00f3n del desempe\u00f1o de modelos\n", "\n", "Muestra los resultados obtenidos por los diferentes algoritmos escogidos de forma gr\u00e1fica y comparada/superpuesta."], ["#C\u00f3digo para mostrar la comparaci\u00f3n de m\u00e9tricas de desempe\u00f1o de las dos propuestas en tabla"], ["#C\u00f3digo para mostrar la comparaci\u00f3n de m\u00e9tricas de desempe\u00f1o de las dos propuestas en gr\u00e1fica"], ["Construya un p\u00e1rrafo con los principales hallazgos."], ["## Discusi\u00f3n de los resultados obtenidos y argumentos sobre c\u00f3mo se podr\u00edan mejorar de dichos resultados"], ["Realice en este espacio todo el an\u00e1lsis de resultados final incluyendo: \n", "+ Resultados comparados. Conclusiones objetivas y significantes con base a las diferentes m\u00e9tricas escogidas. \n", "+ Argumentos que describan con qu\u00e9 t\u00e9cnica se obtienen mejores resultados en base a las diferentes m\u00e9tricas que hayas escogido\n", "+ Explicaci\u00f3n de c\u00f3mo se podr\u00edan mejorar los resultados obtenidos por las redes neuronales, independientemente de que mejoren o no a los algoritmos no basados en redes neuronales."]]}], "Iliasx18/Trafic-accidents": [{"notebook.ipynb": [["# Trafic accidents (Ilias SAGHIR)\n", "\n", "**Goal** : predict whether a car accident is serious or not based on attributes such as : time and space circumstances, road and vehicle characteristics, usage of safety gear, driver and passenger info...\n", "\n", "**Data** : For this, we will use the 2018 public data provided by the French ministry of transport which can be found on [this page](https://www.data.gouv.fr/fr/datasets/base-de-donnees-accidents-corporels-de-la-circulation/#_)."], ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt"], ["charac=pd.read_csv('data/caracteristiques-2018.csv',encoding='ISO-8859-1')\n", "user=pd.read_csv('data/usagers-2018.csv',encoding='ISO-8859-1')\n", "loc=pd.read_csv('data/lieux-2018.csv',encoding='ISO-8859-1')"], ["**charac** : accident circumstances : time, place, weather, lighting...\n", "\n", "**user** : info on the people involved\n", "\n", "**loc** : info on the place of accident : road type, circulation..."], ["#Merging tables by accident/user/location correspondences\n", "\n", "df=pd.merge(user, pd.merge(charac, loc, on='Num_Acc'), on='Num_Acc').drop(columns=['Num_Acc'])\n", "\n", "#Dataset provides 4 categories for accident gravity, we will simplify by only considering 2 categories (serious vs non serious)\n", "\n", "df['grav']=((df['grav']==2) | (df['grav']==3))\n", "print(df['grav'].sum(),len(df))"], ["There is a noticeable class imbalance which we should account for in training and in the evaluation of our model."], ["We choose to eliminate some attributes and only keep possibly relevent ones."], ["df=df[['grav','lum','agg','int','atm','catr','circ','nbv','prof','plan','surf','infra','catu','sexe','an_nais','secu']]"], ["This next preprocessing step encodes the categorical variables into dummy variables (0-1), this helps the model further discriminate important values for the variables, and enables us to use a logistic regression."], ["df=df.dropna()\n", "\n", "for c in df.columns:\n", "    if c in ['lum','int','atm','circ','prof','plan','infra','catu','surf']:\n", "        m=int(max(df[c]))\n", "        for i in range(1,m+1):\n", "            df[c+str(i)]=(df[c]==i)\n", "        df=df.drop(columns=c)\n", "    elif c in ['agg','sexe']:\n", "        df[c]=(df[c]==2)\n", "    elif c=='catr':\n", "        for i in range(1,7):\n", "            df[c+str(i)]=(df[c]==i)\n", "        df=df.drop(columns=c)\n", "    elif c=='an_nais':\n", "        df['age']=(2018-df[c])/(2018-min(df[c]))\n", "        df=df.drop(columns=c) \n", "    elif c=='secu':\n", "        df[c]=df[c].astype(int)\n", "        df['secu1']=df[c].apply(lambda x:int(str(x)[-1])==1)\n", "        df['secu2']=df[c].apply(lambda x:int(str(x)[-1])==2)\n", "        df=df.drop(columns=c)"], ["df"], ["We now have 66 attributes consisting of dummy variables and continuous variables"], ["features=df.columns.drop('grav')"], ["We construct our train/test sets using the train_test_split function, but we make sure that our training set contains the same proportion of both classes to avoid a bias in our model, hence the undersampling"], ["from sklearn.model_selection import train_test_split\n", "\n", "train, test = train_test_split(df, test_size=0.3,random_state=42)\n", "\n", "#undersampling\n", "X_train_1=train.loc[train['grav']==1,features]\n", "y_train_1=train.loc[train['grav']==1,'grav']\n", "X_train_2=train.loc[train['grav']==0,features].head(n=len(X_train_1))\n", "y_train_2=train.loc[train['grav']==0,'grav'].head(n=len(X_train_1))\n", "\n", "X_train=pd.concat([X_train_1,X_train_2],axis=0)\n", "y_train=pd.concat([y_train_1,y_train_2],axis=0)\n", "\n", "X_test=test[features]\n", "y_test=test['grav']"], ["We use either a logistic regression or a GradBoost model, we notice that the latter performs better"], ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "\n", "model=GradientBoostingClassifier()\n", "#model=LogisticRegression(max_iter=300)\n", "model.fit(X_train,y_train)"], ["For evaluation, we first measure the main scores of the model : accuracy, precision, recall and F-score"], ["y_pred=model.predict(X_train)\n", "print(\"Train accuracy : \",np.mean(y_pred==y_train))\n", "\n", "y_pred=model.predict(X_test)\n", "print(\"Test accuracy : \",np.mean(y_pred==y_test))\n", "\n", "from sklearn.metrics import precision_recall_fscore_support\n", "\n", "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))"], ["Since the classes are not balanced, another evaluation method is to plot the ROC and precision-recall curves, and calculate the AUC scores for each."], ["# roc curve and auc\n", "\n", "from sklearn.metrics import roc_curve\n", "from sklearn.metrics import roc_auc_score\n", "\n", "ns_probs = [0 for _ in range(len(y_test))]\n", "lr_probs = model.predict_proba(X_test)\n", "# keep probabilities for the positive outcome only\n", "lr_probs = lr_probs[:, 1]\n", "# calculate scores\n", "ns_auc = roc_auc_score(y_test, ns_probs)\n", "lr_auc = roc_auc_score(y_test, lr_probs)\n", "# summarize scores\n", "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n", "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n", "# calculate roc curves\n", "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n", "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n", "# plot the roc curve for the model\n", "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n", "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n", "# axis labels\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "# show the legend\n", "plt.legend()\n", "# show the plot\n", "plt.show()"], ["#precision-recall curve and AUC\n", "from sklearn.metrics import precision_recall_fscore_support\n", "from sklearn.metrics import precision_recall_curve\n", "from sklearn.metrics import f1_score\n", "from sklearn.metrics import auc\n", "\n", "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n", "lr_f1, lr_auc = f1_score(y_test, y_pred), auc(lr_recall, lr_precision)\n", "# summarize scores\n", "print('Model: auc=%.3f' % (lr_auc))\n", "# plot the precision-recall curves\n", "no_skill = len(y_test[y_test==1]) / len(y_test)\n", "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n", "plt.plot(lr_recall, lr_precision, marker='.', label='Model')\n", "# axis labels\n", "plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "# show the legend\n", "plt.legend()\n", "# show the plot\n", "plt.show()"], ["In both figures, we compare the model to a random guess, and it turns out to perform significantly better.\n", "\n", "We can also look at the most important features :"], ["feats=pd.Series(model.feature_importances_,features).sort_values(ascending=False).head(20)\n", "plt.bar(feats.index,feats.values)\n", "plt.xticks(rotation=45)\n", "plt.show()"], ["df[['grav']+list(feats.index)].corr().head(1)"], ["### Concluding remarks\n", "\n", "We notice that the type of road (**catr** variable) plays a great part in the gravity of an accident. \n", "The use of a safety gear (**secu**) seems to reduce the risk.\n", "Accidents in agglomerations tend to be less serious.\n", "Obviously, this work is only a demo of what can be achieved with the given dataset and can still be improved. Other variables that we haven't accounted for can be introduced, and some that weren't necessary can be removed for additional model stability, for example via PCA. Also, model hyperparameters can be chosen more optimaly using a GridSearch."]]}], "akshay3236/Aviation-Accident": [{"Aviation_Accident_Analysis.ipynb": [["### Import the standard libraries"], ["import pandas as pd #data processing and I/O operations\n", "import numpy as np #linear algebra\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "from datetime import date, datetime, timedelta #pip install datatime"], ["### Import my datatset"], ["data = pd.read_csv('Dataset1.csv')"], ["data.shape"], ["data.head(10)"], ["data.tail(5)"], ["### Check for missing values"], ["data.isnull().sum()"], ["data['Time'] = data['Time'].replace(np.nan, '00:00')"], ["data.isnull().sum()"], ["data['Time'].value_counts()"], ["data['Time'] = data['Time'].str.replace('c: ','')\n", "data['Time'] = data['Time'].str.replace('c:','')\n", "data['Time'] = data['Time'].str.replace('c','')\n", "data['Time'] = data['Time'].str.replace('12\\'20','12:20')\n", "data['Time'] = data['Time'].str.replace('18.40','18:40')\n", "data['Time'] = data['Time'].str.replace('0943','09:43')\n", "data['Time'] = data['Time'].str.replace('22\\'08','22:08')\n", "data['Time'] = data['Time'].str.replace('114:20','00:00')"], ["data['Time'] = data['Date'] + ' ' +data['Time']\n", "\n", "def todate(x):\n", "    return datetime.strptime(x, '%m/%d/%Y %H:%M')\n", "\n", "data['Time'] = data['Time'].apply(todate)"], ["print('Data ranges from ' + str(data.Time.min()) + ' to ' + str(data.Time.max()))"], ["data.Operator = data.Operator.str.upper()"], ["data.head()"], ["### Exploratory Data Analysis"], ["### Total Accidents by year"], ["Temp = data.groupby(data.Time.dt.year)[['Date']].count()\n", "Temp.head()"], ["Temp = Temp.rename(columns={'Date':'Count'})"], ["Temp.head(1)"], ["plt.figure(figsize=(12,6))\n", "plt.style.use('bmh')\n", "plt.plot(Temp.index, 'Count', data=Temp, color='blue', marker='.', linewidth=1)\n", "plt.xlabel('Year', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by year', fontsize=15)\n", "plt.show()"], ["import matplotlib.pylab as pl\n", "import matplotlib.gridspec as gridspec\n", "\n", "\n", "gs = gridspec.GridSpec(2,2)\n", "pl.figure(figsize=(15,10))\n", "plt.style.use('seaborn-muted')\n", "ax = pl.subplot(gs[0,:])\n", "sns.barplot(data.groupby(data.Time.dt.month)[['Date']].count().index, 'Date',\n", "            data = data.groupby(data.Time.dt.month)[['Date']].count(), color='lightskyblue', linewidth=2)\n", "plt.xticks(data.groupby(data.Time.dt.month)[['Date']].count().index, \n", "           ['Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n", "plt.xlabel('Month', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by month', fontsize=14)\n", "\n", "ax = pl.subplot(gs[1,0])\n", "sns.barplot(data.groupby(data.Time.dt.weekday)[['Date']].count().index, 'Date',\n", "            data = data.groupby(data.Time.dt.weekday)[['Date']].count(), color='lightskyblue', linewidth=2)\n", "plt.xticks(data.groupby(data.Time.dt.weekday)[['Date']].count().index, \n", "           ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n", "plt.xlabel('Weekday', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by Weekday', fontsize=14)\n", "\n", "ax = pl.subplot(gs[1,1])\n", "sns.barplot(data[data.Time.dt.hour != 0].groupby(data.Time.dt.hour )[['Date']].count().index, 'Date',\n", "            data = data[data.Time.dt.hour != 0].groupby(data.Time.dt.hour)[['Date']].count(), color='lightskyblue', linewidth=2)\n", "plt.xlabel('Hour', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by Hour', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()"], ["### Passenger vs military flights"], ["Temp = data.copy()\n", "Temp['isMilitary'] = Temp.Operator.str.contains('MILITARY')\n", "Temp = Temp.groupby('isMilitary')[['isMilitary']].count()\n", "Temp.index = ['Passenger', 'Military']\n", "Temp"], ["Temp2 = data.copy()\n", "Temp2['Military'] = Temp2.Operator.str.contains('MILITARY')\n", "Temp2['Passenger'] = Temp2.Military == False\n", "Temp2 = Temp2.loc[:,['Time', 'Military', 'Passenger']]\n", "Temp2"], ["Temp2 = Temp2.groupby(Temp2.Time.dt.year)[['Military', 'Passenger']].aggregate(np.count_nonzero)"], ["Temp2"], ["colors = ['yellowgreen', 'lightskyblue']\n", "plt.figure(figsize=(15,6))\n", "plt.subplot(1,2,1)\n", "patches, texts = plt.pie(Temp.isMilitary, colors=colors, labels=Temp.isMilitary, startangle=90)\n", "plt.legend(patches, Temp.index, fontsize=12)\n", "plt.axis('equal')\n", "plt.title('Total number of accidents by flight type', fontsize=15)\n", "\n", "plt.subplot(1,2,2)\n", "plt.plot(Temp2.index, 'Military', data=Temp2, color='lightskyblue', marker='.', linewidth=1)\n", "plt.plot(Temp2.index, 'Passenger', data=Temp2, color='yellowgreen', marker='.', linewidth=1)\n", "plt.legend(fontsize=12)\n", "plt.xlabel('Year', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by flight type', fontsize=15)\n", "plt.tight_layout()\n", "plt.show()"], ["### Total Number of fatalities"], ["Fatalities = data.groupby(data.Time.dt.year).sum()\n", "Fatalities['Proportion'] = Fatalities['Fatalities'] / Fatalities['Aboard']\n", "\n", "plt.figure(figsize=(15,6))\n", "plt.subplot(1,2,1)\n", "plt.fill_between(Fatalities.index, 'Aboard', data=Fatalities, color='skyblue', alpha=0.2)\n", "plt.plot(Fatalities.index, 'Aboard', data=Fatalities, marker='.', color='Slateblue', alpha=0.6, linewidth=1)\n", "\n", "plt.fill_between(Fatalities.index, 'Fatalities', data=Fatalities, color='olive', alpha=0.2)\n", "plt.plot(Fatalities.index, 'Fatalities', data=Fatalities, marker='.', color='olive', alpha=0.6, linewidth=1)\n", "\n", "plt.legend(fontsize=12)\n", "plt.xlabel('Year', fontsize=12)\n", "plt.ylabel('Number of People', fontsize=12)\n", "plt.title('Total number of Fatalities by Year')\n", "\n", "\n", "\n", "plt.subplot(1,2,2)\n", "plt.plot(Fatalities.index, 'Proportion', data=Fatalities, marker='.', color='red', linewidth=2)\n", "plt.xlabel('Year', fontsize=12)\n", "plt.ylabel('Fatalities ratio', fontsize=12)\n", "plt.title('Fatalities ratio by year', fontsize=15)\n", "plt.show()"], ["### Dataset 2 import"], ["Totals = pd.read_csv('Dataset2.csv')\n", "Totals.head()"], ["Totals = Totals.drop(['Country Name', 'Country Code', 'Indicator Code', 'Indicator Name'], axis=1)"], ["Totals = Totals.replace(np.nan, 0)"], ["Totals = pd.DataFrame(Totals.sum())"], ["Totals.tail()"], ["Totals = Totals.drop(Totals.index[0:10])\n", "Totals = Totals['1970':'2008']\n", "Totals.columns = ['Sum']\n", "Totals.index.name = 'Year'"], ["Totals.head()"], ["Fatalities = Fatalities.reset_index()"], ["Fatalities.head()"], ["Fatalities.Time = Fatalities.Time.apply(str)\n", "Fatalities.index = Fatalities['Time']\n", "del Fatalities['Time']\n", "Fatalities = Fatalities['1970':'2008']\n", "Fatalities = Fatalities[['Fatalities']]\n", "Totals = pd.concat([Totals,Fatalities], axis=1)\n", "Totals['Ratio'] = Totals['Fatalities'] / Totals['Sum'] * 100"], ["Totals.head()"], ["gs = gridspec.GridSpec(2,2)\n", "pl.figure(figsize=(15,10))\n", "\n", "ax= pl.subplot(gs[0,0])\n", "plt.plot(Totals.index, 'Sum', data=Totals, marker='.', color='green', linewidth=1)\n", "plt.xlabel('Year')\n", "plt.ylabel('Number of passengers')\n", "plt.title('Total number of passengers by Year', fontsize=15)\n", "plt.xticks(rotation=90)\n", "\n", "x= pl.subplot(gs[0,1])\n", "plt.plot(Fatalities.index, 'Fatalities', data=Totals, marker='.', color='red', linewidth=1)\n", "plt.xlabel('Year')\n", "plt.ylabel('Number of Deaths')\n", "plt.title('Total number of Deaths by Year', fontsize=15)\n", "plt.xticks(rotation=90)\n", "\n", "x= pl.subplot(gs[1,:])\n", "plt.plot(Totals.index, 'Ratio', data=Totals, marker='.', color='orange', linewidth=1)\n", "plt.xlabel('Year')\n", "plt.ylabel('Ratio')\n", "plt.title('Fatalities/Total number of passengers ratio by Year', fontsize=15)\n", "plt.xticks(rotation=90)\n", "plt.tight_layout()\n", "plt.show()"], ["### Plot ratio and number of deaths in one plot"], ["fig = plt.figure(figsize=(12,6))\n", "ax1 = fig.subplots()\n", "ax1.plot(Totals.index, 'Ratio', data=Totals, color='orange', marker='.', linewidth=1)\n", "ax1.set_xlabel('Year', fontsize=12)\n", "for label in ax1.xaxis.get_ticklabels():\n", "    label.set_rotation(45)\n", "ax1.set_ylabel('Ratio', color='orange', fontsize=12)\n", "ax1.tick_params('y', colors='orange')\n", "ax2 = ax1.twinx()\n", "ax2.plot(Fatalities.index, 'Fatalities', data=Fatalities, color='green', marker='.', linewidth=1)\n", "ax2.set_ylabel('Number of Fatalities', color='green', fontsize=12)\n", "ax2.tick_params('y', colors='g')\n", "plt.title('Fatalities VS Ratio by year', fontsize=15)\n", "plt.tight_layout()\n", "plt.show()"], ["### Operator Analysis"], ["data.Operator = data.Operator.str.upper()\n", "data.Operator = data.Operator.replace(\"A B AEROTRANSPORT\", 'AB AEROTRANSPORT')\n", "\n", "Total_by_Op = data.groupby('Operator')[['Operator']].count()\n", "Total_by_Op = Total_by_Op.rename(columns={'Operator':'Count'})\n", "Total_by_Op = Total_by_Op.sort_values(by='Count', ascending=False).head(15)"], ["Total_by_Op"], ["plt.figure(figsize=(12,6))\n", "sns.barplot(y=Total_by_Op.index, x='Count', data=Total_by_Op, palette='gist_heat', orient='h')\n", "plt.xlabel('Count', fontsize=12)\n", "plt.ylabel('Operator', fontsize=12)\n", "plt.title(\"Total Count of the Operator\", fontsize=15)\n", "plt.show()"], ["Prop_by_Op = data.groupby('Operator')[['Fatalities']].sum()\n", "Prop_by_Op = Prop_by_Op.rename(columns={'Operator':'Fatalities'})\n", "Prop_by_Op = Prop_by_Op.sort_values(by='Fatalities', ascending=False)\n", "Prop_by_OpTop = Prop_by_Op.head(15)"], ["plt.figure(figsize=(12,6))\n", "sns.barplot(y=Prop_by_OpTop.index, x='Fatalities', data=Prop_by_OpTop, palette='gist_heat', orient='h')\n", "plt.xlabel('Fatalities', fontsize=12)\n", "plt.ylabel('Operator', fontsize=12)\n", "plt.title(\"Total Fatalities of the Operator\", fontsize=15)\n", "plt.show()"], ["Prop_by_Op[Prop_by_Op['Fatalities'] == Prop_by_Op.Fatalities.min()].index.tolist()"], ["Aeroflot = data[data.Operator == 'AEROFLOT']\n", "Count_by_year = Aeroflot.groupby(data.Time.dt.year)[['Date']].count()\n", "Count_by_year = Count_by_year.rename(columns={'Date':'Count'})\n", "\n", "plt.figure(figsize=(12,6))\n", "plt.plot(Count_by_year.index, 'Count', data=Count_by_year, marker='.', color='red', linewidth=1)\n", "plt.xlabel('Year', fontsize=11)\n", "plt.ylabel('Count', fontsize=11)\n", "plt.title('Count of accidents by year (Aeroflot)', fontsize=16)\n", "plt.show()"], []]}], "LopezChris/Accident-Analysis": [{"Accident-Data-Analysis.ipynb": [["# Accident response analysis\n", "\n", "## How likely is it that a  \ud83d\ude93 officer will attend the scene of an accident?\n", "\n", "Christian Lopez \ud83d\ude03\n", "\n", "\n", "### How to use\n", "\n", "Run these commands on your terminal of choice:\n", "\n", "~~~bash\n", "mkdir $HOME/github\n", "cd\n", "cd github\n", "git clone https://github.com/LopezChris/challenge.git\n", "jupyter notebook\n", "~~~\n", "\n", "finally select this notebook and run it"], ["# Create pandas dataframes\n", "import pandas as pd \n", "# Import data from local file system\n", "import os\n", "# Check if file exists\n", "from os import path\n", "# Plot graph\n", "from matplotlib import pyplot as plt\n", "# Heatmap\n", "import seaborn as sns\n", "# Display multiple lines\n", "from IPython.display import display\n", "# Dont show warning or deprication messages\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "# Split data into train and test datasets\n", "from sklearn.model_selection import train_test_split\n", "# Random forest classifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import mean_squared_error\n", "# Review results with a confusion matrix\n", "from sklearn.metrics import confusion_matrix\n", "# Show reports of trained model\n", "from sklearn.metrics import classification_report\n", "from sklearn.model_selection import KFold, cross_val_score\n", "# Upsampling to address the big difference in the data\n", "from imblearn.over_sampling import SMOTE\n", "from pathlib import Path\n", "home = str(Path.home())\n", "import math\n", "import pickle\n", "\n", "# Data Path\n", "data_path=\"/github/h2o/input/DfTRoadSafety_Accidents_2014.csv\"\n", "# To save the model\n", "filename='accident_police_present.sav'"], ["### Data Exploration\n", "\n", "First I will inspect the data in order to see if there are any null columns or values, I will also ensure that the data is in its proper format"], ["# Read data from local file storage \n", "accidents = pd.read_csv(home + data_path)\n", "# Sample data\n", "display(accidents.head(10))\n", "# Ensure that the data is in the correct format\n", "display(accidents.info())\n", "\n", "# Ensure that there are no null values\n", "display(accidents['Time'].describe().T)\n", "display(\"Null Values in Time \" + str(accidents['Time'].isnull().sum()))"], ["**Note that the date feature in the data set is an object when it should be a datetime variable, let's fix that**"], ["# After inspecting the data info note that the date is an object when it should be a datetime variable\n", "accidents['Date']= pd.to_datetime(accidents['Date'], format=\"%d/%m/%Y\")\n", "\n", "# Keep an eye out on the dtype\n", "display(accidents.iloc[:,9:10].info())\n", "display(accidents['Time'].head())"], ["**Cool, now it is in its proper value. I think that the hour in which an accident is far more relevant than the minute, so I will isolate the hours from the minutes in a new column. Further, the time of day in which the accident occured is more relevant than a single hour, we'll address that in the next cells**"], ["# Create new column named Hours\n", "# print(accidents['Time'].str[0:2])\n", "accidents['Hours'] = accidents['Time'].str[0:2]\n", "# Convert from str to float64\n", "Hours = pd.to_numeric(accidents['Hours'])\n", "# Convert to int\n", "Hours = accidents['Hours'].astype('int')\n", "accidents['Hours'] = Hours\n", "\n", "# Verify conversion\n", "print(\"Hours are now type: \"+ str(type(accidents['Hours'][0])))\n", "display(accidents['Hours'].head(10))\n", "\n", "# Are there any null values?\n", "print('\\n')\n", "display(\"NA values in accidents[Hours]= \"+ str(accidents['Hours'].isna().sum()))"], ["# Gain a little more insight on the new column\n", "print(\"\\nDescribing 'Hours'\")\n", "display(accidents['Hours'].describe().T)"], ["**To futher explore this relationship we can break down the time of the accident into a few categories\n", " Intuitively, accidents are more likely to happen when there are more vehicles on the road, let's make it \n", " easier to explore this by categorizing out times into the following:**\n", "\n", "1. Morning Commute hours -> **5AM - 9AM**\n", "2. Regular traffic hours -> **10AM - 3PM**\n", "3. Afternoon Commute hours -> **4PM - 7PM**\n", "4. Night time commute hours -> **8PM - 4AM**"], ["# Key value pair to be assigned based on the 24hr time of accident\n", "time_of_day = {\n", "                1: 'Morning Commute',\n", "                2: 'Regular traffic',\n", "                3: 'Afternoon Commute',\n", "                4: 'Night time'\n", "              }\n", "\n", "# Simple function to assign our hours a categor\n", "def define_time(Hours):\n", "    if(Hours >= 5 and Hours <= 10): return 1\n", "    elif(Hours > 10 and Hours <= 15): return 2\n", "    elif(Hours > 15 and Hours <= 19): return 3\n", "    elif(Hours > 19 and Hours <= 23 or Hours < 5): return 4\n", "    \n", "accidents['time_of_day'] = accidents['Hours'].apply(define_time)\n", "\n", "# Compare the Time, hours and our new category\n", "display(accidents[['Time', 'Hours', 'time_of_day']].head(15))\n", "\n", "# Drop the temporary column Hours\n", "accidents = accidents.drop(columns='Hours')"], ["# Drop some (probably unnecessary lables) with more time and resources I would like to explore these and\n", "# find if they have a deeper inpact for our application\n", "accidents = accidents.drop(columns=['2nd_Road_Class' , '2nd_Road_Number', \n", "                                   'Location_Easting_OSGR', 'Location_Northing_OSGR',\n", "                                   'LSOA_of_Accident_Location',\n", "                                   '1st_Road_Class', '1st_Road_Number'])\n", "\n", "accidents = accidents.dropna()\n", "accidents.isna().sum().sum()"], ["## Explore taget data\n", "\n", "**Lets now take a closer look at the taget feature, *Did_Police_Officer_Attend_Scene_of_Accident* and we'll also try to isolate data that might not be too relevant**"], ["### Categorical data\n", "\n", "Converting categorical data currently stored in the dataframe as int or string, model will perform better if\n", "these features are in the format they are intended to be in. We'll also have to do some hot encoding later\n", "because of the number of categories per feature"], ["for col in ['Accident_Severity', 'time_of_day',\n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards']:\n", "    accidents[col] = accidents[col].astype('category')"], ["categorical = ['Accident_Severity', 'time_of_day', \n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards']\n", "target = ['Did_Police_Officer_Attend_Scene_of_Accident']\n", "\n", "cols = categorical + target\n", "\n", "# copy dataframe - just to be safe\n", "df_model = accidents[cols].copy()\n", "display(df_model.shape)"], ["**Great now we have our categorical data into one dataframe; however, if we feed this to our ML algorithm it might not perfom as well as it could. Therefore, we will perform *one hot encoding* on our data and thankfully this is very easy with pandas using the *getdummies* function**"], ["dummies = pd.get_dummies(df_model[categorical], drop_first=True)\n", "df_model = pd.concat([df_model[target], dummies], axis=1)\n", "display(\"shape of our DF\"), display(df_model.shape)\n", "display(df_model.head(20))"], ["### Find and remove duplicated values\n", "\n", "some values are in the set twice, let's fix that"], ["# Find and display duplicate columns\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "display(print(\"Duplicated Columns\\n\",duplicate_columns))\n", "\n", "# Remove duplicate columns\n", "df_model = df_model.loc[:,~df_model.columns.duplicated()]\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "\n", "# Ensure we dont have duplicates again\n", "display(print(duplicate_columns))"], ["# Training our model"], ["# First we need to clearly define what column we want to target and which are the features\n", "features = df_model.drop(['Did_Police_Officer_Attend_Scene_of_Accident'], axis=1)\n", "\n", "target = df_model[['Did_Police_Officer_Attend_Scene_of_Accident']]\n", "\n", "# 70/30 split\n", "X, X_test, y, y_test = train_test_split(features, target, test_size=0.3)"], ["Looking even closer at the target data note that it is incredibly uneven, there are 119,607 samples of an officer\n", "showin up at the scene but only 26,715 of an officer not showing up.\n", "\n", "When one class dominates the other SMOTE is useful, it is Synthetic Minority Oversampling TEchnique.\n", "Here is an explanation of how [SMOTE](http://rikunert.com/SMOTE_explained) works "], ["display(df_model['Did_Police_Officer_Attend_Scene_of_Accident'].value_counts())\n", "\n", "# resample data\n", "X_resampled, y_resampled = SMOTE().fit_sample(X, y) \n", "\n", "X, X_test, y, y_test = train_test_split(X_resampled, y_resampled, random_state=0)\n", "display(pd.Series(y_resampled).value_counts())"], ["# Our model, a RandomForestClassifier\n", "# criterion = entropy, gini does not perform as well in this case\n", "# n_jobs = -1 means use all processors\n", "# n_estimators = numer of trees in the forest, 100 by default\n", "# min_samples_split = minimum number of samples needed to split in a node\n", "RFT_Model = RandomForestClassifier(criterion='entropy', n_jobs=-1, n_estimators=300, min_samples_split=4)\n", "\n", "\n", "# train\n", "RFT_Model.fit(X, y)\n", "\n", "# predict\n", "y_test_preds = RFT_Model.predict(X_test)\n", "\n", "# evaluate\n", "report = classification_report(y_test, y_test_preds)\n", "print('Classification Report Random Forest - with Entropy and SMOTE Upsampling: \\n', report)\n", "\n", "mse = mean_squared_error(y_test, y_test_preds)\n", "print(\"RMSE \"+str(math.sqrt(mse)))\n", "\n", "print(\"Cross Validation \")\n", "display(cross_val_score(RFT_Model, X, y, scoring='f1_macro', n_jobs=-1))"], ["# create confusion matrix# create confusion matrix\n", "matrix = confusion_matrix(y_test, y_test_preds)\n", "\n", "# create dataframe\n", "class_names = df_model.Did_Police_Officer_Attend_Scene_of_Accident.values\n", "dataframe = pd.DataFrame(matrix, index=['Attended', 'Did not attend'], \n", "                         columns=['Attended', 'Did not attend'])\n", "\n", "# create heatmap\n", "sns.heatmap(dataframe, annot=True, cbar=None, cmap='Blues')\n", "plt.title('Did Officer Attend The Scene Of the Accident')\n", "plt.tight_layout(), plt.xlabel('Actual'), plt.ylabel('Prediction')\n", "plt.show()\n", "\n", "# plot the important features\n", "feat_importances = pd.Series(RFT_Model.feature_importances_, index=features.columns)\n", "feat_importances.nlargest(10).sort_values().plot(kind='bar',  figsize=(15,8), color='darkblue')\n", "plt.xlabel('Feature importance');"], ["# save the model to disk\n", "if(not path.exists(filename)):\n", "    pickle.dump(RFT_Model, open(filename, 'wb'))\n", "else:\n", "    print(\"Removing old model in favor of new one\")\n", "    os.remove(filename)\n", "    pickle.dump(RFT_Model, open(filename, 'wb'))"], []]}, {"Accident-Data-Analysis-h2o.ipynb": [["# Accident response analysis\n", "\n", "## How likely is it that an officer will attend the scene of an accident?\n", "\n", "\n", "### How to use\n", "\n", "Run these commands on your terminal of choice:\n", "\n", "~~~bash\n", "mkdir $HOME/github\n", "cd\n", "cd github\n", "git clone https://github.com/LopezChris/challenge.git\n", "jupyter notebook\n", "~~~\n", "\n", "finally select this notebook and run it"], ["# Create pandas dataframes\n", "import pandas as pd \n", "# Import data from local file system\n", "import os\n", "# Check if file exists\n", "from os import path\n", "# Plot graph\n", "from matplotlib import pyplot as plt\n", "# Heatmap\n", "import seaborn as sns\n", "# Display multiple lines\n", "from IPython.display import display\n", "# Dont show warning or deprication messages\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "# Split data into train and test datasets\n", "from sklearn.model_selection import train_test_split\n", "# Random forest classifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import mean_squared_error\n", "# Review results with a confusion matrix\n", "from sklearn.metrics import confusion_matrix\n", "# Show reports of trained model\n", "from sklearn.metrics import classification_report\n", "from sklearn.model_selection import KFold, cross_val_score\n", "# Upsampling to address the big difference in the data\n", "from imblearn.over_sampling import SMOTE\n", "from pathlib import Path\n", "home = str(Path.home())\n", "import pickle\n", "import h2o\n", "from h2o.automl import H2OAutoML\n", "\n", "# Data Path\n", "data_path=\"/github/h2o/input/DfTRoadSafety_Accidents_2014.csv\"\n", "# To save the model\n", "filename='accident_police_present.sav'"], ["### Data Exploration\n", "\n", "First I will inspect the data in order to see if there are any null columns or values, I will also ensure that the data is in its proper format"], ["# Read data from local file storage \n", "accidents = pd.read_csv(home + data_path)\n", "# Sample data\n", "display(accidents.head(10))\n", "# Ensure that the data is in the correct format\n", "display(accidents.info())\n", "\n", "# Ensure that there are no null values\n", "display(accidents['Time'].describe().T)\n", "display(\"Null Values in Time \" + str(accidents['Time'].isnull().sum()))"], ["**Note that the date feature in the data set is an object when it should be a datetime variable, let's fix that**"], ["# After inspecting the data info note that the date is an object when it should be a datetime variable\n", "accidents['Date']= pd.to_datetime(accidents['Date'], format=\"%d/%m/%Y\")\n", "\n", "# Keep an eye out on the dtype\n", "display(accidents.iloc[:,9:10].info())\n", "display(accidents['Time'].head())"], ["**Cool, now it is in its proper value. I think that the hour in which an accident is far more relevant than the minute, so I will isolate the hours from the minutes in a new column. Further, the time of day in which the accident occured is more relevant than a single hour, we'll address that in the next cells**"], ["# Create new column named Hours\n", "# print(accidents['Time'].str[0:2])\n", "accidents['Hours'] = accidents['Time'].str[0:2]\n", "# Convert from str to float64\n", "Hours = pd.to_numeric(accidents['Hours'])\n", "# Convert to int\n", "Hours = accidents['Hours'].astype('int')\n", "accidents['Hours'] = Hours\n", "\n", "# Verify conversion\n", "print(\"Hours are now type: \"+ str(type(accidents['Hours'][0])))\n", "display(accidents['Hours'].head(10))\n", "\n", "# Are there any null values?\n", "print('\\n')\n", "display(\"NA values in accidents[Hours]= \"+ str(accidents['Hours'].isna().sum()))"], ["# Gain a little more insight on the new column\n", "print(\"\\nDescribing 'Hours'\")\n", "display(accidents['Hours'].describe().T)"], ["**To futher explore this relationship we can break down the time of the accident into a few categories\n", " Intuitively, accidents are more likely to happen when there are more vehicles on the road, let's make it \n", " easier to explore this by categorizing out times into the following:**\n", "\n", "1. Morning Commute hours -> **5AM - 9AM**\n", "2. Regular traffic hours -> **10AM - 3PM**\n", "3. Afternoon Commute hours -> **4PM - 7PM**\n", "4. Night time commute hours -> **8PM - 4AM**"], ["# Key value pair to be assigned based on the 24hr time of accident\n", "time_of_day = {\n", "                1: 'Morning Commute',\n", "                2: 'Regular traffic',\n", "                3: 'Afternoon Commute',\n", "                4: 'Night time'\n", "              }\n", "\n", "# Simple function to assign our hours a categor\n", "def define_time(Hours):\n", "    if(Hours >= 5 and Hours <= 10): return 1\n", "    elif(Hours > 10 and Hours <= 15): return 2\n", "    elif(Hours > 15 and Hours <= 19): return 3\n", "    elif(Hours > 19 and Hours <= 23 or Hours < 5): return 4\n", "    \n", "accidents['time_of_day'] = accidents['Hours'].apply(define_time)\n", "\n", "# Compare the Time, hours and our new category\n", "display(accidents[['Time', 'Hours', 'time_of_day']].head(15))\n", "\n", "# Drop the temporary column Hours\n", "accidents = accidents.drop(columns='Hours')"], ["# Drop some (probably unnecessary lables) with more time and resources I would like to explore these and\n", "# find if they have a deeper inpact for our application\n", "accidents = accidents.drop(columns=['2nd_Road_Class' , '2nd_Road_Number', \n", "                                   'Location_Easting_OSGR', 'Location_Northing_OSGR',\n", "                                   'LSOA_of_Accident_Location',\n", "                                   '1st_Road_Class', '1st_Road_Number'])\n", "\n", "accidents = accidents.dropna()\n", "accidents.isna().sum().sum()"], ["## Explore taget data\n", "\n", "**Lets now take a closer look at the taget feature, *Did_Police_Officer_Attend_Scene_of_Accident* and we'll also try to isolate data that might not be too relevant**"], ["### Categorical data\n", "\n", "Converting categorical data currently stored in the dataframe as int or string, model will perform better if\n", "these features are in the format they are intended to be in. We'll also have to do some hot encoding later\n", "because of the number of categories per feature"], ["for col in ['Accident_Severity', 'time_of_day',\n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards', 'Urban_or_Rural_Area']:\n", "    accidents[col] = accidents[col].astype('category')"], ["categorical = ['Accident_Severity', 'time_of_day', \n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards', 'Urban_or_Rural_Area']\n", "target = ['Did_Police_Officer_Attend_Scene_of_Accident']\n", "\n", "cols = categorical + target\n", "\n", "# copy dataframe - just to be safe\n", "df_model = accidents[cols].copy()\n", "display(df_model.shape)"], ["**Great now we have our categorical data into one dataframe; however, if we feed this to our ML algorithm it might not perfom as well as it could. Therefore, we will perform *one hot encoding* on our data and thankfully this is very easy with pandas using the *getdummies* function**"], ["dummies = pd.get_dummies(df_model[categorical], drop_first=True)\n", "df_model = pd.concat([df_model[target], dummies], axis=1)\n", "display(\"shape of our DF\"), display(df_model.shape)\n", "display(df_model.head(20))"], ["# Training our model"], ["h2o.init()"], ["# Find and display duplicate columns\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "display(print(duplicate_columns))\n", "# Remove duplicate columns\n", "df_model = df_model.loc[:,~df_model.columns.duplicated()]\n", "\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "\n", "display(print(duplicate_columns))"], ["features = df_model.drop(['Did_Police_Officer_Attend_Scene_of_Accident'], axis=1)\n", "\n", "target = df_model[['Did_Police_Officer_Attend_Scene_of_Accident']]\n", "\n", "hf = h2o.H2OFrame(df_model)\n", "X = ['Did_Police_Officer_Attend_Scene_of_Accident', 'Accident_Severity_2',\n", "       'Accident_Severity_3', 'time_of_day_2', 'time_of_day_3',\n", "       'time_of_day_4', 'Speed_limit_30', 'Speed_limit_40', 'Speed_limit_50',\n", "       'Speed_limit_60', 'Speed_limit_70', 'Urban_or_Rural_Area_2',\n", "       'Police_Force_3', 'Police_Force_4', 'Police_Force_5', 'Police_Force_6',\n", "       'Police_Force_7', 'Police_Force_10', 'Police_Force_11',\n", "       'Police_Force_12', 'Police_Force_13', 'Police_Force_14',\n", "       'Police_Force_16', 'Police_Force_17', 'Police_Force_20',\n", "       'Police_Force_21', 'Police_Force_22', 'Police_Force_23',\n", "       'Police_Force_30', 'Police_Force_31', 'Police_Force_32',\n", "       'Police_Force_33', 'Police_Force_34', 'Police_Force_35',\n", "       'Police_Force_36', 'Police_Force_37', 'Police_Force_40',\n", "       'Police_Force_41', 'Police_Force_42', 'Police_Force_43',\n", "       'Police_Force_44', 'Police_Force_45', 'Police_Force_46',\n", "       'Police_Force_47', 'Police_Force_48', 'Police_Force_50',\n", "       'Police_Force_52', 'Police_Force_53', 'Police_Force_54',\n", "       'Police_Force_55', 'Police_Force_60', 'Police_Force_61',\n", "       'Police_Force_62', 'Police_Force_63', 'Police_Force_91',\n", "       'Police_Force_92', 'Police_Force_93', 'Police_Force_94',\n", "       'Police_Force_95', 'Police_Force_96', 'Police_Force_97',\n", "       'Police_Force_98', 'Day_of_Week_2', 'Day_of_Week_3', 'Day_of_Week_4',\n", "       'Day_of_Week_5', 'Day_of_Week_6', 'Day_of_Week_7',\n", "       'Pedestrian_Crossing-Human_Control_1',\n", "       'Pedestrian_Crossing-Human_Control_2', 'Light_Conditions_4',\n", "       'Light_Conditions_5', 'Light_Conditions_6', 'Light_Conditions_7',\n", "       'Road_Surface_Conditions_1', 'Road_Surface_Conditions_2',\n", "       'Road_Surface_Conditions_3', 'Road_Surface_Conditions_4',\n", "       'Road_Surface_Conditions_5', 'Special_Conditions_at_Site_0',\n", "       'Special_Conditions_at_Site_1', 'Special_Conditions_at_Site_2',\n", "       'Special_Conditions_at_Site_3', 'Special_Conditions_at_Site_4',\n", "       'Special_Conditions_at_Site_5', 'Special_Conditions_at_Site_6',\n", "       'Special_Conditions_at_Site_7', 'Carriageway_Hazards_0',\n", "       'Carriageway_Hazards_1', 'Carriageway_Hazards_2',\n", "       'Carriageway_Hazards_3', 'Carriageway_Hazards_6',\n", "       'Carriageway_Hazards_7']\n", "aml = H2OAutoML(max_models = 10, seed = 1)\n", "aml.train(x = X, y = 'Did_Police_Officer_Attend_Scene_of_Accident', training_frame = hf)\n"], ["lb = aml.leaderboard"], ["lb.head()"], []]}, {"Accidents-Data-Analysis-XGBoost.ipynb": [["# Accident response analysis\n", "\n", "## How likely is it that an officer will attend the scene of an accident?\n", "\n", "\n", "### How to use\n", "\n", "Run these commands on your terminal of choice:\n", "\n", "~~~bash\n", "mkdir $HOME/github\n", "cd\n", "cd github\n", "git clone https://github.com/LopezChris/challenge.git\n", "jupyter notebook\n", "~~~\n", "\n", "finally select this notebook and run it"], ["\n", "import xgboost as xgb\n", "\n", "# Create pandas dataframes\n", "import pandas as pd \n", "# Import data from local file system\n", "import os\n", "# Check if file exists\n", "from os import path\n", "# Plot graph\n", "from matplotlib import pyplot as plt\n", "# Heatmap\n", "import seaborn as sns\n", "# Display multiple lines\n", "from IPython.display import display\n", "# Dont show warning or deprication messages\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "# Split data into train and test datasets\n", "from sklearn.model_selection import train_test_split\n", "# Random forest classifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "#from sklearn import GradientBoostingClassifie\n", "from sklearn.metrics import mean_squared_error\n", "# Review results with a confusion matrix\n", "from sklearn.metrics import confusion_matrix\n", "# Show reports of trained model\n", "from sklearn.metrics import classification_report\n", "from sklearn.model_selection import KFold, cross_val_score\n", "# Upsampling to address the big difference in the data\n", "from imblearn.over_sampling import SMOTE\n", "from pathlib import Path\n", "home = str(Path.home())\n", "import pickle\n", "import numpy as np\n", "\n", "# Data Path\n", "data_path=\"/github/h2o/input/DfTRoadSafety_Accidents_2014.csv\"\n", "# To save the model\n", "filename='accident_police_present.sav'"], ["### Data Exploration\n", "\n", "First I will inspect the data in order to see if there are any null columns or values, I will also ensure that the data is in its proper format"], ["# Read data from local file storage \n", "accidents = pd.read_csv(home + data_path)\n", "# Sample data\n", "display(accidents.head(10))\n", "# Ensure that the data is in the correct format\n", "display(accidents.info())\n", "\n", "# Ensure that there are no null values\n", "display(accidents['Time'].describe().T)\n", "display(\"Null Values in Time \" + str(accidents['Time'].isnull().sum()))"], ["**Note that the date feature in the data set is an object when it should be a datetime variable, let's fix that**"], ["# After inspecting the data info note that the date is an object when it should be a datetime variable\n", "accidents['Date']= pd.to_datetime(accidents['Date'], format=\"%d/%m/%Y\")\n", "\n", "# Keep an eye out on the dtype\n", "display(accidents.iloc[:,9:10].info())\n", "display(accidents['Time'].head())"], ["**Cool, now it is in its proper value. I think that the hour in which an accident is far more relevant than the minute, so I will isolate the hours from the minutes in a new column. Further, the time of day in which the accident occured is more relevant than a single hour, we'll address that in the next cells**"], ["# Create new column named Hours\n", "# print(accidents['Time'].str[0:2])\n", "accidents['Hours'] = accidents['Time'].str[0:2]\n", "# Convert from str to float64\n", "Hours = pd.to_numeric(accidents['Hours'])\n", "# Convert to int\n", "Hours = accidents['Hours'].astype('int')\n", "accidents['Hours'] = Hours\n", "\n", "# Verify conversion\n", "print(\"Hours are now type: \"+ str(type(accidents['Hours'][0])))\n", "display(accidents['Hours'].head(10))\n", "\n", "# Are there any null values?\n", "print('\\n')\n", "display(\"NA values in accidents[Hours]= \"+ str(accidents['Hours'].isna().sum()))"], ["# Gain a little more insight on the new column\n", "print(\"\\nDescribing 'Hours'\")\n", "display(accidents['Hours'].describe().T)"], ["**To futher explore this relationship we can break down the time of the accident into a few categories\n", " Intuitively, accidents are more likely to happen when there are more vehicles on the road, let's make it \n", " easier to explore this by categorizing out times into the following:**\n", "\n", "1. Morning Commute hours -> **5AM - 9AM**\n", "2. Regular traffic hours -> **10AM - 3PM**\n", "3. Afternoon Commute hours -> **4PM - 7PM**\n", "4. Night time commute hours -> **8PM - 4AM**"], ["# Key value pair to be assigned based on the 24hr time of accident\n", "time_of_day = {\n", "                1: 'Morning Commute',\n", "                2: 'Regular traffic',\n", "                3: 'Afternoon Commute',\n", "                4: 'Night time'\n", "              }\n", "\n", "# Simple function to assign our hours a categor\n", "def define_time(Hours):\n", "    if(Hours >= 5 and Hours <= 10): return 1\n", "    elif(Hours > 10 and Hours <= 15): return 2\n", "    elif(Hours > 15 and Hours <= 19): return 3\n", "    elif(Hours > 19 and Hours <= 23 or Hours < 5): return 4\n", "    \n", "accidents['time_of_day'] = accidents['Hours'].apply(define_time)\n", "\n", "# Compare the Time, hours and our new category\n", "display(accidents[['Time', 'Hours', 'time_of_day']].head(15))\n", "\n", "# Drop the temporary column Hours\n", "accidents = accidents.drop(columns='Hours')"], ["# Drop some (probably unnecessary lables) with more time and resources I would like to explore these and\n", "# find if they have a deeper inpact for our application\n", "accidents = accidents.drop(columns=['2nd_Road_Class' , '2nd_Road_Number', \n", "                                   'Location_Easting_OSGR', 'Location_Northing_OSGR',\n", "                                   'LSOA_of_Accident_Location',\n", "                                   '1st_Road_Class', '1st_Road_Number'])\n", "\n", "accidents = accidents.dropna()\n", "accidents.isna().sum().sum()"], ["## Explore taget data\n", "\n", "**Lets now take a closer look at the taget feature, *Did_Police_Officer_Attend_Scene_of_Accident* and we'll also try to isolate data that might not be too relevant**"], ["### Categorical data\n", "\n", "Converting categorical data currently stored in the dataframe as int or string, model will perform better if\n", "these features are in the format they are intended to be in. We'll also have to do some hot encoding later\n", "because of the number of categories per feature"], ["for col in ['Accident_Severity', 'time_of_day',\n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards', 'Urban_or_Rural_Area']:\n", "    accidents[col] = accidents[col].astype('category')"], ["categorical = ['Accident_Severity', 'time_of_day', \n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards', 'Urban_or_Rural_Area']\n", "target = ['Did_Police_Officer_Attend_Scene_of_Accident']\n", "\n", "cols = categorical + target\n", "\n", "# copy dataframe - just to be safe\n", "df_model = accidents[cols].copy()\n", "display(df_model.shape)"], ["**Great now we have our categorical data into one dataframe; however, if we feed this to our ML algorithm it might not perfom as well as it could. Therefore, we will perform *one hot encoding* on our data and thankfully this is very easy with pandas using the *getdummies* function**"], ["dummies = pd.get_dummies(df_model[categorical], drop_first=True)\n", "df_model = pd.concat([df_model[target], dummies], axis=1)\n", "display(\"shape of our DF\"), display(df_model.shape)\n", "display(df_model.head(20))"], ["# Training our model"], ["# Find and display duplicate columns\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "display(print(duplicate_columns))\n", "# Remove duplicate columns\n", "df_model = df_model.loc[:,~df_model.columns.duplicated()]\n", "\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "\n", "display(print(duplicate_columns))\n", "\n", "# First we need to clearly define what column we want to target and which are the features\n", "features = df_model.drop(['Did_Police_Officer_Attend_Scene_of_Accident'], axis=1)\n", "\n", "target = df_model[['Did_Police_Officer_Attend_Scene_of_Accident']]\n", "\n", "# 70/30 split\n", "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3)\n", "\n", "\n", "\n", "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.8, max_depth = 5, alpha = 10, n_estimators = 70)\n", "\n", "data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n", "\n", "xg_reg.fit(X_train,y_train)\n", "\n", "preds = xg_reg.predict(X_test)\n", "\n", "rmse = np.sqrt(mean_squared_error(y_test, preds))\n", "print(\"RMSE: %f\" % (rmse))"], ["# save the model to disk\n", "if(not path.exists(filename)):\n", "    pickle.dump(RFT_Model, open(filename, 'wb'))\n", "else:\n", "    print(\"Removing old model in favor of new one\")\n", "    os.remove(filename)\n", "    pickle.dump(RFT_Model, open(filename, 'wb'))"], []]}, {"Accidents-Data-Analysis-XGBoost-checkpoint.ipynb": [["# Accident response analysis\n", "\n", "## How likely is it that an officer will attend the scene of an accident?\n", "\n", "\n", "### How to use\n", "\n", "Run these commands on your terminal of choice:\n", "\n", "~~~bash\n", "mkdir $HOME/github\n", "cd\n", "cd github\n", "git clone https://github.com/LopezChris/challenge.git\n", "jupyter notebook\n", "~~~\n", "\n", "finally select this notebook and run it"], ["\n", "import xgboost as xgb\n", "\n", "# Create pandas dataframes\n", "import pandas as pd \n", "# Import data from local file system\n", "import os\n", "# Check if file exists\n", "from os import path\n", "# Plot graph\n", "from matplotlib import pyplot as plt\n", "# Heatmap\n", "import seaborn as sns\n", "# Display multiple lines\n", "from IPython.display import display\n", "# Dont show warning or deprication messages\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "# Split data into train and test datasets\n", "from sklearn.model_selection import train_test_split\n", "# Random forest classifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "#from sklearn import GradientBoostingClassifie\n", "from sklearn.metrics import mean_squared_error\n", "# Review results with a confusion matrix\n", "from sklearn.metrics import confusion_matrix\n", "# Show reports of trained model\n", "from sklearn.metrics import classification_report\n", "from sklearn.model_selection import KFold, cross_val_score\n", "# Upsampling to address the big difference in the data\n", "from imblearn.over_sampling import SMOTE\n", "from pathlib import Path\n", "home = str(Path.home())\n", "import pickle\n", "import numpy as np\n", "\n", "# Data Path\n", "data_path=\"/github/h2o/input/DfTRoadSafety_Accidents_2014.csv\"\n", "# To save the model\n", "filename='accident_police_present.sav'"], ["### Data Exploration\n", "\n", "First I will inspect the data in order to see if there are any null columns or values, I will also ensure that the data is in its proper format"], ["# Read data from local file storage \n", "accidents = pd.read_csv(home + data_path)\n", "# Sample data\n", "display(accidents.head(10))\n", "# Ensure that the data is in the correct format\n", "display(accidents.info())\n", "\n", "# Ensure that there are no null values\n", "display(accidents['Time'].describe().T)\n", "display(\"Null Values in Time \" + str(accidents['Time'].isnull().sum()))"], ["**Note that the date feature in the data set is an object when it should be a datetime variable, let's fix that**"], ["# After inspecting the data info note that the date is an object when it should be a datetime variable\n", "accidents['Date']= pd.to_datetime(accidents['Date'], format=\"%d/%m/%Y\")\n", "\n", "# Keep an eye out on the dtype\n", "display(accidents.iloc[:,9:10].info())\n", "display(accidents['Time'].head())"], ["**Cool, now it is in its proper value. I think that the hour in which an accident is far more relevant than the minute, so I will isolate the hours from the minutes in a new column. Further, the time of day in which the accident occured is more relevant than a single hour, we'll address that in the next cells**"], ["# Create new column named Hours\n", "# print(accidents['Time'].str[0:2])\n", "accidents['Hours'] = accidents['Time'].str[0:2]\n", "# Convert from str to float64\n", "Hours = pd.to_numeric(accidents['Hours'])\n", "# Convert to int\n", "Hours = accidents['Hours'].astype('int')\n", "accidents['Hours'] = Hours\n", "\n", "# Verify conversion\n", "print(\"Hours are now type: \"+ str(type(accidents['Hours'][0])))\n", "display(accidents['Hours'].head(10))\n", "\n", "# Are there any null values?\n", "print('\\n')\n", "display(\"NA values in accidents[Hours]= \"+ str(accidents['Hours'].isna().sum()))"], ["# Gain a little more insight on the new column\n", "print(\"\\nDescribing 'Hours'\")\n", "display(accidents['Hours'].describe().T)"], ["**To futher explore this relationship we can break down the time of the accident into a few categories\n", " Intuitively, accidents are more likely to happen when there are more vehicles on the road, let's make it \n", " easier to explore this by categorizing out times into the following:**\n", "\n", "1. Morning Commute hours -> **5AM - 9AM**\n", "2. Regular traffic hours -> **10AM - 3PM**\n", "3. Afternoon Commute hours -> **4PM - 7PM**\n", "4. Night time commute hours -> **8PM - 4AM**"], ["# Key value pair to be assigned based on the 24hr time of accident\n", "time_of_day = {\n", "                1: 'Morning Commute',\n", "                2: 'Regular traffic',\n", "                3: 'Afternoon Commute',\n", "                4: 'Night time'\n", "              }\n", "\n", "# Simple function to assign our hours a categor\n", "def define_time(Hours):\n", "    if(Hours >= 5 and Hours <= 10): return 1\n", "    elif(Hours > 10 and Hours <= 15): return 2\n", "    elif(Hours > 15 and Hours <= 19): return 3\n", "    elif(Hours > 19 and Hours <= 23 or Hours < 5): return 4\n", "    \n", "accidents['time_of_day'] = accidents['Hours'].apply(define_time)\n", "\n", "# Compare the Time, hours and our new category\n", "display(accidents[['Time', 'Hours', 'time_of_day']].head(15))\n", "\n", "# Drop the temporary column Hours\n", "accidents = accidents.drop(columns='Hours')"], ["# Drop some (probably unnecessary lables) with more time and resources I would like to explore these and\n", "# find if they have a deeper inpact for our application\n", "accidents = accidents.drop(columns=['2nd_Road_Class' , '2nd_Road_Number', \n", "                                   'Location_Easting_OSGR', 'Location_Northing_OSGR',\n", "                                   'LSOA_of_Accident_Location',\n", "                                   '1st_Road_Class', '1st_Road_Number'])\n", "\n", "accidents = accidents.dropna()\n", "accidents.isna().sum().sum()"], ["## Explore taget data\n", "\n", "**Lets now take a closer look at the taget feature, *Did_Police_Officer_Attend_Scene_of_Accident* and we'll also try to isolate data that might not be too relevant**"], ["### Categorical data\n", "\n", "Converting categorical data currently stored in the dataframe as int or string, model will perform better if\n", "these features are in the format they are intended to be in. We'll also have to do some hot encoding later\n", "because of the number of categories per feature"], ["for col in ['Accident_Severity', 'time_of_day',\n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards', 'Urban_or_Rural_Area']:\n", "    accidents[col] = accidents[col].astype('category')"], ["categorical = ['Accident_Severity', 'time_of_day', \n", "            'Speed_limit', 'Urban_or_Rural_Area',\n", "            'Police_Force', 'Day_of_Week',\n", "            'Pedestrian_Crossing-Human_Control', 'Light_Conditions',\n", "            'Road_Surface_Conditions', 'Special_Conditions_at_Site',\n", "            'Carriageway_Hazards', 'Urban_or_Rural_Area']\n", "target = ['Did_Police_Officer_Attend_Scene_of_Accident']\n", "\n", "cols = categorical + target\n", "\n", "# copy dataframe - just to be safe\n", "df_model = accidents[cols].copy()\n", "display(df_model.shape)"], ["**Great now we have our categorical data into one dataframe; however, if we feed this to our ML algorithm it might not perfom as well as it could. Therefore, we will perform *one hot encoding* on our data and thankfully this is very easy with pandas using the *getdummies* function**"], ["dummies = pd.get_dummies(df_model[categorical], drop_first=True)\n", "df_model = pd.concat([df_model[target], dummies], axis=1)\n", "display(\"shape of our DF\"), display(df_model.shape)\n", "display(df_model.head(20))"], ["# Training our model"], ["# Find and display duplicate columns\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "display(print(duplicate_columns))\n", "# Remove duplicate columns\n", "df_model = df_model.loc[:,~df_model.columns.duplicated()]\n", "\n", "duplicate_columns = df_model.columns[df_model.columns.duplicated()]\n", "\n", "display(print(duplicate_columns))\n", "\n", "# First we need to clearly define what column we want to target and which are the features\n", "features = df_model.drop(['Did_Police_Officer_Attend_Scene_of_Accident'], axis=1)\n", "\n", "target = df_model[['Did_Police_Officer_Attend_Scene_of_Accident']]\n", "\n", "# 70/30 split\n", "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3)\n", "\n", "\n", "\n", "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.8, max_depth = 5, alpha = 10, n_estimators = 70)\n", "\n", "data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n", "\n", "xg_reg.fit(X_train,y_train)\n", "\n", "preds = xg_reg.predict(X_test)\n", "\n", "rmse = np.sqrt(mean_squared_error(y_test, preds))\n", "print(\"RMSE: %f\" % (rmse))"], ["# Our model, a RandomForestClassifier\n", "# criterion = entropy, gini does not perform as well in this case\n", "# n_jobs = -1 means use all processors\n", "# n_estimators = numer of trees in the forest, 100 by default\n", "# min_samples_split = minimum number of samples needed to split in a node\n", "RFT_Model = RandomForestClassifier(criterion='entropy', n_jobs=-1, n_estimators=300, min_samples_split=4)\n", "\n", "\n", "# train\n", "RFT_Model.fit(X, y)\n", "\n", "# predict\n", "y_test_preds = RFT_Model.predict(X_test)\n", "\n", "# evaluate\n", "report = classification_report(y_test, y_test_preds)\n", "print('Classification Report Random Forest - with Entropy and SMOTE Upsampling: \\n', report)"], ["# create confusion matrix# create confusion matrix\n", "matrix = confusion_matrix(y_test, y_test_preds)\n", "\n", "# create dataframe\n", "class_names = df_model.Did_Police_Officer_Attend_Scene_of_Accident.values\n", "dataframe = pd.DataFrame(matrix, index=['Attended', 'Did not attend'], \n", "                         columns=['Attended', 'Did not attend'])\n", "\n", "# create heatmap\n", "sns.heatmap(dataframe, annot=True, cbar=None, cmap='Blues')\n", "plt.title('Did Officer Attend The Scene Of the Accident')\n", "plt.tight_layout(), plt.xlabel('Actual'), plt.ylabel('Prediction')\n", "plt.show()\n", "\n", "# plot the important features\n", "feat_importances = pd.Series(RFT_Model.feature_importances_, index=features.columns)\n", "feat_importances.nlargest(10).sort_values().plot(kind='bar',  figsize=(15,8), color='darkblue')\n", "plt.xlabel('Feature importance');"], ["# save the model to disk\n", "if(not path.exists(filename)):\n", "    pickle.dump(RFT_Model, open(filename, 'wb'))\n", "else:\n", "    print(\"Removing old model in favor of new one\")\n", "    os.remove(filename)\n", "    pickle.dump(RFT_Model, open(filename, 'wb'))"], []]}], "yjschein/Predicting_Car_Accident_Severity": [{"data_preprocessing.ipynb": [["## Predicting the Severity of a Car Accident\n", "- Student names: Jagandeep Singh & Yehuda Schein\n", "- Student pace: full-time\n", "- Scheduled project review date: 21 August\n", "- Instructor name: Sean Wilson"], ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from scipy.stats import chi2_contingency\n", "from statsmodels.stats.proportion import proportions_ztest\n", "from statsmodels.stats.outliers_influence import variance_inflation_factor\n", "from sklearn.preprocessing import PolynomialFeatures\n", "pd.set_option('display.max_columns', None)\n", "pd.set_option('display.max_rows', None)\n", "plt.rcParams.update({'font.size': 12})"], ["## Data Cleaning"], ["# read in the df\n", "df = pd.read_csv('Traffic_Crashes_-_Crashes.csv')\n", "\n", "# original shape of df\n", "print('original shape', df.shape) "], [" - Target variable is the 'CRASH_TYPE' column which will be renamed 'SEVERE'\n", " - The severity of the crash will be determined on whether or not their is an injury / a car being towed\n", " - This column will be a binary column with 0 representing no injury and 1 representing an injury"], ["# rename column\n", "df.rename(columns = {'CRASH_TYPE': 'SEVERE'}, inplace = True) \n", "df.SEVERE = df.SEVERE.replace({'NO INJURY / DRIVE AWAY':0,'INJURY AND / OR TOW DUE TO CRASH':1})"], ["df.SEVERE.value_counts()"], ["###### Dropped the inconclusive columns and the null values"], ["to_drop = ['CRASH_DATE_EST_I', 'RD_NO', 'LANE_CNT', 'REPORT_TYPE', 'INTERSECTION_RELATED_I', 'NOT_RIGHT_OF_WAY_I',\n", "           'HIT_AND_RUN_I','DATE_POLICE_NOTIFIED', 'STREET_NO','STREET_DIRECTION', 'BEAT_OF_OCCURRENCE',\n", "           'PHOTOS_TAKEN_I','STATEMENTS_TAKEN_I', 'DOORING_I', 'WORK_ZONE_I', 'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I',\n", "           'LATITUDE', 'LONGITUDE','LOCATION','CRASH_RECORD_ID','INJURIES_UNKNOWN','ALIGNMENT','SEC_CONTRIBUTORY_CAUSE',\n", "           'DEVICE_CONDITION', 'STREET_NAME']\n", "df.drop(columns = to_drop, inplace = True)\n", "\n", "# New shape of df after dropped columns \n", "print('new shape', df.shape) "], ["df.drop(df[df.MOST_SEVERE_INJURY.isna()].index, inplace = True)\n", "# New shape of df after dropped rows \n", "print('new shape', df.shape) "], ["###### Changed the dtype of the 'CRASH_DATE' column"], ["df['CRASH_DATE'] = pd.to_datetime(df.CRASH_DATE)"], ["###### Capped some values from certain continuous columns"], ["# capped at 4\n", "print(df['NUM_UNITS'].value_counts())\n", "df['NUM_UNITS'].values[df['NUM_UNITS'] > 4] = 4\n", "print(df['NUM_UNITS'].value_counts())"], ["# capped at 4\n", "print(df['INJURIES_TOTAL'].value_counts())\n", "df['INJURIES_TOTAL'].values[df['INJURIES_TOTAL'] > 4] = 4\n", "print(df['INJURIES_TOTAL'].value_counts())"], ["# capped at 5\n", "print(df['INJURIES_NO_INDICATION'].value_counts())\n", "df['INJURIES_NO_INDICATION'].values[df['INJURIES_NO_INDICATION'] > 5] = 5\n", "print(df['INJURIES_NO_INDICATION'].value_counts())"], ["###### Merged/Replaced values in certain categorical columns "], ["# Merged unknown and other into one variable \n", "df.TRAFFIC_CONTROL_DEVICE = df.TRAFFIC_CONTROL_DEVICE.replace({'UNKNOWN':'UNKNOWN/OTHER','OTHER':'UNKNOWN/OTHER'})\n", "df.ROAD_DEFECT = df.ROAD_DEFECT.replace({'UNKNOWN':'UNKNOWN/OTHER','OTHER':'UNKNOWN/OTHER'})"], ["# Changed to 6 categories\n", "df.WEATHER_CONDITION = df.WEATHER_CONDITION.replace({'UNKNOWN':'UNKNOWN/OTHER','OTHER':'UNKNOWN/OTHER',\n", "                                                    'SLEET/HAIL': 'SNOW', 'FREEZING RAIN/DRIZZLE': 'RAIN',\n", "                                                    'SEVERE CROSS WIND GATE': 'CLOUDY/OVERCAST','BLOWING SNOW': 'SNOW',\n", "                                                    'BLOWING SAND, SOIL, DIRT': 'UNKNOWN/OTHER'})\n", "df.WEATHER_CONDITION = df.WEATHER_CONDITION.replace({'FOG/SMOKE/HAZE': 'FOG', 'CLOUDY/OVERCAST': 'CLOUDY'})\n", "df.WEATHER_CONDITION.value_counts()"], ["# merged dusk and dawn because they are very similar lighting conditions \n", "df.LIGHTING_CONDITION = df.LIGHTING_CONDITION.replace({'DUSK': 'DUSK/DAWN','DAWN': 'DUSK/DAWN' })\n", "df.LIGHTING_CONDITION.value_counts()"], ["# merged two variables to 'Other'\n", "df.FIRST_CRASH_TYPE = df.FIRST_CRASH_TYPE.replace({'OTHER OBJECT':'OTHER','OTHER NONCOLLISION':'OTHER'})"], ["# merged the different types of intersections into one feature and unknown/other into one feature\n", "df.TRAFFICWAY_TYPE = df.TRAFFICWAY_TYPE.replace({'T-INTERSECTION':'INTERSECTION', 'UNKNOWN INTERSECTION TYPE':'INTERSECTION',\n", "                                                'Y-INTERSECTION':'INTERSECTION','L-INTERSECTION':'INTERSECTION',\n", "                                                 'FIVE POINT, OR MORE': 'INTERSECTION', 'FOUR WAY':'INTERSECTION',\n", "                                                 'ROUNDABOUT': 'INTERSECTION', 'OTHER': 'UNKNOWN/OTHER',\n", "                                                'UNKNOWN':'UNKNOWN/OTHER','NOT REPORTED':'UNKNOWN/OTHER',\n", "                                                'TRAFFIC ROUTE':'UNKNOWN/OTHER'})\n", "df.TRAFFICWAY_TYPE.value_counts()"], ["# Changed to 4 categories\n", "df.ROADWAY_SURFACE_COND = df.ROADWAY_SURFACE_COND.replace({'UNKNOWN':'UNKNOWN/OTHER','OTHER':'UNKNOWN/OTHER',\n", "                                                          'SAND, MUD, DIRT': 'UNKNOWN/OTHER','ICE': 'SNOW OR SLUSH'})\n", "df.ROADWAY_SURFACE_COND = df.ROADWAY_SURFACE_COND.replace({'SNOW OR SLUSH': 'SNOW'})\n", "df.ROADWAY_SURFACE_COND.value_counts()"], [], [], ["## Feature Engineering and Data Visualization"], ["###### Creating a date column by creating a day of month column and then merging it with the crash month column"], ["# creating a day column as a string \n", "df['day'] = pd.DatetimeIndex(df['CRASH_DATE']).day.astype(str)\n", "# merging month and day\n", "df['date'] = df['CRASH_MONTH'].astype(str).str.cat(df['day'].values.astype(str), sep='-')"], ["###### Creating a season column"], ["df['season'] = ' '\n", "df['season'] = np.where((df['CRASH_MONTH'] == 1) | \n", "                        (df['CRASH_MONTH'] == 2) | \n", "                        (df['CRASH_MONTH'] == 3), \n", "                        'Winter', df['season'])\n", "df['season'] = np.where((df['CRASH_MONTH'] == 4) | \n", "                        (df['CRASH_MONTH'] == 5) | \n", "                        (df['CRASH_MONTH'] == 6), \n", "                        'Spring', df['season'])\n", "df['season'] = np.where((df['CRASH_MONTH'] == 7) | \n", "                        (df['CRASH_MONTH'] == 8) | \n", "                        (df['CRASH_MONTH'] == 9), \n", "                        'Summer', df['season'])\n", "df['season'] = np.where((df['CRASH_MONTH'] == 10) | \n", "                        (df['CRASH_MONTH'] == 11) | \n", "                        (df['CRASH_MONTH'] == 12), \n", "                        'Fall', df['season'])"], ["df['day'] = df['day'].astype(int)"], ["## Data Visualization"], ["# CRASH_HOUR histogram plot\n", "plt.figure(figsize=(10,8))\n", "df.CRASH_HOUR.plot(kind = 'hist', bins = 24)\n", "plt.title('Hour of Accident')\n", "plt.xlabel('Hour of Day')\n", "plt.ylabel('Number of Accidents')\n", "plt.show()"], ["This graph shows that there is an increase an accidents around rush hour time"], ["df.PRIM_CONTRIBUTORY_CAUSE.value_counts(normalize = True).head(10).plot(kind = 'bar', figsize = (10,7))\n", "plt.title('Primary Cause of Accidents')\n", "plt.xlabel('Cause of Accident')\n", "plt.ylabel('Number of Accidents')\n", "plt.show()"], ["This graph shows that in most cases we are unable to determine the primary casue of the accident"], ["w = df.groupby(['WEATHER_CONDITION', 'SEVERE'])['WEATHER_CONDITION'].count().unstack()\n", "print((w/w.sum())*100)\n", "((w/w.sum())*100).transpose().plot(kind = 'bar', stacked = True, figsize = (12,8))\n", "plt.title('Weather Condition During Accident')\n", "plt.xlabel('SEVERITY')\n", "plt.ylabel('Percentage per weather condition')\n", "plt.show()"], ["r = df.groupby(['ROADWAY_SURFACE_COND', 'SEVERE'])['ROADWAY_SURFACE_COND'].count().unstack()\n", "print((r/r.sum())*100)\n", "((r/r.sum())*100).transpose().plot(kind = 'bar', stacked = True, figsize = (12,8))\n", "plt.title('Road Condition During Accident')\n", "plt.xlabel('SEVERITY')\n", "plt.ylabel('Percentage per road condition')\n", "plt.show()"], ["plt.figure(figsize=(10,8))\n", "g=sns.countplot(x=\"ROADWAY_SURFACE_COND\", data=df,hue=\"SEVERE\", palette=\"muted\")\n", "plt.title('Road Conditions')\n", "plt.xlabel('Road Condition')\n", "plt.ylabel('Number of Accidents')\n", "g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right', fontsize=10);"], ["plt.figure(figsize=(10,8))\n", "g=sns.countplot(x=\"DAMAGE\", data=df,hue=\"SEVERE\", palette=\"muted\")\n", "plt.title('Cost of Damage')\n", "plt.xlabel('Cost')\n", "plt.ylabel('Number of Accidents')\n", "g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right', fontsize=10);"], ["plt.figure(figsize=(10,8))\n", "g=sns.countplot(x=\"season\", data=df,hue=\"SEVERE\")\n", "plt.title('Season')\n", "plt.xlabel('Season')\n", "plt.ylabel('Number of Accidents')\n", "g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right', fontsize=10);"], ["s = df.groupby('FIRST_CRASH_TYPE').SEVERE.value_counts().unstack().transpose()[['PEDALCYCLIST', 'PEDESTRIAN']]\n", "\n", "s.transpose().plot(kind = 'bar', figsize = (8,6), title = 'Severity with Pedestrian and Pedalcyclist', stacked = True)\n", "plt.xlabel('Crash Type')\n", "plt.ylabel('Number of Accidents')\n"], ["### Creating Dummies"], ["dummies = ['TRAFFIC_CONTROL_DEVICE', 'WEATHER_CONDITION', 'LIGHTING_CONDITION', 'FIRST_CRASH_TYPE'\n", "          ,'TRAFFICWAY_TYPE', 'ROADWAY_SURFACE_COND', 'ROAD_DEFECT', 'DAMAGE', 'PRIM_CONTRIBUTORY_CAUSE', 'MOST_SEVERE_INJURY'\n", "          ,'season']\n", "df =  pd.get_dummies(df, columns = dummies, drop_first = True )\n", "df.shape"], ["##### Feature Interaction"], ["cont = ['POSTED_SPEED_LIMIT', 'NUM_UNITS', 'INJURIES_TOTAL', 'INJURIES_FATAL', 'INJURIES_INCAPACITATING',\n", "        'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION',\n", "        'CRASH_HOUR', 'CRASH_DAY_OF_WEEK','CRASH_MONTH' ,'day']\n", "poly_df = df[cont]\n", "poly = PolynomialFeatures(degree=2, include_bias=False)\n", "poly_data = poly.fit_transform(poly_df)\n", "poly_columns = poly.get_feature_names(poly_df.columns)\n", "df_poly = pd.DataFrame(poly_data, columns=poly_columns, index = df.index)\n", "\n", "df = pd.concat([df,df_poly], axis=1, join='inner')\n", "df = df.loc[:,~df.columns.duplicated()]\n", "df.shape"], ["## Feature Selection"], ["target = df['SEVERE']\n", "df_features = df.drop(columns = ['CRASH_DATE','SEVERE', 'date'])"], ["corr_matrix = df_features.corr().abs()\n", "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n", "#upper"], ["corr_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n", "print(corr_drop)"], ["df_features.drop(columns = corr_drop, inplace = True)"], ["## Hypothesis Tests"], ["w = w.transpose()\n", "print(w)\n", "\n", "# Null Hypothesis: Proportion of Severe Accidents is equal in Clear, Wet and Snow Weather Conditions.\n", "# Alternate hypothesis: Proportion of Severe Accidents is not equal in Clear, Wet and Snow Weather Conditions.\n", "\n", "\n", "data = [[261066, 27483, 12064], [79804, 11982, 4321]]\n", "stat, p, dof, expected = chi2_contingency(data)\n", "# interpret p-value \n", "alpha = 0.05\n", "print(\"p value is \" + str(p)) \n", "if p <= alpha: \n", "    print('Dependent (reject H0)') \n", "else: \n", "    print('Independent (H0 holds true)') \n", "    \n", "print('Proportion of Severe Accidents is not equal in Clear, Wet and Snow Weather Conditions')"], ["r = r.transpose()\n", "print(r)\n", "\n", "# Null Hypothesis: Proportion of Severe Accidents is equal in Dry, Snow and Wet Road Conditions.\n", "# Alternate hypothesis: Proportion of Severe Accidents is not equal Dry, Snow and Wet Road Conditions.\n", "\n", "\n", "data = [[247038, 13907, 41217], [75952, 4398, 1893]]\n", "stat, p, dof, expected = chi2_contingency(data)\n", "# interpret p-value \n", "alpha = 0.05\n", "print(\"p value is \" + str(p)) \n", "if p <= alpha: \n", "    print('Dependent (reject H0)') \n", "else: \n", "    print('Independent (H0 holds true)') \n", "    \n", "print('Proportion of Severe Accidents is not equal in Dry, Snow and Wet Road Conditions.')\n"], ["print(s)\n", "#Null Hypothesis: the proportion of severe and non severe accidents is equal in accidents involving predestrians.\n", "#Alternate Hypothesis : the proportion of severe accidenst is more than non severe accidents involving predestrians.\n", "\n", "stat, pval = proportions_ztest(4241, 6313, value = 0, alternative = 'larger')\n", "stat, pval\n", "\n", "#pval <<< 0.05\n", "# Reject Null Hypothesis\n", "# The proportion of severe accidenst is more than non severe accidents involving predestrians.\n"], ["p = df.groupby('FIRST_CRASH_TYPE').SEVERE.value_counts().unstack().transpose()[ 'PEDESTRIAN'].plot(kind = 'bar',\n", "                                title = 'Severity with Pedestrians',figsize = (8,6))\n", "plt.xlabel('Crash Type')\n", "plt.ylabel('Number of Accidents')"], ["### VIF"], ["vif = pd.DataFrame()\n", "vif[\"VIF Factor\"] = [variance_inflation_factor(df_features.values, i) for i in range(df_features.shape[1])]\n", "vif[\"features\"] = df_features.columns\n", "vif.round(1)"], ["vif_drop =  list(vif[vif['VIF Factor'] > 9]['features'])\n", "print(vif_drop)\n", "print(len(vif_drop))"], ["df_features.drop(columns = vif_drop, inplace = True)"], ["df_features['target'] = target"], ["df_features.to_pickle(\"data.pkl\")"], []]}, {"graph.ipynb": [["## Predicting the Severity of a Car Accident\n", "- Student names: Jagandeep Singh & Yehuda Schein\n", "- Student pace: full-time\n", "- Scheduled project review date: 21 August\n", "- Instructor name: Sean Wilson"], ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import geopandas as gpd\n", "from shapely.geometry import Point, Polygon\n", "pd.set_option('display.max_columns', None)\n", "pd.set_option('display.max_rows', None)\n", "plt.rcParams.update({'font.size': 12})"], ["df = pd.read_csv('Traffic_Crashes_-_Crashes.csv')"], ["df.isna().sum()"], ["df.LOCATION.dtype"], ["df.drop(df[df.LOCATION.isna()].index, inplace = True)"], ["df.head()"], ["street_map = gpd.read_file('geo_export_0d48d92e-3832-4cc0-a89c-b024e43268c8.shp')"], ["\n", "# zip x and y coordinates into single feature\n", "geometry = [Point(xy) for xy in zip(df['LONGITUDE'], df['LATITUDE'])]\n", "# create GeoPandas dataframe\n", "geo_df = gpd.GeoDataFrame(df, geometry = geometry)"], ["geo_df.head()"], ["first_10000 = geo_df.head(10000)"], ["\n", "\n", "fig, ax = plt.subplots(figsize = (15,15))\n", "street_map.plot(ax=ax, alpha = 0.8, color = 'black')\n", "\n", "first_10000[first_10000['CRASH_TYPE'] == 'NO INJURY / DRIVE AWAY'].plot(ax = ax , markersize = 2,\n", "                      color = 'red', marker = 's', label = 'Injury', aspect = 1.5)\n", "plt.legend(prop = {'size':12})\n", "ax.set_title('Not Severe Accidents in Chicago', fontdict ={'fontsize': 20})\n", "ax.set_ylabel(\"Latitude\",fontdict = {'fontsize': 20})\n", "ax.set_xlabel(\"Longitude\",fontdict = {'fontsize': 20})\n", "\n", "plt.xlim(-87.5,-88)\n", "plt.ylim( 41.6,42.05)\n", "# show map\n", "plt.show()\n"], ["fig, ax = plt.subplots(figsize = (15,15))\n", "street_map.plot(ax=ax, alpha = 0.8, color = 'black')\n", "first_10000[first_10000['CRASH_TYPE'] == 'INJURY AND / OR TOW DUE TO CRASH'].plot(ax = ax , markersize = 2,\n", "                      color = 'yellow', marker = 's', label = 'Injury', aspect = 1.5)\n", "plt.legend(prop = {'size':12})\n", "ax.set_title('Severe Accidents in Chicago', fontdict ={'fontsize': 20})\n", "ax.set_ylabel(\"Latitude\",fontdict = {'fontsize': 20})\n", "ax.set_xlabel(\"Longitude\",fontdict = {'fontsize': 20})\n", "\n", "plt.xlim(-87.5,-88)\n", "plt.ylim( 41.6,42.05)\n", "# show map\n", "plt.show()"], ["fig, ax = plt.subplots(figsize = (15,15))\n", "street_map.plot(ax=ax, alpha = 0.8, color = 'lightgrey')\n", "\n", "first_10000[first_10000['CRASH_TYPE'] == 'INJURY AND / OR TOW DUE TO CRASH'].plot(ax = ax , markersize = 2,\n", "                      color = 'blue',marker = 's',label = 'Injury', aspect = 1.5)\n", "first_10000[first_10000['CRASH_TYPE'] == 'NO INJURY / DRIVE AWAY'].plot(ax = ax , markersize = 2,\n", "                      color = 'red',marker = 'v',label = 'No-Injury',aspect = 1.5)\n", "\n", "\n", "plt.legend(prop = {'size':12})\n", "ax.set_title('Car Accidents in Chicago', fontdict ={'fontsize': 20})\n", "ax.set_ylabel(\"Latitude\",fontdict = {'fontsize': 20})\n", "ax.set_xlabel(\"Longitude\",fontdict = {'fontsize': 20})\n", "plt.xlim(-87.5,-88)\n", "plt.ylim( 41.6,42.05)\n", "# show map\n", "plt.show()"]]}, {"models.ipynb": [["## Predicting the Severity of a Car Accident\n", "- Student names: Jagandeep Singh & Yehuda Schein\n", "- Student pace: full-time\n", "- Scheduled project review date:  21 August\n", "- Instructor name: Sean Wilson"], ["import pandas as pd\n", "import numpy as np\n", "from sklearn.linear_model import Lasso\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import train_test_split\n", "import matplotlib.pyplot as plt\n", "from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn.preprocessing import StandardScaler\n", "from statsmodels.stats.outliers_influence import variance_inflation_factor\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn import metrics\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.utils import resample\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.ensemble import BaggingClassifier\n", "from sklearn.ensemble import VotingClassifier\n", "from sklearn.feature_selection import RFECV\n", "from sklearn.preprocessing import OneHotEncoder\n", "import xgboost as xgb\n", "from matplotlib import pyplot\n", "from sklearn.metrics import plot_confusion_matrix\n", "pd.set_option('display.max_columns', None)\n", "pd.set_option('display.max_rows', None)"], ["df = pd.read_pickle(\"data.pkl\")"], ["df.shape"], ["df.head()"], ["## Lasso"], ["target = df['target']\n", "df_features = df.drop(columns = ['target'])"], ["lasso = Lasso(alpha = 0.01, normalize=False)\n", "lasso.fit(df_features, target)"], ["lasso_coef01 = pd.DataFrame(data=lasso.coef_).T\n", "lasso_coef01.columns = df_features.columns\n", "lasso_coef01 = lasso_coef01.T.sort_values(by=0).T\n", "lasso_coef01.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))"], ["lasso_coef01.T"], ["df_lr = df_features"], ["lasso_drop = list(lasso_coef01.T[lasso_coef01.T[0] == 0].index)"], ["df_lr.drop(columns = lasso_drop, inplace = True)"], ["df_lr.shape"], ["df_lr.head()"], ["## Logistic Regression with Lasso"], ["X_train, X_test, y_train, y_test = train_test_split(df_lr, target, random_state=1)\n", "scaler = StandardScaler() \n", "scaler.fit(X_train)\n", "X_train = scaler.transform(X_train)  \n", "X_test = scaler.transform(X_test)"], ["logreg = LogisticRegression()\n", "logreg.fit(X_train, y_train)\n", "y_pred_train = logreg.predict(X_train)\n", "y_pred = logreg.predict(X_test)\n"], ["print(metrics.f1_score(y_train, y_pred_train))\n", "print(metrics.accuracy_score(y_test, y_pred))\n", "print(metrics.f1_score(y_test, y_pred))"], ["scores = {}\n", "scores['Logistic with Lasso'] = [metrics.f1_score(y_train, y_pred_train), metrics.f1_score(y_test, y_pred),metrics.accuracy_score(y_test, y_pred)]\n", "scores\n"], ["## Logistic without Lasso"], ["target = df['target']\n", "df_features = df.drop(columns = ['target'])"], ["X_train, X_test, y_train, y_test = train_test_split(df_features, target, random_state=1)\n", "scaler = StandardScaler() \n", "scaler.fit(X_train)\n", "X_train = scaler.transform(X_train)  \n", "X_test = scaler.transform(X_test)"], ["logreg = LogisticRegression()\n", "logreg.fit(X_train, y_train)\n", "y_pred_train = logreg.predict(X_train)\n", "y_pred = logreg.predict(X_test)"], ["print(metrics.f1_score(y_train, y_pred_train))\n", "print(metrics.accuracy_score(y_test, y_pred))\n", "print(metrics.f1_score(y_test, y_pred))"], ["scores['Logistic Regression'] = [metrics.f1_score(y_train, y_pred_train), metrics.f1_score(y_test, y_pred), metrics.accuracy_score(y_test, y_pred)]\n", "scores"], ["## Logistic with Class weight balanced\n"], ["lr_clf_weighted = LogisticRegression(solver='liblinear', class_weight = 'balanced')\n", "lr_clf_weighted.fit(X_train, y_train)\n", "y_weighted_train = lr_clf_weighted.predict(X_train)\n", "y_weighted_test = lr_clf_weighted.predict(X_test)\n", "\n", "f1_train = metrics.f1_score(y_train, y_weighted_train)\n", "f1_test = metrics.f1_score(y_test, y_weighted_test)\n", "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_weighted_test))\n", "print('Train F1 score: ', f1_train)\n", "print('Test F1 score: ', f1_test)"], ["len(lr_clf_weighted.coef_[0])"], ["len(df_features.columns )"], ["coef_names = list(zip(df_features.columns, lr_clf_weighted.coef_[0]))"], ["log_bal = sorted(coef_names, key=lambda x: abs(x[1]), reverse = True)"], ["log_bal[0:15]"], ["scores['Logistic with class weight balanced'] = [f1_train, f1_test, metrics.accuracy_score(y_test, y_weighted_test)]"], ["scores"], ["## Decision Tree class weight balanced"], ["clf = DecisionTreeClassifier(class_weight='balanced')"], ["clf = clf.fit(X_train,y_train)\n", "y_pred_train = clf.predict(X_train)\n", "y_pred_test = clf.predict(X_test)\n", "\n", "# Model Accuracy, how often is the classifier correct?\n", "print(\"Training F1 Score:\",metrics.f1_score(y_train, y_pred_train))\n", "df1 = metrics.f1_score(y_test, y_pred_test)\n", "print(\"Testing F1 Score:\",df1)"], ["metrics.accuracy_score(y_test, y_pred_test)"], ["importance = clf.feature_importances_\n", "dt_coef = list(zip(df_features.columns, importance))"], ["dt_coef = sorted(coef_names, key=lambda x: abs(x[1]), reverse = True)\n", "dt_coef[0:15]"], ["scores['Decision Tree'] = [metrics.f1_score(y_train, y_pred_train), metrics.f1_score(y_test, y_pred_test), metrics.accuracy_score(y_test, y_pred_test)]"], ["scores"], ["## Random Forest"], ["X_train, X_test, y_train, y_test = train_test_split(df_features, target, test_size=0.25, random_state=23 )"], ["rfc = RandomForestClassifier(random_state = 1, n_estimators = 500, max_depth=2, n_jobs = -1)\n", "rfc.fit(X_train, y_train)"], ["rfc_preds_train = rfc.predict(X_train)\n", "rfc_preds = rfc.predict(X_test)\n", "\n", "rfc_f1_train = metrics.f1_score(y_train, rfc_preds_train)\n", "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n", "accuracy = metrics.accuracy_score(y_test, rfc_preds)\n", "print('Train F1 score: ', rfc_f1_train)\n", "print('Test F1 score: ', rfc_f1)\n"], ["scores['Random Forest'] = [rfc_f1_train, rfc_f1, accuracy]"], ["scores"], ["## Grid Search with Random Forest"], ["params = { \n", "    'n_estimators': [100,500],\n", "    'max_depth': [2,3,4],\n", "    'max_features': [0.6, 0.7],\n", "    'criterion': ['gini', 'entropy'],\n", "    'class_weight': [None, 'balanced']}"], ["rf = RandomForestClassifier()\n", "grid_tree = GridSearchCV(rf, params, cv=5, scoring='f1', n_jobs =-1, verbose = 1)\n", "grid_tree.fit(X_train,y_train)"], ["print(grid_tree.best_score_)\n", "\n", "print(grid_tree.best_params_)\n", "\n", "print(grid_tree.best_estimator_)\n"], ["y_pred_train = grid_tree.best_estimator_.predict(X_train)\n", "y_pred_test = grid_tree.best_estimator_.predict(X_test)\n", "\n", "\n", "# checking accuracy\n", "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_test))\n", "print(\"Training F1:\",metrics.f1_score(y_train, y_pred_train))\n", "print(\"Test F1:\",metrics.f1_score(y_test, y_pred_test))\n"], ["scores['Random Forest with Grid Search'] = [0.5355843799327432, 0.5381568455978806, 0.6653122087604846]\n", "scores"], ["### Random Forest with class weight balanced "], ["rfc = RandomForestClassifier(class_weight = 'balanced',random_state = 1, n_estimators = 500, max_depth=2, n_jobs = -1, verbose = 1)\n", "rfc.fit(X_train, y_train)\n", "rfc_preds_train = rfc.predict(X_train)\n", "rfc_preds = rfc.predict(X_test)\n", "\n", "rfc_f1_train = metrics.f1_score(y_train, rfc_preds_train)\n", "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n", "rfc_accuracy = metrics.accuracy_score(y_test, rfc_preds)\n", "print('Train F1 score: ', rfc_f1_train)\n", "print('Test F1 score: ', rfc_f1)\n", "print('Accuracy: ', rfc_accuracy)"], ["rf_coef = list(zip(df_features.columns, rfc.feature_importances_))\n", "rf_coef = sorted(coef_names, key=lambda x: abs(x[1]), reverse = True)"], ["rf_coef[0:15]"], ["scores['Random Forest with class balance'] = [rfc_f1_train, rfc_f1, rfc_accuracy]"], ["scores"], ["### Decision Tree with Grid Search"], ["dcg = DecisionTreeClassifier(class_weight='balanced',random_state=1)"], ["parameters = {'criterion': ['gini', 'entropy'],\n", "            'max_depth': range(1,10,2),\n", "              'max_features': [0.5,0.6,0.7,0.8]\n", "        }"], ["grid_tree = GridSearchCV(dcg, parameters, cv=10, scoring='f1', verbose = 1, n_jobs = -1)\n", "grid_tree.fit(X_train,y_train)"], ["print(grid_tree.best_score_)\n", "print(grid_tree.best_params_)\n", "print(grid_tree.best_estimator_)"], ["y_pred_test = grid_tree.best_estimator_.predict(X_test)\n", "y_pred_train = grid_tree.best_estimator_.predict(X_train)\n", "dcg_train = metrics.f1_score(y_train, y_pred_train)\n", "dcg = metrics.f1_score(y_test, y_pred_test)\n", "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_test))\n", "print(\"F1 train:\",dcg_train)\n", "print(\"F1 test:\",dcg)"], ["scores['Decision tree with grid search'] = [0.5563545515859067, 0.5578221601755036, 0.7107176141658901]"], ["scores"], ["### XG Boost"], ["xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n", "                           colsample_bytree = 0.5, \n", "                           subsample = 0.5,\n", "                           learning_rate = 0.1,\n", "                           max_depth = 4, \n", "                           alpha = 1, \n", "                           n_estimators = 1000)\n", "xg_clf.fit(X_train,y_train)"], ["preds_train = xg_clf.predict(X_train)\n", "preds_test = xg_clf.predict(X_test)\n", "\n", "xg_f1 = metrics.f1_score(y_test, preds_test)\n", "xg_f1_train = metrics.f1_score(y_train, preds_train)\n", "xg_acc = metrics.accuracy_score(y_test, preds_test)\n", "\n", "print(\"Accuracy: %f\" % (xg_acc))\n", "print(\"F1 Test: %f\" % (xg_f1))\n", "print(\"F1 Train: %f\" % (xg_f1))"], ["xg_f1_train"], ["scores['XG Boost'] = [xg_f1_train, xg_f1, xg_acc]\n", "scores"], ["### XG Boost with Grid Search"], ["clf_xgb = xgb.XGBClassifier(objective = \"binary:logistic\")\n", "param_dist = {'max_depth':[2,3],\n", "              'eta':[0.1,0.2,],\n", "              'n_estimators':[100, 200],\n", "              'colsample_bytree':[ 0.6, 0.7],\n", "              'learning_rate': [0.1, 0.2, 0.5]\n", "             }"], ["gsearch1 = GridSearchCV(\n", "    estimator = clf_xgb,\n", "    param_grid = param_dist, \n", "    scoring='f1',\n", "    n_jobs=-1,\n", "    verbose=1,\n", "    iid=False, \n", "    cv=5)\n", "gsearch1.fit(X_train,y_train)"], ["gsearch1.best_params_"], ["gsearch1.best_score_"], ["x = xgb.XGBClassifier(objective = \"binary:logistic\", colsample_bytree = 0.6, eta = 0.1, learning_rate = 0.5,\n", "                     max_depth = 3, n_estimators = 200)\n", "x.fit(X_train,y_train)"], ["preds_train = x.predict(X_train)\n", "preds_test = x.predict(X_test)\n", "\n", "x_f1 = metrics.f1_score(y_test, preds_test)\n", "x_f1_train = metrics.f1_score(y_train, preds_train)\n", "x_acc = metrics.accuracy_score(y_test, preds_test)\n", "\n", "print(\"Accuracy: %f\" % (x_acc))\n", "print(\"F1 Test: %f\" % (x_f1_train))\n", "print(\"F1 Train: %f\" % (x_f1))"], ["scores['XG Boost with Grid Search'] = [x_f1_train, x_f1, x_acc]"], ["scores"], ["### Logistic Regression with Grid Search"], ["params = { \n", "    'C': [0.01,0.1,1],\n", "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n", "    'max_iter': [100,200,300,500]}\n"], ["lr_clf_grid = LogisticRegression(solver='liblinear', class_weight = 'balanced')\n", "\n", "grid_search_lr = GridSearchCV(estimator = lr_clf_grid, param_grid = params, scoring='f1', cv=10, n_jobs=-1, verbose = 1)\n", "grid_search_lr = grid_search_lr.fit(X_train,y_train)\n"], ["grid_search_lr.best_score_"], ["grid_search_lr.best_params_"], ["l = LogisticRegression(solver='liblinear', class_weight = 'balanced',C = 1, max_iter = 100, penalty = 'l1')\n", "l.fit(X_train, y_train)\n", "train = l.predict(X_train)\n", "test = l.predict(X_test)\n", "\n", "tra = metrics.f1_score(y_train, train)\n", "tes = metrics.f1_score(y_test, test)\n", "acc = metrics.accuracy_score(y_test, test)\n", "print('Test Accuracy score: ', acc)\n", "print('Train F1 score: ', tra)\n", "print('Test F1 score: ', tes)\n"], ["scores['Logistic Regression with Grid Search'] = [tra, tes, acc]"], ["scores"], ["### Voting Classifier"], ["X_train, X_test, y_train, y_test = train_test_split(df_features, target, random_state=1)\n", "scaler = StandardScaler()"], ["lrv = LogisticRegression(solver='liblinear', class_weight = 'balanced')\n", "dtv = DecisionTreeClassifier(class_weight='balanced')\n", "rfv = RandomForestClassifier(class_weight = 'balanced',random_state = 1, n_estimators = 500, max_depth=2, n_jobs = -1, verbose = 1)\n"], ["voting_clf = VotingClassifier(\n", "                estimators=[('Logistic Regression', lrv), ('Decision tree', dtv), ('Random Forest', rfv)], \n", "                voting='hard', n_jobs = -1, verbose = 1 )\n", "\n", "voting_clf.fit(X_train, y_train)"], ["vc_preds_train = voting_clf.predict(X_train)\n", "vc_preds_test = voting_clf.predict(X_test)"], ["vc_f1_train = metrics.f1_score(y_train, vc_preds_train)\n", "vc_f1_test = metrics.f1_score(y_test, vc_preds_test)\n", "accuracy = metrics.accuracy_score(y_test, vc_preds_test)"], ["print('f1 train: ', vc_f1_train )\n", "print('f1 test: ', vc_f1_test )\n", "print('Accuracy: ', accuracy )"], ["scores['Voting Classifier'] = [vc_f1_train, vc_f1_test, accuracy]\n", "scores"], ["X_train, X_test, y_train, y_test = train_test_split(df_features, target, random_state=1)\n", "scaler = StandardScaler()\n", "\n", "knn = KNeighborsClassifier(n_neighbors=5, n_jobs = -1)\n", "\n", "knn.fit(X_train, y_train)\n", "knn_pred_train = knn.predict(X_train)\n", "knn_pred_test = knn.predict(X_test)"], ["knn_f1_train = metrics.f1_score(y_train,knn_pred_train)\n", "knn_f1_test = metrics.f1_score(y_test,knn_pred_test)\n", "knn_f1_acc = metrics.accuracy_score(y_test,knn_pred_test)"], ["print('KNN f1 train', knn_f1_train)\n", "print('KNN f1 test', knn_f1_test)\n", "print('KNN f1 accuracy', knn_f1_acc)"], ["# Ran on different computer\n", "#f1_scores['Bagging Classifier for Logistic'] = 0.36281786105365615"], ["s = pd.DataFrame.from_dict(scores, orient='index',columns = ['Train F1', 'Test F1', 'Accuracy'])"], ["s.index.name = 'Models'"], ["s.sort_values(by = 'Test F1', ascending = False)"], ["f1_scores"], ["#rf_coef[0:10] #random forest"], ["dt_coef[0:15]"], ["log_bal[0:15]"], ["common = []\n", "for i in dt_coef[0:30]:\n", "    for j in log_bal[0:30]:\n", "        if i[0] == j[0]:\n", "            common.append(i)"], ["common"], ["## Running Logistic Regression with class weight balanced on complete dataset"], ["scaler = StandardScaler() \n", "scaler.fit(df_features)"], ["final = LogisticRegression(solver='liblinear', class_weight = 'balanced', n_jobs = -1, verbose = 1)\n", "final.fit(df_features, target)\n", "\n", "final_predictions = final.predict(df_features)\n", "final_f1 = metrics.f1_score(target, final_predictions)\n", "print('Final Accuracy score: ', metrics.accuracy_score(target, final_predictions))\n", "\n", "print('Final F1 score: ', final_f1)"], ["metrics.confusion_matrix(target, final_predictions)"], ["plot_confusion_matrix(final, df_features, target, cmap=plt.cm.Blues)\n", "plt.show()"], ["## Conclusion"], ["Our final model gave us an F1 score of .57 on our final data set. The reason why we chose F1 score over accuracy and other evaluation metrics is because F1 score describes our predictions in a better way. While accuracy is used to emphasize the true positives and true negatives, F1-score is used to show when the false positives and false negatives are crucial. Additionally, F1-score is used when the classes are not balanced and in our case there is a high class imbalance with 87-13"], ["### Using a Logistic Regression Classifier we can predict with an f1 score of 0.57, whether a car accident will be severe."], ["Our final models show that certain columns have more of an effect on the severity of a car accident than others. For example, accidents where pedestrians are involved have are more likely to be severe than accidents where there is just a side swipe. A few examples of this is accidents with pedestrians, cyclists or where the driver is going above the authorized speed limit. An example of a feature where there is a negative correlation with having a severe accident is when there is either a same direction or opposite direction sideswipe.\n", "\n", "One application where this prediction and our model could be considered is with the addition of self driving cars. When an accident occurs, the car will know and immediately notify the EMTs. This notification can also include what features were involved in the accident (i.e. pedestrian, sideswipe, speeding etc.)  and be able to conclude whether or not this accident is severe and how much attention it needs."]]}], "kiranreddy4/Accidental_analaysis": [{"Analysis_of_Accidents_in_2017.ipynb": []}], "rsinclairchin/aviation_accidents": [{"Airline Safety Crashes.ipynb": [["# Aviation Accidents"], ["## Import Libraries"], ["import scipy\n", "import numpy as np\n", "import matplotlib \n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import sklearn\n", "import seaborn as sns\n", "sns.set_style(\"whitegrid\")\n", "%matplotlib inline"], ["## Data Dictionary"], [], ["## Exploratory Data Analysis"], ["Read in the data..."], ["data = pd.read_csv(\"table10_2014.csv\")\n", "print(data.index)\n", "print(data.dtypes)\n", "print(data.shape)"], ["Take a look..."], ["print(\"First 20 rows:\")\n", "print(data.head(20))"], ["print(\"Last 20 rows:\")\n", "print(data.tail(20))"], ["So apparently this dataset has notes at the bottom of it as well as headers and subheaders. First I'm going to trim those off, and then I'll add in my own headers."], ["data = data[4:-21]\n", "data.head()"], ["headers = ['Year', 'All Accidents', 'Fatal Accidents', 'Total Fatalities', 'Fatalities Aboard', 'Flight Hours', 'All Accidents Per 100,000 Flight Hours', 'Fatal Accidents Per 100,000 Flight Hours', '', '']\n", "# So it turns out there's two sneaky columns in here that're totally empty.\n", "# I'm going to give them empty headers for now, and delete them when I'm done with headers.\n", "data.columns = headers\n", "data.head()"], ["data = data.drop(columns=[''])"], ["Yay! Looks like this dataset is finally in good shape!!!"], ["## Summary Statistics "], ["data.describe()"], ["# Find the percentage of accidents that are fatal\n", "# fatal_percentage = float(data['All Accidents'].sum()) / float(data['Fatal Accidents'].sum())\n", "# print('%0.f percent of accidents were fatal'%fatal_percentage)\n", "data['All Accidents'].sum()"], ["In trying to find what percentage of accidents were fatal, I discovered that the values in this dataset have commas. I will need to take out the commas and try again."], ["data['All Accidents'] = data['All Accidents'].str.replace(',', '')\n", "data['All Accidents'] = data['All Accidents'].astype(int)\n", "\n", "data['Fatal Accidents'] = data['Fatal Accidents'].str.replace(',', '')\n", "data['Fatal Accidents'] = data['Fatal Accidents'].astype(int)\n", "\n", "# be as well doing this for all columns, will need them at some point.\n", "data['Flight Hours'] = data['Flight Hours'].str.replace(',', '')\n", "data['Flight Hours'] = data['Flight Hours'].str.replace(' - ', '')\n", "data.loc[data['Flight Hours'] == '']\n", "data = data.drop(data[data['Flight Hours'] == ''].index)\n", "data['Flight Hours'] = data['Flight Hours'].astype(int)\n", "\n", "data['Total Fatalities'] = data['Total Fatalities'].str.replace(',', '')\n", "data['Total Fatalities'] = data['Total Fatalities'].astype(int)\n", "\n", "data['Fatalities Aboard'] = data['Fatalities Aboard'].str.replace(',', '')\n", "data['Fatalities Aboard'] = data['Fatalities Aboard'].astype(int)\n", "\n", "data['All Accidents Per 100,000 Flight Hours'] = data['All Accidents Per 100,000 Flight Hours'].str.replace(',', '')\n", "data['All Accidents Per 100,000 Flight Hours'] = data['All Accidents Per 100,000 Flight Hours'].astype(float)\n", "\n", "data['Fatal Accidents Per 100,000 Flight Hours'] = data['Fatal Accidents Per 100,000 Flight Hours'].str.replace(',', '')\n", "data['Fatal Accidents Per 100,000 Flight Hours'] = data['Fatal Accidents Per 100,000 Flight Hours'].astype(float)"], ["# Find the percentage of accidents that are fatal\n", "fatal_percentage = float(data['All Accidents'].sum()) / float(data['Fatal Accidents'].sum())\n", "print('%0.f percent of accidents were fatal'%fatal_percentage)"], ["## Feature Engineering"], ["I don't actually have any columns I'd like to feature engineer right now."], ["## Plots And Graphs"], ["data.columns"], ["data.hist(bins = 10, figsize=(14, 10))\n", "plt.show()"], ["data.plot(kind='density', figsize=(20, 35), layout=(10,4), subplots=True, sharex=False) \n", "plt.show()"], ["data.plot(kind='box', subplots=True, figsize=(20, 35), layout=(7,6), sharex=False, sharey=False) \n", "plt.show()"], ["data.corr()"], ["corr = data.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\n", "plt.figure(figsize=(12, 12))\n", "sns.heatmap(corr, vmax=1, square=True)"], ["sns.regplot(x = 'Fatal Accidents', y = 'Flight Hours', data = data, color = 'Orange');"], ["## Build A Model"], ["import statsmodels.api as sm\n", "\n", "lm = LogisticRegression()\n", "\n", "X = data[['', '', '', '', '']]\n", "X = sm.add_constant(X) # what is this\n", "y = data['']\n", "\n", "lm = sm.Logit(y, X)\n", "result = lm.fit()\n", "\n", "result.summary()\n", "\n", "`````````````````````````````\n", "\n", "from sklearn.tree import DecisionTreeClassifier\n", "\n", "model = DecisionTreeClassifier()\n", "\n", "X = data[['', '', '', '', '']]\n", "y = X['']\n", "X.drop('', axis=1, inplace=True)\n", "\n", "model.fit(X, y)\n", "\n", "``````````````````````````````\n", "\n"], ["## Conclusion"], []]}], "zehranuralkan/Traffic_Accident_Detection": [{"ANimatedAccidentData_Test.ipynb": [["import numpy as np\n", "import pandas as pd \n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.keras import layers\n", "from time import perf_counter \n", "import os\n", "import joblib\n", "from keras.models import load_model\n", "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n"], ["batch_size = 64\n", "img_height = 250\n", "img_width = 250\n", "\n"], ["training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Animated_accident_data',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=batch_size\n", "\n", ")"], ["testing_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/test',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=338\n", "\n", ")"], ["model=load_model('/home/zehra/Desktop/Animated_accident_data/Model17.h5')\n", " \n"], ["AccuracyVector = []\n", "plt.figure(figsize=(30, 30))\n", "for images, labels in testing_ds.take(1):\n", "    predictions = model.predict(images)\n", "    predlabel = []\n", "    prdlbl = []\n", "    \n", "class_names = training_ds.class_names\n", "\n", "for mem in predictions:\n", "        deneme=predlabel.append(class_names[np.argmax(mem)])\n", "        print(class_names[np.argmax(mem)])\n", "        deneme=prdlbl.append(np.argmax(mem))\n", "    \n", "    \n", "AccuracyVector = np.array(prdlbl) == labels\n", "print(\"AccuracyVector: \", AccuracyVector)\n", "   \n", "sayac=0\n", "for values in AccuracyVector:\n", "    if(values==True):\n", "        sayac=sayac+1\n", "testAccuarcy=sayac/44\n", "print(\"Test Accuarcy: \", testAccuarcy)"], []]}, {"AnimatedAccidentData_Train.ipynb": [["import numpy as np\n", "import pandas as pd \n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.keras import layers\n", "from time import perf_counter \n", "import os"], ["## Defining batch specfications\n", "batch_size = 64\n", "img_height = 250\n", "img_width = 250"], ["training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Animated_accident_data',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=batch_size\n", "\n", ")"], ["class_names = training_ds.class_names\n", "\n", "## Configuring dataset for performance\n", "AUTOTUNE = tf.data.experimental.AUTOTUNE\n", "training_ds = training_ds.cache().prefetch(buffer_size=AUTOTUNE)"], ["MyCnn = tf.keras.models.Sequential([\n", "  layers.BatchNormalization(),\n", "  layers.Conv2D(32, 3, activation='relu'),\n", "  layers.MaxPooling2D(),\n", "  layers.Conv2D(64, 3, activation='relu'),\n", "  layers.MaxPooling2D(),\n", "  layers.Conv2D(128, 3, activation='relu'),\n", "  layers.MaxPooling2D(),\n", "  layers.Flatten(),\n", "  layers.Dense(256, activation='relu'),\n", "  layers.Dense(len(class_names), activation= 'softmax')\n", "])\n", "\n", "MyCnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"], ["retVal = MyCnn.fit(training_ds, validation_data= training_ds, epochs = 100)"], ["plt.plot(retVal.history['loss'], label = 'training loss')\n", "plt.plot(retVal.history['accuracy'], label = 'training accuracy')\n", "plt.grid(True)\n", "plt.legend()\n", "\n", "## stats on validation data\n", "plt.plot(retVal.history['val_loss'], label = 'validation loss')\n", "plt.plot(retVal.history['val_accuracy'], label = 'validation accuracy')\n", "plt.grid(True)\n", "plt.legend()\n"], ["testing_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Animated_accident_data',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=338\n", "\n", ")"], [" AccuracyVector = []\n", "plt.figure(figsize=(30, 30))\n", "for images, labels in testing_ds.take(1):\n", "    predictions = MyCnn.predict(images)\n", "    predlabel = []\n", "    prdlbl = []\n", "    \n", "      "], [" for mem in predictions:\n", "        deneme=predlabel.append(class_names[np.argmax(mem)])\n", "        print(class_names[np.argmax(mem)])\n", "        deneme=prdlbl.append(np.argmax(mem))"], ["   \n", "    AccuracyVector = np.array(prdlbl) == labels\n", "    print(\"AccuracyVector: \", AccuracyVector)\n", "    print(\"Type: \", type(AccuracyVector))\n", "    print(\"prdlbl: \", prdlbl)\n", "    print(\"labels: \", labels)"], ["sayac=0\n", "for values in AccuracyVector:\n", "    if(values==True):\n", "        sayac=sayac+1\n", "testAccuarcy=sayac/338\n", "print(\"Test Accuarcy: \", testAccuarcy)"], ["MyCnn.save('/home/zehra/Desktop/Animated_accident_data/Model17.h5')  "], []]}, {"Cnn_Model.ipynb": [["import numpy as np\n", "import pandas as pd \n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.keras import layers\n", "from time import perf_counter \n", "import os"], ["batch_size = 200\n", "img_height = 250\n", "img_width = 250"], ["training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Deneme/Dataset/data/train',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=batch_size\n", "\n", ")"], ["validation_ds =  tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Deneme/Dataset/data/val',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=batch_size)\n", "\n"], ["testing_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Deneme/Dataset/data/test',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=batch_size)\n"], ["class_names = training_ds.class_names"], ["AUTOTUNE = tf.data.experimental.AUTOTUNE\n", "training_ds = training_ds.cache().prefetch(buffer_size=AUTOTUNE)\n", "testing_ds = testing_ds.cache().prefetch(buffer_size=AUTOTUNE)\n", "MyCnn = tf.keras.models.Sequential([\n", "layers.BatchNormalization(),\n", "layers.Conv2D(32, 3, activation='relu'),\n", "layers.MaxPooling2D(),\n", "layers.Conv2D(64, 3, activation='relu'),\n", "layers.MaxPooling2D(),\n", "layers.Conv2D(128, 3, activation='relu'),\n", "layers.MaxPooling2D(),\n", "layers.Flatten(),\n", "layers.Dense(256, activation='relu'),\n", "layers.Dense(len(class_names), activation= 'softmax')\n", "])\n"], ["MyCnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n", "# lets train our CNN\n", "retVal = MyCnn.fit(training_ds, validation_data= validation_ds, epochs =100)\n", "\n", "\n", "plt.plot(retVal.history['loss'], label = 'training loss')\n", "plt.plot(retVal.history['accuracy'], label = 'training accuracy')\n", "plt.grid(True)\n", "plt.legend()\n", "\n", "## stats on validation data\n", "plt.plot(retVal.history['val_loss'], label = 'validation loss')\n", "plt.plot(retVal.history['val_accuracy'], label = 'validation accuracy')\n", "plt.grid(True)\n", "plt.legend()"], ["AccuracyVector = []\n", "plt.figure(figsize=(30, 30))\n", "for images, labels in testing_ds.take(1):\n", "    predictions = MyCnn.predict(images)\n", "    predlabel = []\n", "    prdlbl = []\n", "    \n", "   "], [" for mem in predictions:\n", "        deneme=predlabel.append(class_names[np.argmax(mem)])\n", "        print(class_names[np.argmax(mem)])\n", "        deneme=prdlbl.append(np.argmax(mem))\n", "      \n", "    \n"], [" AccuracyVector = np.array(prdlbl) == labels\n", "print(\"AccuracyVector: \", AccuracyVector)\n", "   "], ["sayac=0\n", "for values in AccuracyVector:\n", "    if(values==True):\n", "        sayac=sayac+1\n", "testAccuarcy=sayac/78\n", "print(\"Test Accuarcy: \", testAccuarcy)"], ["sayac=0\n", "for values in AccuracyVector:\n", "    if(values==True):\n", "        sayac=sayac+1\n", "testAccuarcy=sayac/78\n", "print(\"Test Accuarcy: \", testAccuarcy)"], [], []]}, {"Load_Cnn.ipynb": [["import numpy as np\n", "import pandas as pd \n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.keras import layers\n", "from time import perf_counter \n", "import os\n", "import joblib\n", "from keras.models import load_model"], ["batch_size = 64\n", "img_height = 250\n", "img_width = 250"], ["training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Deneme/Dataset/data/train',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=batch_size\n", "\n", ")\n", "validation_ds =  tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Deneme/Dataset/data/val',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=batch_size)\n", "\n", "testing_ds = tf.keras.preprocessing.image_dataset_from_directory(\n", "    '/home/zehra/Desktop/Deneme/Dataset/data/test',\n", "    seed=42,\n", "    image_size= (img_height, img_width),\n", "    batch_size=100)\n"], ["model=load_model('/home/zehra/Desktop/Deneme/M8.h5')\n", " "], ["AccuracyVector = []\n", "plt.figure(figsize=(30, 30))\n", "for images, labels in testing_ds.take(1):\n", "    predictions = model.predict(images)\n", "    predlabel = []\n", "    prdlbl = []"], ["class_names = training_ds.class_names\n"], ["for mem in predictions:\n", "       deneme=predlabel.append(class_names[np.argmax(mem)])\n", "       print(class_names[np.argmax(mem)])\n", "       deneme=prdlbl.append(np.argmax(mem))\n"], ["AccuracyVector = np.array(prdlbl) == labels\n", "print(\"AccuracyVector: \", AccuracyVector)\n", "   "], ["sayac=0\n", "for values in AccuracyVector:\n", "    if(values==True):\n", "        sayac=sayac+1\n", "testAccuarcy=sayac/78\n", "print(\"Test Accuarcy: \", testAccuarcy)"], [], []]}], "entrepreneur-interet-general/predisauvetage": [{"parsing_sitrep.ipynb": [["import mailparser\n", "import re\n", "import pandas as pd\n", "import io\n", "import glob\n", "import numpy as np"], ["\n", "class EmlFileParser(object):\n", "    def __init__(self, file):\n", "        super().__init__()\n", "        self.file = file   \n", "\n", "    def parse_metropolitaine(self):\n", "        df = self.parse()\n", "        df = df.filter(regex='SITREP|^J\\s|^L\\s|^N\\s')\n", "        df.iloc[:,0] =df.columns[0]\n", "        df = df.rename(columns={df.columns[0]: \"cross_sitrep\"})\n", "        df['cross_sitrep'] = df.cross_sitrep.str.replace('=C9', \"E\").str.extract(\"(?:MRCC|CROSS|JRCC) ((?:LA*\\s)*[A-Z]+\\/\\d+/(?:N\u00b0)*\\d+)\")\n", "        df = self.colname_homogeneous(df)\n", "        return df\n", "  \n", "    def parse_tahiti(self):\n", "        df = self.parse()\n", "        df = df.filter(regex='BT|JRCC|^J-|^L-|^N-')\n", "        df.iloc[:,0] =df.columns[0]\n", "        df = df.rename(columns={df.columns[0]: \"cross_sitrep\"})\n", "        df['cross_sitrep'] = \"TAHITI/\"+df['BT'].str.extract('(\\d+/\\d+)')\n", "        df = df.drop(['BT', 'JRCC TAHITI'], axis=1)\n", "        df = self.colname_homogeneous(df)\n", "        return df\n", "    \n", "    def parse_reunion(self):\n", "        df = self.parse()\n", "        df = df.filter(regex='FM CROSS REUNION|^J-|^L-|^N-')\n", "        df.iloc[:,0] =df.columns[0]\n", "        df['cross_sitrep'] = df['FM CROSS REUNION'].str.extract('(?:CROSS) (R=C9UNION/\\d+/\\d+)').str.replace('=C9', 'E')\n", "        df = df.drop('FM CROSS REUNION', axis=1)\n", "        df = self.colname_homogeneous(df)\n", "        return df\n", "        \n", "    def colname_homogeneous(self, df):\n", "        di={'J - INITIAL ACTIONS TAKEN': 'J - PREMIERES MESURES PRISES',\n", "           'J - PREMI\u00c8RES MESURES PRISES':'J - PREMIERES MESURES PRISES',\n", "           'J \u2013 PREMIERES MESURES PRISES':'J - PREMIERES MESURES PRISES',\n", "           'J-    PREMIERES MESURES PRISES':'J - PREMIERES MESURES PRISES',\n", "           'L - CHRONOLOGIE':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'L - COORDINATING INSTRUCTIONS':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'L-    INSTRUCTIONS POUR LA COORDINATION':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'L \u2013 CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'N - ADDITIONAL INFORMATION':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N - RENSEIGNEMENTS COMPL\u00c9MENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N - RENSEIGNEMENTS COMPLMENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N \u2013 RENSEIGNEMENTS COMPLEMENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N-    RENSEIGNEMENTS COMPLEMENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES'}\n", "        return df.rename(columns=lambda x: x.strip()).rename(columns=di)\n", "      \n", "    def parse(self):\n", "        data = self.text_to_df()\n", "        data.columns =['lines'] #Name the dataframe column\n", "        data['A'], data['B'] = data['lines'].str.split('\\n',1).str #Split lines \n", "        data = data.drop_duplicates('A', keep='first') #Drop duplicated lines\n", "        data = data.transpose()# Transpose the Data frame \n", "        data.columns = data.iloc[1] #Name colums with info in row A\n", "        data = data.reindex(data.index.drop(['A','lines'])).reset_index() \n", "        return data\n", "    \n", "    def eml_to_text(self):\n", "        mail = mailparser.parse_from_file(self.file)\n", "        return mail.body\n", "\n", "    def text_to_df(self):\n", "        a =  self.eml_to_text().split('\\n\\n')\n", "        return pd.DataFrame(a)\n", "\n", "\n", "\n", "class EmlFolderParser(object):\n", "    def __init__(self, folder, output):\n", "        super().__init__()\n", "        self.folder = folder\n", "        self.output = output\n", "    \n", "    def parse_folder(self):\n", "        text_df = pd.DataFrame()\n", "        for filename in glob.glob(self.folder+'/*.eml'):\n", "            if ('TAHITI' in filename)== True:            \n", "                out = EmlFileParser(filename).parse_tahiti()\n", "            elif ('UNION' in filename)== True:\n", "                out = EmlFileParser(filename).parse_reunion()\n", "            else:\n", "                out = EmlFileParser(filename).parse_metropolitaine()\n", "            text_df = text_df.append(out, ignore_index=True)\n", "        text_df.to_csv(self.output)\n", "    "], ["#EmlFolderParser('./liste_sitrep', './liste_sitrep/test_all3.csv').parse_folder()"], ["def parse_sitrep(input_path, output_path):\n", "    text_df = pd.DataFrame() \n", "    for filename in glob.glob(input_path+'/*.eml'):\n", "        mail = mailparser.parse_from_file(filename)\n", "        text = mail.body\n", "        a = text.split('\\n\\n')\n", "        df = pd.DataFrame(a)\n", "        df.columns =['test']\n", "        df['A'], df['B'] = df['test'].str.split('\\n',1).str \n", "        df = df.drop_duplicates('A', keep='first')\n", "        df1 = df.transpose()\n", "        df1.columns = df1.iloc[1]\n", "        df2 = df1.reindex(df1.index.drop('A')).reset_index()\n", "        df3 = df2.filter(regex='SITREP|^J\\s|^L\\s|^N\\s').drop(0)\n", "        df3.iloc[:,0] =df3.columns[0]\n", "        df3 = df3.rename(columns={ df3.columns[0]: \"cross_sitrep\"})\n", "        df3['cross_sitrep'] = df3.cross_sitrep.str.extract(\"(?:MRCC|CROSS|JRCC) ([A-Z]+\\/\\d+/(?:N\u00b0)*\\d+)\")#ajouter un truc pour La garde \n", "        text_df = text_df.append(df3, ignore_index=True)\n", "    text_df.to_csv(output_path)\n", "\n", "\n", "    "], ["parse_sitrep('./liste_sitrep', './liste_sitrep/test.csv')"], ["class EmlFileParserTest(object):\n", "    def __init__(self, file):\n", "        super().__init__()\n", "        self.file = file   \n", "\n", "    def parse_metropolitaine(self):\n", "        df = self.parse()\n", "        df = df.filter(regex='SITREP|^J\\s|^L\\s|^N\\s')\n", "        df.iloc[:,0] =df.columns[0]\n", "        df = df.rename(columns={df.columns[0]: \"cross_sitrep\"})\n", "        df['cross_sitrep'] = df.cross_sitrep.str.replace('=C9', \"E\").str.extract(\"(?:MRCC|CROSS|JRCC) ((?:LA*\\s)*[A-Z]+\\/\\d+/(?:N\u00b0)*\\d+)\")\n", "        df = self.colname_homogeneous(df)\n", "        return df\n", "  \n", "    def parse_tahiti(self):\n", "        df = self.parse()\n", "        df = df.filter(regex='|BT|JRCC|^J-|^L-|^N-')\n", "      #  df.iloc[:,0] =df.columns[0]\n", "       # df = df.rename(columns={df.columns[0]: \"cross_sitrep\"})\n", "       # df['cross_sitrep'] = \"TAHITI/\"+df['BT'].str.extract('(\\d+/\\d+)')\n", "       # df = df.drop(['BT', 'JRCC TAHITI'], axis=1)\n", "        #df = self.colname_homogeneous(df)\n", "        return df\n", "    \n", "    def parse_reunion(self):\n", "        df = self.parse()\n", "        df = df.filter(regex='FM CROSS REUNION|^J-|^L-|^N-')\n", "        df.iloc[:,0] =df.columns[0]\n", "        df['cross_sitrep'] = df['FM CROSS REUNION'].str.extract('(?:CROSS) (R=C9UNION/\\d+/\\d+)').str.replace('=C9', 'E')\n", "        df = df.drop('FM CROSS REUNION', axis=1)\n", "        df = self.colname_homogeneous(df)\n", "        return df\n", "        \n", "    def colname_homogeneous(self, df):\n", "        di={'J - INITIAL ACTIONS TAKEN': 'J - PREMIERES MESURES PRISES',\n", "           'J - PREMI\u00c8RES MESURES PRISES':'J - PREMIERES MESURES PRISES',\n", "           'J \u2013 PREMIERES MESURES PRISES':'J - PREMIERES MESURES PRISES',\n", "           'J-    PREMIERES MESURES PRISES':'J - PREMIERES MESURES PRISES',\n", "           'L - CHRONOLOGIE':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'L - COORDINATING INSTRUCTIONS':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'L-    INSTRUCTIONS POUR LA COORDINATION':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'L \u2013 CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION':'L - CHRONOLOGIE ET INSTRUCTIONS DE COORDINATION',\n", "           'N - ADDITIONAL INFORMATION':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N - RENSEIGNEMENTS COMPL\u00c9MENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N - RENSEIGNEMENTS COMPLMENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N \u2013 RENSEIGNEMENTS COMPLEMENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES',\n", "           'N-    RENSEIGNEMENTS COMPLEMENTAIRES':'N - RENSEIGNEMENTS COMPLEMENTAIRES'}\n", "        return df.rename(columns=lambda x: x.strip()).rename(columns=di)\n", "      \n", "    def parse(self):\n", "        data = self.text_to_df()\n", "        data.columns =['lines'] #Name the dataframe column\n", "        data['A'], data['B'] = data['lines'].str.split('\\n',1).str #Split lines \n", "        data = data.drop_duplicates('A', keep='first') #Drop duplicated lines\n", "        data = data.transpose()# Transpose the Data frame \n", "        data.columns = data.iloc[1] #Name colums with info in row A\n", "        data = data.reindex(data.index.drop(['A','lines'])).reset_index() \n", "        return data\n", "    \n", "    def eml_to_text(self):\n", "        mail = mailparser.parse_from_file(self.file)\n", "        return mail.body\n", "\n", "    def text_to_df(self):\n", "        a =  self.eml_to_text().split('\\n\\n')\n", "        return pd.DataFrame(a)\n", "\n", "\n", "\n", "class EmlFolderParser(object):\n", "    def __init__(self, folder, output):\n", "        super().__init__()\n", "        self.folder = folder\n", "        self.output = output\n", "    \n", "    def parse_folder(self):\n", "        text_df = pd.DataFrame()\n", "        for filename in glob.glob(self.folder+'/*.eml'):\n", "            if ('TAHITI' in filename)== True:            \n", "                out = EmlFileParser(filename).parse_tahiti()\n", "            elif ('UNION' in filename)== True:\n", "                out = EmlFileParser(filename).parse_reunion()\n", "            else:\n", "                out = EmlFileParser(filename).parse_metropolitaine()\n", "            text_df = text_df.append(out, ignore_index=True)\n", "        text_df.to_csv(self.output)"], ["EmlFileParserTest('./liste_sitrep/JRCC TAHITI - SITREP SAR 406-2017- UN ET FINAL.eml').parse_tahiti()"], ["#Handle cross reunion\n", "#Add message error\n", "#Split parsing function\n", "#make test"]]}, {"Sauvamer_cleaning.ipynb": []}], "thmegy/AccidentsDeLaRoute": [{"accidents_route.ipynb": []}, {"population_density.ipynb": [["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import matplotlib.dates\n", "import numpy as np\n", "import json\n", "import geopandas as gpd\n", "import unidecode\n", "import osmnx as ox\n", "import matplotlib as mpl\n", "import os\n", "\n", "plt.rcParams['font.size'] = 16\n", "plt.rcParams['lines.linewidth'] = 2"], ["shapefile = gpd.read_file('input/JRC_GRID_2018/JRC_POPULATION_2018.shp')"], ["shapefile[shapefile['Country'] == 'FR'].plot()"], ["dico_communes = json.load(open('input/dico_communes_insee.json', 'r'))"], ["communes = [\n", "     'Clermont-Ferrand',\n", "#      'Aulnat',\n", "      'Aubi\u00e8re',\n", "#       'C\u00e9bazat',\n", "       'Chamali\u00e8res',\n", "       'Beaumont, Puy-de-dome',\n", "#       'Cournon d\\'Auvergne',\n", "#       'Royat',\n", "#       'Le Cendre',\n", "#       'Orcines',\n", "#       'Lempdes',\n", "#       'Pont-du-chateau',\n", "#       'Durtol',\n", "#       'Nohanent',\n", "#       'Blanzat',\n", "#       'Ch\u00e2teaugay',\n", "#       'Gerzat',\n", "#       'Romagnat',\n", "#       'P\u00e9rignat-l\u00e8s-Sarli\u00e8ve',\n", "#       'Saint-G\u00e8nes-Champanelle',\n", "#       'Ceyrat'\n", "]\n", "\n", "communes_insee = [dico_communes[unidecode.unidecode(c.split(',')[0]).upper()] for c in communes]\n", "\n", "limites = ox.geocode_to_gdf(communes)\n", "limites.to_crs(crs=3035, inplace=True)"], ["routes_main = gpd.GeoDataFrame()\n", "routes_other = gpd.GeoDataFrame()\n", "\n", "for com, insee in zip(communes, communes_insee):\n", "    print(com, insee)\n", "    filename = f'input/routes/{insee}'\n", "    if os.path.isfile(filename):\n", "        r_main = gpd.read_file(f'{filename}_main.geojson')\n", "        r_main.to_crs(crs=3035, inplace=True)\n", "        \n", "        r_other = gpd.read_file(f'{filename}_other.geojson')\n", "        r_other.to_crs(crs=3035, inplace=True)\n", "        \n", "    else:\n", "        r = ox.geometries_from_place(com, tags={'highway':True})\n", "        mask_main = r.highway.isin(['primary', 'primary_link', 'secondary', 'secondary_link', 'motorway'])\n", "        mask_other = r.highway.isin(['pedestrian', 'tertiary', 'tertiary_link', 'residential'])\n", "        r_main = r[['geometry']][mask_main]\n", "        r_main.to_file(f'{filename}_main.geojson')\n", "        r_main.to_crs(crs=3035, inplace=True)\n", "        \n", "        r_other = r[['geometry']][mask_other]\n", "        r_other.to_file(f'{filename}_other.geojson')\n", "        r_other.to_crs(crs=3035, inplace=True)\n", "        \n", "    routes_main = routes_main.append(r_main)\n", "    routes_other = routes_other.append(r_other)"], ["shapefile_clipped = gpd.clip(shapefile, limites)"], ["bounds = np.linspace(0, 11000, 12)\n", "norm = mpl.colors.BoundaryNorm(bounds, 250)\n", "\n", "ax = shapefile_clipped.plot(cmap='YlOrBr', norm=norm, column=shapefile_clipped['TOT_P_2018'],\n", "                            legend=True, figsize=(22,15), legend_kwds={'fraction':0.025})\n", "limites.plot(ax=ax, facecolor='None', edgecolor='black', linewidth=3)\n", "routes_main.plot(ax=ax, color='gray', alpha=0.5, linewidth=3)\n", "routes_other.plot(ax=ax, color='gray', alpha=0.5, linewidth=1)\n", "\n", "ax.set_title('Densit\u00e9 de population (hab/km2) - Clermont-Fd')\n", "plt.axis('off')\n", "#plt.savefig('density_metro.jpg', dpi=300)\n", "plt.savefig('density_clermont.jpg', dpi=150)\n", "plt.show()\n"], ["plt.boxplot(shapefile_clipped['TOT_P_2018'])"], ["plt.hist(shapefile_clipped['TOT_P_2018'], bins=bounds)"]]}, {"schema_cyclable.ipynb": [["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import matplotlib.dates\n", "import numpy as np\n", "import json\n", "import geopandas as gpd\n", "from utils import getInputs, drawMap\n", "import unidecode\n", "import osmnx as ox\n", "import os\n", "\n", "plt.rcParams['font.size'] = 16\n", "plt.rcParams['lines.linewidth'] = 2\n", "# enable completion\n", "%config Completer.use_jedi = False"], ["df = gpd.read_file('/home/thmegy/T\u00e9l\u00e9chargements/R\u00e9seau_et_sch\u00e9ma_cyclable_m\u00e9tropolitain.geojson')"], ["df.columns"], ["df['amgt_exi'].unique()"], ["mask_amgt_propo = pd.notnull(df['amgt_propo'])\n", "mask_amgt_exi = pd.notnull(df['amgt_exi'])"], ["df_propo = df[mask_amgt_propo]\n", "df_exi = df[mask_amgt_exi]"], ["df_new['SHAPE__Length'].sum() / 1000"], ["df_exi['SHAPE__Length'].sum() / 1000"], ["piste = ['Piste cyclable bidirectionnelle', 'Piste cyclable bidirectionnelle ou voie verte',\n", "        'Piste cyclable monodirectionnelle', 'Voie verte', 'Passerelle', 'Piste cyclable',\n", "         'Am\u00e9nagement sur trottoir']\n", "partage = ['Zone 30 ou partage de voirie', 'Zone 30', 'Partage sans transit', 'Partage de voirie',\n", "           'Zone rencontre - Z30']\n", "bande = ['Bande cyclable', 'Bandes cyclables','Bande cyclable un c\u00f4t\u00e9', 'Voie centrale banalis\u00e9e']\n", "zone_rencontre = ['Zone de rencontre', 'Zone 20', 'Aire pi\u00e9tonne']\n", "autre = ['Encorbellement', 'Am\u00e9nagement mixte', 'Autre', 'Couloir bus/v\u00e9lo',\n", "        'Inconnu', 'A definir', 'Test', 'Franchissement']"], ["df_propo['amgt_propo'] = df_propo['amgt_propo'].replace(piste, 'Piste s\u00e9par\u00e9e')\n", "df_propo['amgt_propo'] = df_propo['amgt_propo'].replace(bande, 'Bande cyclable')\n", "df_propo['amgt_propo'] = df_propo['amgt_propo'].replace(partage, 'Partage de voirie')\n", "df_propo['amgt_propo'] = df_propo['amgt_propo'].replace(autre, 'Autres')\n", "df_propo['amgt_propo'] = df_propo['amgt_propo'].replace(zone_rencontre, 'Zone de rencontre')"], ["df_exi['amgt_exi'] = df_exi['amgt_exi'].replace(piste, 'Piste s\u00e9par\u00e9e')\n", "df_exi['amgt_exi'] = df_exi['amgt_exi'].replace(bande, 'Bande cyclable')\n", "df_exi['amgt_exi'] = df_exi['amgt_exi'].replace(partage, 'Partage de voirie')\n", "df_exi['amgt_exi'] = df_exi['amgt_exi'].replace(autre, 'Autres')\n", "df_exi['amgt_exi'] = df_exi['amgt_exi'].replace(zone_rencontre, 'Zone de rencontre')"], ["df_propo['amgt_propo'].unique()"], ["df_exi['amgt_exi'].unique()"], ["# Am\u00e9nagements propos\u00e9s"], ["amgt_norm_propo = df_propo.groupby(['amgt_propo'])['SHAPE__Length'].sum() * 100 / df_propo['SHAPE__Length'].sum()\n", "amgt_propo = df_propo.groupby(['amgt_propo'])['SHAPE__Length'].sum() / 1000\n", "print(amgt_propo)\n", "\n", "plt.figure(figsize=(12,10), facecolor='white')\n", "plt.xticks(rotation=30, ha='right')\n", "plt.ylabel('Longueur [km]')\n", "plt.bar(amgt_propo.index, amgt_propo)\n", "plt.show()"], ["fig1, ax1 = plt.subplots(facecolor='white')\n", "ax1.pie(amgt_norm_propo, labels=amgt_propo.index, autopct='%1.1f%%',\n", "        shadow=True, startangle=0)\n", "ax1.axis('equal')\n", "plt.show()"], ["# Am\u00e9nagements existants"], ["amgt_norm_exi = df_exi.groupby(['amgt_exi'])['SHAPE__Length'].sum() * 100 / df_exi['SHAPE__Length'].sum()\n", "amgt_exi = df_exi.groupby(['amgt_exi'])['SHAPE__Length'].sum() / 1000\n", "print(amgt_exi)\n", "\n", "plt.figure(figsize=(12,10))\n", "plt.xticks(rotation=30, ha='right')\n", "plt.bar(amgt_exi.index, amgt_exi)\n", "plt.show()"], ["fig1, ax1 = plt.subplots()\n", "ax1.pie(amgt_norm_exi, labels=amgt_exi.index, autopct='%1.1f%%',\n", "        shadow=True, startangle=0)\n", "ax1.axis('equal')\n", "plt.show()"], ["# Cartes"], ["communes = [\n", "    'Clermont-Ferrand',\n", "    'Aulnat',\n", "    'Aubi\u00e8re',\n", "    'C\u00e9bazat',\n", "    'Chamali\u00e8res',\n", "    'Beaumont, Puy-de-dome',\n", "    'Cournon d\\'Auvergne',\n", "    'Royat',\n", "    'Le Cendre',\n", "#    'Orcines',\n", "    'Lempdes',\n", "    'Pont-du-chateau',\n", "    'Durtol',\n", "    'Nohanent',\n", "    'Blanzat',\n", "    'Ch\u00e2teaugay',\n", "    'Gerzat',\n", "    'Romagnat',\n", "    'P\u00e9rignat-l\u00e8s-Sarli\u00e8ve',\n", "#    'Saint-G\u00e8nes-Champanelle',\n", "    'Ceyrat'\n", "]\n", "\n", "limites = ox.geocode_to_gdf(communes)"], ["ax = limites.plot(color='white', edgecolor='gray', linewidth=1)\n", "\n", "#mask = (df_propo['amgt_propo'] == 'Partage de voirie') | (df_propo['amgt_propo'] == 'Double sens cyclable')\n", "mask = df_propo['amgt_propo'] == 'Piste s\u00e9par\u00e9e'\n", "#mask = (df_propo['amgt_propo'] == 'Piste s\u00e9par\u00e9e') | (df_propo['amgt_propo'] == 'Bande cyclable')\n", "gdf_propo = gpd.GeoDataFrame(df_propo[mask], geometry=df_propo[mask].geometry)\n", "gdf_propo.plot(ax=ax, color='red', linewidth=3)\n", "\n", "mask = df_exi['amgt_exi'] == 'Piste s\u00e9par\u00e9e'\n", "#mask = (df_propo['amgt_propo'] == 'Piste s\u00e9par\u00e9e') | (df_propo['amgt_propo'] == 'Bande cyclable')\n", "gdf_exi = gpd.GeoDataFrame(df_exi[mask], geometry=df_exi[mask].geometry)\n", "gdf_exi.plot(ax=ax, color='black', linewidth=3)\n", "\n", "plt.axis('off')\n", "plt.show()"], ["ax = limites.plot(color='white', edgecolor='gray', linewidth=1)\n", "\n", "mask = df_propo['amgt_propo'] == 'Piste s\u00e9par\u00e9e'\n", "gdf_propo = gpd.GeoDataFrame(df_propo[mask], geometry=df_propo[mask].geometry)\n", "gdf_propo.plot(ax=ax, color='red', linewidth=3)\n", "\n", "mask = df_exi['amgt_exi'] == 'Piste s\u00e9par\u00e9e'\n", "gdf_exi = gpd.GeoDataFrame(df_exi[mask], geometry=df_exi[mask].geometry)\n", "gdf_exi.plot(ax=ax, color='black', linewidth=3)\n", "\n", "mask = df_propo['amgt_propo'] == 'Bande cyclable'\n", "gdf_propo = gpd.GeoDataFrame(df_propo[mask], geometry=df_propo[mask].geometry)\n", "ax = gdf_propo.plot(ax=ax, linestyle='dashed', color='red')\n", "\n", "mask = df_exi['amgt_exi'] == 'Bande cyclable'\n", "gdf_exi = gpd.GeoDataFrame(df_exi[mask], geometry=df_exi[mask].geometry)\n", "gdf_exi.plot(ax=ax, linestyle='dashed', color='black')\n", "\n", "plt.axis('off')\n", "plt.show()"], []]}], "vinemp/Accidenttwilio": [{"ACCIDetect.ipynb": [["from keras.models import load_model\n", "from collections import deque\n", "import numpy as np\n", "import argparse\n", "import time\n", "import pickle\n", "import cv2\n", "from twilio.rest import Client\n", "camid = 101   \n", "location = 'Hackabit BIT MESHRA,Ranchi, Jharkhand ,India' \n", "\n", "ap = argparse.ArgumentParser()\n", "ap.add_argument(\"-m\", \"--model\", required=True,\n", "\thelp=\"path to trained serialized model\")\n", "ap.add_argument(\"-l\", \"--label-bin\", required=True,\n", "\thelp=\"path to  label binarizer\")\n", "ap.add_argument(\"-i\", \"--input\", required=False,\n", "\thelp=\"path to our input video\")\n", "ap.add_argument(\"-o\", \"--output\", required=True,\n", "\thelp=\"path to our output video\")\n", "ap.add_argument(\"-s\", \"--size\", type=int, default=128,\n", "\thelp=\"size of queue for averaging\")\n", "args = vars(ap.parse_args('-m model -l label-bin -o output'.split()))\n", "\n", "print(\"[INFO] loading model and label binarizer...\")\n", "model = load_model(args[\"model\"])\n", "lb = pickle.loads(open(args[\"label_bin\"], \"rb\").read())\n", "\n", "mean = np.array([123.68, 116.779, 103.939][::1], dtype=\"float32\")\n", "Q = deque(maxlen=args[\"size\"])\n", "\n", "print(\"[INFO] starting video stream...\")\n", "vs = cv2.VideoCapture('demo.mp4')\n", "writer = None\n", "time.sleep(2.0)\n", "#-------------------\n", "(W, H) = (None, None)\n", "client = Client(\"AC358d6a22cb7b8966c3267905a9XXXXXXX\", \"2f9227d6e04fd21e44d85772c5XXXXX\") \n", "prelabel = ''\n", "prelabel = ''\n", "ok = 'Normal'\n", "fi_label = []\n", "framecount = 0\n", "while True:\n", "\tflag,frame = vs.read()\n", "\n", "\n", "\tif W is None or H is None:\n", "\t\t(H, W) = frame.shape[:2]\n", "\n", "\n", "\toutput = frame.copy()\n", "\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n", "\tframe = cv2.resize(frame, (224, 224)).astype(\"float32\")\n", "\tframe -= mean\n", "\n", "\n", "\tpreds = model.predict(np.expand_dims(frame, axis=0))[0]\n", "\n", "\tprediction = preds.argmax(axis=0)\n", "\tQ.append(preds)\n", "\n", "\n", "\tresults = np.array(Q).mean(axis=0)\n", "\tprint('Results = ', results)\n", "\tmaxprob = np.max(results)\n", "\tprint('Maximun Probability = ', maxprob)\n", "\ti = np.argmax(results)\n", "\tlabel = lb[i]\n", "\n", "\trest = 1 - maxprob\n", "    \n", "\tdiff = (maxprob) - (rest)\n", "\tprint('Difference of prob ', diff)\n", "\tth = 100\n", "\tif diff > .80:\n", "\t\tth = diff\n", "      \n", "        \n", "        \n", "        \n", "\tif (preds[prediction]) < th:\n", "\t\ttext = \"Alert : {} - {:.2f}%\".format((ok), 100 - (maxprob * 100))\n", "\t\tcv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 5)\n", "\telse:\n", "\t\tfi_label = np.append(fi_label, label)\n", "\t\ttext = \"Alert : {} - {:.2f}%\".format((label), maxprob * 100)\n", "\t\tcv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 5) \n", "\t\tif label != prelabel: \n", "\t\t\tclient.messages.create(to=\"+917975720047\", \n", "                       from_=\"+12054967936\", \n", "                       body='\\n'+ str(text) +'\\n Satellite: ' + str(camid) + '\\n Orbit: ' + location)\n", "\t\tprelabel = label\n", "\n", "\n", "\tif writer is None:\n", "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n", "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 30,\n", "\t\t\t(W, H), True)\n", "\n", "\twriter.write(output)\n", "\n", "\t# show the output image\n", "\tcv2.imshow(\"Output\", output)\n", "\tkey = cv2.waitKey(1) & 0xFF\n", "\n", "\tif key == ord(\"q\"):\n", "\t\tbreak\n", "\n", "# release the file pointers\n", "print(\"[INFO] cleaning up...\")\n", "writer.release()\n", "vs.stream.release()\n"], [], []]}], "databinary/AccidentAnalysis": [{"Finalproject.ipynb": [["# Term Project - DSC530\n", "# Lenin Kamma 08/09/2019\n", "from __future__ import print_function, division\n", "\n", "import os\n", "os.chdir(r\"C:/Users/Owner/Desktop/Lenin Files/Data Sciences/Assignments/DSC530 Final Project\")\n", "\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# Import csv file\n", "colnames = ['Accident_Severity','Road_Surface_Conditions','Day_of_Week','Speed_limit','Road_Type']\n", "df = pd.read_csv('acchist.csv', usecols = colnames)\n", "\n", "# Replace blank values with Nan\n", "df['Road_Surface_Conditions'].replace('', np.nan, inplace=True)\n", "\n", "# Remove rows with Nan\n", "df.dropna(subset=['Road_Surface_Conditions'], inplace=True)\n", "\n", "# Define a function convert strings into numbers\n", "def roadtype_to_numeric(x):\n", "    if x=='Single carriageway':\n", "        return 1\n", "    if x=='Dual carriageway':\n", "        return 2\n", "    if x=='One way street':\n", "        return 3\n", "    if x=='Roundabout':\n", "        return 4\n", "    if x=='Slip road':\n", "        return 5\n", "    if x=='Unknown':\n", "        return 6\n", "    \n", "# Convert strings into numbers\n", "\n", "def roadsurf_to_numeric(y):\n", "    if y=='Dry':\n", "        return 1\n", "    if y=='Wet/Damp':\n", "        return 2\n", "    if y=='Frost/Ice':\n", "        return 3\n", "    if y=='Snow':\n", "        return 4\n", "    if y=='Flood (Over 3cm of water)':\n", "        return 5\n", "    \n", "df['Road_Surface_Conditions'] = df['Road_Surface_Conditions'].apply(roadsurf_to_numeric)\n", "df['Road_Type'] = df['Road_Type'].apply(roadtype_to_numeric)\n", "df_final = df\n", "df_final"], ["import matplotlib.pyplot as plt\n", "\n", "# Create Histograms for Speed limit\n", "df_final['Speed_limit'].hist(bins=10,xlabelsize=12, ylabelsize=12, color ='green')\n", "plt.xlabel(\"Speed Limit\", fontsize=15)\n", "plt.ylabel(\"Frequency\",fontsize=15)\n", "plt.xlim([0,90])"], ["# Create Histograms for Day_of_Week\n", "df_final['Day_of_Week'].hist(bins = 15,xlabelsize=12, ylabelsize=12, color ='green')\n", "plt.xlabel(\"Day of Week\", fontsize=15)\n", "plt.ylabel(\"Frequency\",fontsize=15)\n", "plt.xlim([1, 10])"], ["# Create Histograms for Accident_serverity\n", "df_final['Accident_Severity'].hist(bins=5,xlabelsize=12, ylabelsize=12, color ='green')\n", "plt.xlabel(\"Accident Severity\", fontsize=15)\n", "plt.ylabel(\"Frequency\",fontsize=15)\n", "plt.xlim([0,9])"], ["# Create Histograms for Road Type\n", "df_final['Road_Type'].hist(bins=5,xlabelsize=12, ylabelsize=12, color ='green')\n", "plt.xlabel(\"Road Type\", fontsize=15)\n", "plt.ylabel(\"Frequency\",fontsize=15)\n", "plt.xlim([0,9])"], ["# Create Histograms for Road Surface Condtions\n", "df_final['Road_Surface_Conditions'].hist(bins=5,xlabelsize=12, ylabelsize=12, color ='green')\n", "plt.xlabel(\"Road Surface Conditions\", fontsize=15)\n", "plt.ylabel(\"Frequency\",fontsize=15)\n", "plt.xlim([0, 10])"], ["# Find Mean, Mode, Variance and Standard Deviation\n", "dfmean = df_final.Accident_Severity.mean()\n", "dfmode = df_final.Accident_Severity.mode()\n", "dfvar = df_final.Accident_Severity.var()\n", "dfsd = df_final.Accident_Severity.std()\n", "print(dfmean)\n", "print(dfmode)\n", "print(dfvar)\n", "print(dfsd)"], ["Select a column using dot notation."], ["import numpy as np\n", "import matplotlib.mlab as mlab\n", "import matplotlib.pyplot as plt\n", "# Plot PMF for Accident Severity\n", "df_accident1 = df_final[\"Accident_Severity\"]\n", "heights,bins = np.histogram(df_accident1,bins=50)\n", "heights = heights/sum(heights)\n", "plt.bar(bins[:-1],heights, width=(max(bins) - min(bins))/len(bins), color=\"red\", alpha=0.5)\n", "plt.xlabel('Accident_Severity')\n", "plt.ylabel('Probability')\n", "plt.title('PMF of Accident_Severity for Differnt Days of a Week')"], ["import numpy as np\n", "from pylab import *\n", "import matplotlib.mlab as mlab\n", "import matplotlib.pyplot as plt\n", "# Plot CDF for Accident Severity\n", "num_bins = 10\n", "df_accident1 = df_final[\"Accident_Severity\"]\n", "counts, bin_edges = np.histogram (df_accident1, bins=num_bins)\n", "cdf = np.cumsum (counts)\n", "plt.plot (bin_edges[1:], cdf/cdf[-1])\n", "plt.xlabel('Accident_Severity')\n", "plt.ylabel('Cdf')\n", "plt.title('Accident_Severity (1=Fatal, 2=Serious, 3=Slight)')"], ["# Ceate a new dataframe with severity counts for each day of the week\n", "df_acc = pd.crosstab(df_final.Accident_Severity,df_final.Day_of_Week).replace(0,np.nan).\\\n", "     stack().reset_index().rename(columns={0:'Time'})\n", "# Ceate a new dataframe with only fatal accidents and day of week\n", "df_accident = df_acc[df_acc[\"Accident_Severity\"] ==1]\n", "df_accident\n"], ["# Create a scatter plot-1\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "df_accident.plot(kind='scatter',x='Day_of_Week',y='Accident_Severity',color='red')\n", "plt.show()"], ["# Create a scatter plot-2\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "ax = df_accident.plot(x='Day_of_Week',y='Time',color='red',linestyle='dashed',linewidth=2)\n", "ax.set(xlabel=\"Day_of_Week(1=Sunday, 2=Monday etc..)\", ylabel=\"Total Fatalities\")\n", "plt.show()"], ["# Find the Pearson's correlation coefficient between Day of week and Fatal accidents\n", "from scipy.stats import pearsonr\n", "import scipy\n", "corr,_ = pearsonr(df_accident.Day_of_Week, df_accident.Time)\n", "print('Pearsons correlation: %.3f' % corr)\n"], ["# Calculate covariance between Day of week and Fatal accidents\n", "covariance = cov(df_accident.Day_of_Week, df_accident.Time)\n", "print(covariance)\n"], ["# Run Chi-squared statistic for Hypothesis testing\n", "from scipy import stats\n", "chi_stat, p_val, Dof, ex = stats.chi2_contingency(df_accident)\n", "print(\"Chi Stat\")\n", "print(chi_stat)\n", "print(\"\\n\")\n", "print(\"Degrees of Freedom\")\n", "print(Dof)\n", "print(\"\\n\")\n", "print(\"P-Value\")\n", "print(p_val)\n"], ["# Build Multiple Regression Model\n", "from pandas import DataFrame\n", "from sklearn import linear_model\n", "import statsmodels.api as sm\n", "#two predictors on the x-axis\n", "X = df_final[['Road_Type','Speed_limit']] \n", "Y = df_final['Accident_Severity']\n", " \n", "# with sklearn\n", "regr = linear_model.LinearRegression()\n", "regr.fit(X, Y)\n", "\n", "print('Intercept: \\n', regr.intercept_)\n", "print('Coefficients: \\n', regr.coef_)\n"], []]}], "Shoyan666/Car_Accident_analysis": [{"Car_Accidents_Analysis.ipynb": [["import numpy as np \n", "import pandas as pd \n", "# importing libraries"], ["# loading dataset\n", "road = pd.read_csv(\"Dataset_pandas_assign.csv\")\n", "road.head(5)"], ["# deleting first row which was the unnecessary with indexes\n", "road = road.iloc[1:,:]\n", "road.head(5)"], ["road.groupby('\u041f\u043e\u043b', as_index = False)['\u0412\u043e\u0437\u0440\u0430\u0441\u0442, \u043f\u043e\u043b\u043d\u044b\u0445 \u043b\u0435\u0442'].mean().round()\n", "# average age of males grater on 4"], ["# by checking one of the determinants of accident we get rid of all the non-accident rows\n", "road_accidents = road[~road['\u0421\u0443\u043c\u043c\u0430 \u0443\u0431\u044b\u0442\u043a\u0430'].isna()]\n", "road_accidents"], ["gender_participation = road_accidents[['\u041f\u043e\u043b', 'Unique number']].groupby('\u041f\u043e\u043b').count()\n", "gender_participation"], ["print('car accidents with female', gender_participation.iloc[0]/len(road_accidents))\n", "print('car accidents with male', gender_participation.iloc[1]/len(road_accidents))\n", "# here we can observe that males participate in car accidents almost in 81 percent of cases witch is huge point\n", "# this task was discussed with Adal Abilbekov, thanks to him for providing an advice"], ["### Task 2 What is the difference between average driving experience and mean bonus-malus class for males and females?"], ["road.head(5)"], ["# finding average experience based on gender\n", "road_experience = road.groupby('\u041f\u043e\u043b', as_index = False)['\u0421\u0442\u0430\u0436 \u0432\u043e\u0436\u0434\u0435\u043d\u0438\u044f'].mean()\n", "road_experience"], ["# finding average kbm based on gender\n", "road_kbm = road.groupby('\u041f\u043e\u043b', as_index = False)['\u041a\u0411\u041c'].mean()\n", "road_kbm"], ["road_difference = road.groupby('\u041f\u043e\u043b').mean()[['\u0421\u0442\u0430\u0436 \u0432\u043e\u0436\u0434\u0435\u043d\u0438\u044f','\u041a\u0411\u041c']].round()\n", "road_difference['\u0421\u0442\u0430\u0436 \u0432\u043e\u0436\u0434\u0435\u043d\u0438\u044f'] - road_difference['\u041a\u0411\u041c']\n", "# here we can see that difference between experience and kbm among males is higher than among females"], ["### Task 3 In which year the most significant number of vehicles were built? Please provide the list of top-10 years sorting based on the number of vehicles (descending)."], ["# creating new dataframe to get only years from date\n", "road_new = road.copy()\n", "road_new['year'] = pd.DatetimeIndex(road_new['\u0413\u043e\u0434 \u0432\u044b\u043f\u0443\u0441\u043a\u0430']).year  \n", "road_new.head(5)"], ["# sorting by count of vehicles \n", "road_sorted = road_new.groupby('year').count().sort_values('Unique number', ascending = False)\n", "road_sorted.rename(columns = {'Unique number':'count'}, inplace = True)\n", "road_sorted.iloc[0:10,0:1]"], ["### Task 4 How much customers pay for the contract in average and how this number is correlated with the bonusmalus class? "], ["column1 = road['\u041a\u0411\u041c']\n", "column2 = road['\u0421\u0442\u0440\u0430\u0445\u043e\u0432\u0430\u044f \u043f\u0440\u0435\u043c\u0438\u044f']\n", "correlation = column1.corr(column2)\n", "correlation\n", "# as the result we can observe that correlation coefficient is very small and it could be count as there is no any correlation |"], ["### Task 5 What is the most popular vehicle-make company in any 5 cities? (choose the cities you like most) \n"], ["# for Almaty city\n", "road_almaty = road[road['\u0413\u043e\u0440\u043e\u0434'] == \"\u0410\u043b\u043c\u0430\u0442\u044b\"][['\u0413\u043e\u0440\u043e\u0434', '\u041c\u043e\u0434\u0435\u043b\u044c', 'Unique number']].groupby(['\u041c\u043e\u0434\u0435\u043b\u044c']).count().sort_values('Unique number', ascending=False)\n", "road_almaty.head(1)\n", "# we found that the most popular vehicle-make company is Toyota"], ["# for Nur-Sultan city\n", "road_nursultan = road[road['\u0413\u043e\u0440\u043e\u0434'] == \"\u041d\u0443\u0440-\u0421\u0443\u043b\u0442\u0430\u043d\"][['\u0413\u043e\u0440\u043e\u0434', '\u041c\u043e\u0434\u0435\u043b\u044c', 'Unique number']].groupby(['\u041c\u043e\u0434\u0435\u043b\u044c']).count().sort_values('Unique number', ascending=False)\n", "road_nursultan.head(1)\n", "# we found that the most popular vehicle-make company is Toyota"], ["# for Shymkent city\n", "road_shymkent = road[road['\u0413\u043e\u0440\u043e\u0434'] == \"\u0428\u044b\u043c\u043a\u0435\u043d\u0442\"][['\u0413\u043e\u0440\u043e\u0434', '\u041c\u043e\u0434\u0435\u043b\u044c', 'Unique number']].groupby(['\u041c\u043e\u0434\u0435\u043b\u044c']).count().sort_values('Unique number', ascending=False)\n", "road_shymkent.head(1)\n", "# we found that the most popular vehicle-make company is Volkswagen"], ["# for Atyrau city\n", "road_atyrau = road[road['\u0413\u043e\u0440\u043e\u0434'] == \"\u0410\u0442\u044b\u0440\u0430\u0443\"][['\u0413\u043e\u0440\u043e\u0434', '\u041c\u043e\u0434\u0435\u043b\u044c', 'Unique number']].groupby(['\u041c\u043e\u0434\u0435\u043b\u044c']).count().sort_values('Unique number', ascending=False)\n", "road_atyrau.head(1)\n", "# we found that the most popular vehicle-make company is \u0412\u0410\u0417"], ["# for Oskemen city\n", "road_oskemen = road[road['\u0413\u043e\u0440\u043e\u0434'] == \"\u0423\u0441\u0442\u044c-\u041a\u0430\u043c\u0435\u043d\u043e\u0433\u043e\u0440\u0441\u043a\"][['\u0413\u043e\u0440\u043e\u0434', '\u041c\u043e\u0434\u0435\u043b\u044c', 'Unique number']].groupby(['\u041c\u043e\u0434\u0435\u043b\u044c']).count().sort_values('Unique number', ascending=False)\n", "road_oskemen.head(1)\n", "# we found that the most popular vehicle-make company is Toyota"], ["# there is another way to do this task also this method is much easier than the previous one\n", "top5_cities = ['\u0410\u043b\u043c\u0430\u0442\u044b', '\u041d\u0443\u0440-\u0421\u0443\u043b\u0442\u0430\u043d', '\u0428\u044b\u043c\u043a\u0435\u043d\u0442', '\u0410\u0442\u044b\u0440\u0430\u0443', '\u0423\u0441\u0442\u044c-\u041a\u0430\u043c\u0435\u043d\u043e\u0433\u043e\u0440\u0441\u043a'] \n", "city_most_popular_vehicle_makers = road[road['\u0413\u043e\u0440\u043e\u0434'].isin(top5_cities)][['\u0413\u043e\u0440\u043e\u0434', '\u041c\u043e\u0434\u0435\u043b\u044c', 'Unique number']].groupby(['\u0413\u043e\u0440\u043e\u0434', '\u041c\u043e\u0434\u0435\u043b\u044c']).count().sort_values('Unique number', ascending=False).reset_index().drop_duplicates('\u0413\u043e\u0440\u043e\u0434', keep='first') \n", "city_most_popular_vehicle_makers"], ["### Task 8 What is the distribution of car road accidents for different cities? Which city is in the first place for road accidents? From the top-10 cities, is there any countrysides?"], ["road.value_counts('\u0413\u043e\u0440\u043e\u0434').head(10)\n", "# here we can see that car accidents happens in Almaty"], ["### Task 10 What is the distribution of road accidents based on the bonus-malus class? \n"], ["road_accidents_toKBM = road[~road['\u0421\u0443\u043c\u043c\u0430 \u0443\u0431\u044b\u0442\u043a\u0430'].isna()]\n", "road_accidents_toKBM\n", "# again we are extracting dataframe related to road accidents"], ["road_accidents_toKBM.value_counts('\u041a\u0411\u041c')\n", "# here we can observe that as much \u041a\u0411\u041c is higher amount of accidents also increases. Also, here found out that correlation between number of accidents and \u041a\u0411\u041c is positive"], []]}], "nicholas-sokolov/accident_analisis": [{"accident_analisis.ipynb": [["import pandas as pd\n", "import numpy as np\n", "import json\n", "import os\n", "\n", "from utils import parser"], ["path = os.path.abspath(os.path.join(os.pardir, r'GibddStat\\dtpdata\\2018'))\n", "data = parser.get_data(path)"], ["df = pd.DataFrame(data)"], ["df['count'] = df.groupby(['region_name', 'DTP_V'])['DTP_V'].transform('count')\n", "df.head()"], ["## \u041e\u0431\u0449\u0430\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430 \u043f\u043e \u0442\u0438\u043f\u0430\u043c \u0414\u0422\u041f (\u0441\u0443\u043c\u043c\u0430\u0440\u043d\u043e \u043f\u043e \u0432\u0441\u0435\u043c \u0440\u0435\u0433\u0438\u043e\u043d\u0430\u043c)\n", "df[['DTP_V', 'count']].groupby('DTP_V').aggregate('count').sort_values('count')"], ["## \u0421\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430 \u043f\u043e \u0442\u0438\u043f\u0430\u043c \u0414\u0422\u041f \u0441\u043e \u0441\u043c\u0435\u0440\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u0441\u043b\u0443\u0447\u0430\u044f\u043c\u0438 (\u0441\u0443\u043c\u043c\u0430\u0440\u043d\u043e \u043f\u043e \u0432\u0441\u0435\u043c \u0440\u0435\u0433\u0438\u043e\u043d\u0430\u043c)\n", "df_death = df[df['dead_number'] > 0]\n", "df_death[['DTP_V', 'count']].groupby('DTP_V').aggregate('count').sort_values('count')"], ["## \u0427\u0430\u0441\u0442\u044c \u0434\u043d\u044f \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0441\u043e\u0432\u0435\u0440\u0448\u0430\u044e\u0442\u0441\u044f \u0414\u0422\u041f \u0441 \u043f\u043e\u0433\u0438\u0431\u0448\u0438\u043c\u0438\n", "df_death['dtp_time_day'].value_counts(normalize=True) * 100"], ["## 5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0432\u0438\u0434\u043e\u0432 \u0414\u0422\u041f \u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0436\u0435\u0440\u0442\u0432\n", "top5_death = df_death['DTP_V'].value_counts(normalize=True) * 100\n", "top5_death[:5]"], ["## \u0421\u0443\u043c\u043c\u0430 \u0438 \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043c\u0435\u0440\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u0442\u0440\u0430\u0432\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0441\u0442\u0438 \u0432 \u0414\u0422\u041f \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a\n", "df_death[['dtp_time_day', 'dead_number', 'injured_number', 'DTP_V']].groupby(['dtp_time_day']).aggregate(['sum', 'mean'])"], ["## \u0421\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430 \u0414\u0422\u041f \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0440\u0435\u0433\u0438\u043e\u043d\u0435\n", "df[['count', 'region_name']].groupby(['region_name']).aggregate('count')\n"], ["import matplotlib.pyplot as plt\n", "plt.style.use('classic')\n", "%matplotlib inline"], ["p_df = df[df['DTP_V'] == '\u041d\u0430\u0435\u0437\u0434 \u043d\u0430 \u043f\u0435\u0448\u0435\u0445\u043e\u0434\u0430']\n", "p_df['date'].head()"], ["p_df_by_date = p_df[['count', 'date']].groupby(['date']).aggregate('count').reset_index()\n", "\n", "p_df_by_date['date'] = p_df_by_date['date'].apply(lambda x: datetime.strptime(x, '%d.%m.%Y').date())\n", "p_df_by_date = p_df_by_date.sort_values('date')"], ["import matplotlib.dates as mdates\n", "from datetime import datetime\n", "\n", "\n", "## \u0413\u0440\u0430\u0444\u0438\u043a \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0431\u0438\u0442\u044b\u0445 \u043f\u0435\u0448\u0435\u0445\u043e\u0434\u043e\u0432 \u043a \u0434\u0430\u0442\u0435\n", "months = mdates.MonthLocator()  # every month\n", "date_fmt = mdates.DateFormatter('%m.%y')\n", "\n", "fig, ax = plt.subplots()\n", "\n", "ax.plot('date', 'count', data=p_df_by_date)\n", "\n", "# format the ticks\n", "ax.xaxis.set_major_locator(months)\n", "ax.xaxis.set_major_formatter(date_fmt)\n", "ax.grid(True)\n", "\n", "# round to nearest years.\n", "datemin = np.datetime64(p_df_by_date['date'][0])\n", "datemax = np.datetime64(p_df_by_date['date'].iloc[-1], 'M') + np.timedelta64(1, 'M')\n", "ax.set_xlim(datemin, datemax)\n", "\n", "\n", "fig.autofmt_xdate()"]]}], "Phoenixak2598/Aviation-accident": [{"Aviation accident.ipynb": [["import pandas as pd \n", "import numpy as np \n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "from datetime import date, datetime, timedelta"], ["data=pd.read_csv('E:\\project\\Data science\\Github\\Aviation Accident\\Dataset1.csv')\n", "data.head()"], ["data.shape"], ["data.isnull().sum()"], ["data['Time'] = data['Time'].replace(np.nan, '00:00')"], ["data.isnull().sum()"], ["data['Time'].value_counts()"], ["# Replacing the faulty time with proper format\n", "data['Time'] = data['Time'].str.replace('c: ','')\n", "data['Time'] = data['Time'].str.replace('c:','')\n", "data['Time'] = data['Time'].str.replace('c','')\n", "data['Time'] = data['Time'].str.replace('12\\'20','12:20')\n", "data['Time'] = data['Time'].str.replace('18.40','18:40')\n", "data['Time'] = data['Time'].str.replace('0943','09:43')\n", "data['Time'] = data['Time'].str.replace('22\\'08','22:08')\n", "data['Time'] = data['Time'].str.replace('114:20','00:00')"], ["# Arrange the time in proper format\n", "data['Time'] = data['Date'] + ' ' +data['Time']\n", "\n", "def todate(x):\n", "    return datetime.strptime(x, '%m/%d/%Y %H:%M')\n", "\n", "data['Time'] = data['Time'].apply(todate)"], ["print('Data ranges from ' + str(data.Time.min()) + ' to ' + str(data.Time.max()))"], ["data.Operator = data.Operator.str.upper()"], ["data.head()"], ["# Total Accident by year"], ["Temp = data.groupby(data.Time.dt.year)[['Date']].count()\n", "Temp.head()"], ["# REnaming data with count\n", "Temp = Temp.rename(columns={'Date':'Count'})"], ["# Plotting the count of accidents by year\n", "plt.figure(figsize=(10,5))\n", "plt.style.use('bmh')\n", "plt.plot(Temp.index, 'Count', data=Temp, color='red', marker='.', linewidth=1.5)\n", "plt.xlabel('Year', fontsize=14)\n", "plt.ylabel('Count', fontsize=14)\n", "plt.title('Count of accidents by year', fontsize=18)\n", "plt.show()"], ["### We conclude from the graph that from the year 1940 the chance of accidents has increased and it is highest somewhat around 1970 and it has started decreasing after 2000.\n", "#### Which is good sign :)"], ["# Plotting the graph against month, week and hour"], ["import matplotlib.pylab as pl\n", "import matplotlib.gridspec as gridspec\n", "\n", "# using the grid graph to display in one go\n", "gs = gridspec.GridSpec(2,2)\n", "pl.figure(figsize=(12,8))\n", "plt.style.use('seaborn-muted')\n", "# Introducing the plot for Count of accident by month \n", "ax = pl.subplot(gs[0,:])\n", "sns.barplot(data.groupby(data.Time.dt.month)[['Date']].count().index, 'Date',\n", "            data = data.groupby(data.Time.dt.month)[['Date']].count(), color='green', linewidth=2)\n", "plt.xticks(data.groupby(data.Time.dt.month)[['Date']].count().index, \n", "           ['Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n", "plt.xlabel('Month', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by month', fontsize=14)\n", "\n", "# Introducing the plot for Count of accident by week\n", "ax = pl.subplot(gs[1,0])\n", "sns.barplot(data.groupby(data.Time.dt.weekday)[['Date']].count().index, 'Date',\n", "            data = data.groupby(data.Time.dt.weekday)[['Date']].count(), color='skyblue', linewidth=2)\n", "plt.xticks(data.groupby(data.Time.dt.weekday)[['Date']].count().index, \n", "           ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n", "plt.xlabel('Weekday', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by Weekday', fontsize=14)\n", "\n", "# Introducing the plot for Count of accident by hour\n", "ax = pl.subplot(gs[1,1])\n", "sns.barplot(data[data.Time.dt.hour != 0].groupby(data.Time.dt.hour )[['Date']].count().index, 'Date',\n", "            data = data[data.Time.dt.hour != 0].groupby(data.Time.dt.hour)[['Date']].count(), color='red', linewidth=2)\n", "plt.xlabel('Hour', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by Hour', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()"], ["## Passenger vs military flights"], ["# Counting the value of number of military passenger\n", "Temp = data.copy()\n", "Temp['isMilitary'] = Temp.Operator.str.contains('MILITARY') # contain() used which look up the MILITARY word matching in operator column irrespective of other string\n", "Temp = Temp.groupby('isMilitary')[['isMilitary']].count()\n", "Temp.index = ['Passenger', 'Military']\n", "Temp"], ["# grouping the military with time and passenger\n", "Temp2 = data.copy()\n", "Temp2['Military'] = Temp2.Operator.str.contains('MILITARY')\n", "Temp2['Passenger'] = Temp2.Military == False\n", "Temp2 = Temp2.loc[:,['Time', 'Military', 'Passenger']]\n", "Temp2"], ["# grouping the flight accident as military and passenger according to year\n", "Temp2 = Temp2.groupby(Temp2.Time.dt.year)[['Military', 'Passenger']].aggregate(np.count_nonzero)\n", "Temp2"], ["colors = ['green', 'skyblue']\n", "#creating pie chart of military and passenger\n", "plt.figure(figsize=(15,6))\n", "plt.subplot(1,2,1)\n", "patches, texts = plt.pie(Temp.isMilitary, colors=colors, labels=Temp.isMilitary, startangle=90)\n", "plt.legend(patches, Temp.index, fontsize=12)\n", "plt.axis('equal')\n", "plt.title('Total number of accidents by flight type', fontsize=15)\n", "\n", "#creating line plot of count of accidents by flight type\n", "plt.subplot(1,2,2)\n", "plt.plot(Temp2.index, 'Military', data=Temp2, color='skyblue', marker='.', linewidth=2)\n", "plt.plot(Temp2.index, 'Passenger', data=Temp2, color='lightgreen', marker='.', linewidth=2)\n", "plt.legend(fontsize=12)\n", "plt.xlabel('Year', fontsize=12)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.title('Count of accidents by flight type', fontsize=15)\n", "plt.tight_layout()\n", "plt.show()"], ["## Total number of fatalities"], ["Fatalities = data.groupby(data.Time.dt.year).sum()\n", "Fatalities['Proportion'] = Fatalities['Fatalities'] / Fatalities['Aboard']\n", "# Plotting the graph of abroad vs year and Fatalities vs year\n", "plt.figure(figsize=(20,8))\n", "plt.subplot(1,2,1)\n", "plt.fill_between(Fatalities.index, 'Aboard', data=Fatalities, color='skyblue', alpha=0.2)\n", "plt.plot(Fatalities.index, 'Aboard', data=Fatalities, marker='.', color='Slateblue', alpha=0.6, linewidth=1)\n", "\n", "plt.fill_between(Fatalities.index, 'Fatalities', data=Fatalities, color='lightgreen', alpha=0.2)\n", "plt.plot(Fatalities.index, 'Fatalities', data=Fatalities, marker='.', color='olive', alpha=0.6, linewidth=1)\n", "\n", "plt.legend(fontsize=14)\n", "plt.xlabel('Year', fontsize=14)\n", "plt.ylabel('Number of People', fontsize=14)\n", "plt.title('Total number of Fatalities by Year')\n", "\n", "# Plotting the graph between fatalities ratio vs year \n", "\n", "plt.subplot(1,2,2)\n", "plt.plot(Fatalities.index, 'Proportion', data=Fatalities, marker='.', color='red', linewidth=2)\n", "plt.xlabel('Year', fontsize=14)\n", "plt.ylabel('Fatalities ratio', fontsize=14)\n", "plt.title('Fatalities ratio by year', fontsize=16)\n", "plt.show()"], ["## Working with second data set"], ["Totals = pd.read_csv('E:\\project\\Data science\\Github\\Aviation Accident\\Dataset2.csv')\n", "Totals.head()"], ["# Removing the unwanted data columns\n", "Totals = Totals.drop(['Country Name', 'Country Code', 'Indicator Code', 'Indicator Name'], axis=1)"], ["# Replacing the nan values with zero\n", "Totals = Totals.replace(np.nan, 0)"], ["Totals = pd.DataFrame(Totals.sum())\n", "Totals.tail()"], ["# Assigning new index \n", "Totals = Totals.drop(Totals.index[0:10])\n", "Totals = Totals['1970':'2008']\n", "Totals.columns = ['Sum']\n", "Totals.index.name = 'Year'\n", "Totals.head()"], ["Fatalities = Fatalities.reset_index()\n", "Fatalities.head()"], ["Fatalities.Time = Fatalities.Time.apply(str)\n", "Fatalities.index = Fatalities['Time']\n", "del Fatalities['Time']\n", "Fatalities = Fatalities['1970':'2008']\n", "Fatalities = Fatalities[['Fatalities']]\n", "Totals = pd.concat([Totals,Fatalities], axis=1)\n", "Totals['Ratio'] = Totals['Fatalities'] / Totals['Sum'] * 100\n", "Totals.head()"], ["gs = gridspec.GridSpec(2,2)\n", "pl.figure(figsize=(15,10))\n", "\n", "# Plotting number of passengers by year\n", "ax= pl.subplot(gs[0,0])\n", "plt.plot(Totals.index, 'Sum', data=Totals, marker='.', color='green', linewidth=1)\n", "plt.xlabel('Year')\n", "plt.ylabel('Number of passengers')\n", "plt.title('Total number of passengers by Year', fontsize=15)\n", "plt.xticks(rotation=90)\n", "\n", "# Plotting total number of death by year\n", "x= pl.subplot(gs[0,1])\n", "plt.plot(Fatalities.index, 'Fatalities', data=Totals, marker='.', color='red', linewidth=1)\n", "plt.xlabel('Year')\n", "plt.ylabel('Number of Deaths')\n", "plt.title('Total number of Deaths by Year', fontsize=15)\n", "plt.xticks(rotation=90)\n", "\n", "# Plotting fatalities/Total number of passengers ratio by year\n", "x= pl.subplot(gs[1,:])\n", "plt.plot(Totals.index, 'Ratio', data=Totals, marker='.', color='orange', linewidth=1)\n", "plt.xlabel('Year')\n", "plt.ylabel('Ratio')\n", "plt.title('Fatalities/Total number of passengers ratio by Year', fontsize=15)\n", "plt.xticks(rotation=90)\n", "plt.tight_layout()\n", "plt.show()"], ["## Plot ratio and number of deaths in one plot"], ["fig = plt.figure(figsize=(12,6))\n", "ax1 = fig.subplots()\n", "ax1.plot(Totals.index, 'Ratio', data=Totals, color='orange', marker='.', linewidth=1)\n", "ax1.set_xlabel('Year', fontsize=12)\n", "for label in ax1.xaxis.get_ticklabels():\n", "    label.set_rotation(45)\n", "ax1.set_ylabel('Ratio', color='orange', fontsize=12)\n", "ax1.tick_params('y', colors='orange')\n", "ax2 = ax1.twinx()\n", "ax2.plot(Fatalities.index, 'Fatalities', data=Fatalities, color='green', marker='.', linewidth=1)\n", "ax2.set_ylabel('Number of Fatalities', color='green', fontsize=12)\n", "ax2.tick_params('y', colors='g')\n", "plt.title('Fatalities VS Ratio by year', fontsize=15)\n", "plt.tight_layout()\n", "plt.show()"], ["## Operator Analysis"], ["data.Operator = data.Operator.str.upper()\n", "data.Operator = data.Operator.replace(\"A B AEROTRANSPORT\", 'AB AEROTRANSPORT')\n", "\n", "Total_by_Op = data.groupby('Operator')[['Operator']].count()\n", "Total_by_Op = Total_by_Op.rename(columns={'Operator':'Count'})\n", "Total_by_Op = Total_by_Op.sort_values(by='Count', ascending=False).head(10)\n", "Total_by_Op"], ["plt.figure(figsize=(12,6))\n", "sns.barplot(y=Total_by_Op.index, x='Count', data=Total_by_Op, palette='gist_heat', orient='h')\n", "plt.xlabel('Count', fontsize=12)\n", "plt.ylabel('Operator', fontsize=12)\n", "plt.title(\"Total Count of the Operator\", fontsize=15)\n", "plt.show()"], ["Prop_by_Op = data.groupby('Operator')[['Fatalities']].sum()\n", "Prop_by_Op = Prop_by_Op.rename(columns={'Operator':'Fatalities'})\n", "Prop_by_Op = Prop_by_Op.sort_values(by='Fatalities', ascending=False)\n", "Prop_by_OpTop = Prop_by_Op.head(15)"], ["plt.figure(figsize=(12,6))\n", "sns.barplot(y=Prop_by_OpTop.index, x='Fatalities', data=Prop_by_OpTop, palette='gist_heat', orient='h')\n", "plt.xlabel('Fatalities', fontsize=12)\n", "plt.ylabel('Operator', fontsize=12)\n", "plt.title(\"Total Fatalities of the Operator\", fontsize=15)\n", "plt.show()"], ["Prop_by_Op[Prop_by_Op['Fatalities'] == Prop_by_Op.Fatalities.min()].index.tolist()"], ["Aeroflot = data[data.Operator == 'AEROFLOT']\n", "Count_by_year = Aeroflot.groupby(data.Time.dt.year)[['Date']].count()\n", "Count_by_year = Count_by_year.rename(columns={'Date':'Count'})\n", "\n", "plt.figure(figsize=(12,6))\n", "plt.plot(Count_by_year.index, 'Count', data=Count_by_year, marker='.', color='red', linewidth=1)\n", "plt.xlabel('Year', fontsize=11)\n", "plt.ylabel('Count', fontsize=11)\n", "plt.title('Count of accidents by year (Aeroflot)', fontsize=16)\n", "plt.show()"]]}]}