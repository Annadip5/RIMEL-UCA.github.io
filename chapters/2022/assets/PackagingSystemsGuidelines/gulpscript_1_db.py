# -*- coding: utf-8 -*-
"""GulpScriptV2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ISS0-xjktFypB7V8-69Vu7KzAFTH73X

# Script permettant de récupérer des liens Github vers des Gulpfile

API documentation : https://pygithub.readthedocs.io/en/latest/github_objects/Repository.html
"""

pip install pygithub

from github import Github
import time
from tqdm import tqdm
import github.GithubException
import requests

tokens = ["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","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","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","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","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"]
token=tokens[-1]
g = Github(token)

query = f'filename:gulpfile&per_page=100'
res = g.search_code(query)


headers = {
  'Authorization': 'Token ' + token
}

n,n_excluded=0,0
gulp_task,gulp_series,gulp_require={},{},{}
Dates,Forks,Stars,Sizes,Urls=[],[],[],[],[]

# Méthode 1
response = requests.request("GET", 'https://api.github.com/rate_limit', headers=headers)
print(response.json())

url = "https://api.github.com/search/code?q=gulp&filename:gulpfile.js&type=code&sort=updated&per_page=100&page=1"
response = requests.request("GET", url, headers=headers)
print(response.text)
items = response.json()['items']

for item in items:
  url = item['html_url']
  if url.endswith("gulpfile.js"):
    if "node_modules" in url:
      n_excluded+=1
    else:
      n+=1
      Urls.append(url)


print(len(Urls))
print(Urls)

# Méthode 2
# for k in tqdm(range(50)):
#     try:
#       page = res.get_page(k)
#     except github.GithubException as err:
#       time.sleep(10)
#       page = res.get_page(k)
#     for content in page:
#         url = content.html_url
#         if "node_modules" in url:
#           n_excluded+=1
#         else:
#           n+=1
#           try:
#             skip = False
#             content = content.decoded_content.decode("utf-8")
#           except UnicodeDecodeError:
#             print("error", content)
#             skip = True
#           if not skip:
#             Urls.append(url)

print("\n" + str(n) + " repositories trouvés :")
print("("+ "%.2f" % (100*n_excluded/(n+n_excluded)) +"% excluded)")

print(Urls)